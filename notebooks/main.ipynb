{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sksurv.metrics import concordance_index_ipcw, concordance_index_censored, brier_score\n",
    "\n",
    "from helpers.data_loader import *\n",
    "from helpers.utils_others import *\n",
    "\n",
    "from models.model import *\n",
    "from helpers.utils_others import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_ITERATION               = 5\n",
    "MICE_IMPUTED_DATA_VERSION   = False\n",
    "data_mode                   = 'STRATCANS_v1_new2'\\\n",
    "    + ('_MICE' if MICE_IMPUTED_DATA_VERSION else '')\n",
    "seed                        = 1234\n",
    "\n",
    "(data_xs, data_xt, data_time, data_y, data_tte), \\\n",
    "(feat_static, feat_timevarying), \\\n",
    "(xt_bin_list, xt_con_list) = \\\n",
    "import_dataset_STRATCANS_v1(mice_version=MICE_IMPUTED_DATA_VERSION)\n",
    "\n",
    "\n",
    "# removes first and secondary pearson score\n",
    "data_xs           = data_xs[:, [0,1,2,3,4,5,6]]\n",
    "feat_static       = feat_static[[0,1,2,3,4,5,6]]\n",
    "\n",
    "data_xt           = data_xt[:, :, [0,1,2,3,6,7,8,9,10]]\n",
    "feat_timevarying  = feat_timevarying[[0,1,2,3,6,7,8,9,10]]\n",
    "xt_con_list       = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "x_dim_static      = len(feat_static)\n",
    "x_dim_timevarying = len(feat_timevarying) # this includes delta\n",
    "\n",
    "max_length                  = np.shape(data_time)[1]\n",
    "num_Event                   = len(np.unique(data_y)) - 1  #number of next outcome events\n",
    "\n",
    "data_y_new   = np.zeros([np.shape(data_y)[0], max_length, num_Event])\n",
    "data_tte_new = np.zeros([np.shape(data_y)[0], max_length, num_Event])\n",
    "\n",
    "seq_length = np.sum(np.sum(data_xt, axis=2) != 0, axis=1)\n",
    "\n",
    "for i in range(np.shape(data_y)[0]):\n",
    "    data_y_new[i, :seq_length[i], :]   = data_y[i]\n",
    "    data_tte_new[i, :seq_length[i], :] = data_tte[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#====================================================\n",
    "##### HYPER-PARAMETERS\n",
    "mb_size                     = 64\n",
    "\n",
    "iteration                   = 50000\n",
    "\n",
    "keep_prob                   = 0.6\n",
    "lr_train                    = 1e-3\n",
    "\n",
    "h_dim_RNN                   = 100\n",
    "h_dim_FC                    = 100\n",
    "\n",
    "num_layers_RNN              = 2\n",
    "num_layers_FC               = 3\n",
    "\n",
    "\n",
    "RNN_type                    = 'GRU' #GRU, LSTM\n",
    "BiRNN                       = None #if not 'None''BiRNN'\n",
    "\n",
    "FC_active_fn                = tf.nn.relu\n",
    "\n",
    "initial_W                   = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "alpha                       = 10.0  #for stepahead-prediction loss\n",
    "p_weibull                   = 1.0 #1.1  #1.0 # 0.5 or 1.0 performed good...\n",
    "\n",
    "\n",
    "reg_scale                   = 0.\n",
    "\n",
    "\n",
    "##### MAKE DICTIONARIES\n",
    "# INPUT DIMENSIONS\n",
    "input_dims                  = { 'x_dim_static'      : x_dim_static,\n",
    "                                'x_dim_timevarying' : x_dim_timevarying, #this includes delta\n",
    "                                'num_Event'         : num_Event,         #next-event types\n",
    "                                'xt_con_list'       : xt_con_list,\n",
    "                                'xt_bin_list'       : xt_bin_list,\n",
    "                                'max_length'        : max_length }\n",
    "\n",
    "# NETWORK HYPER-PARMETERS\n",
    "network_settings            = { 'p_weibull'         : p_weibull,\n",
    "                                'h_dim_RNN'         : h_dim_RNN,\n",
    "                                'h_dim_FC'          : h_dim_FC,\n",
    "                                'num_layers_RNN'    : num_layers_RNN,\n",
    "                                'num_layers_FC'     : num_layers_FC,\n",
    "                                'RNN_type'          : RNN_type,\n",
    "                                'BiRNN'             : BiRNN,\n",
    "                                'FC_active_fn'      : FC_active_fn,\n",
    "                                'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                'initial_W'         : initial_W,\n",
    "                                'reg_scale'         : reg_scale}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "out_itr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_TIMES = [180, 365*1, 365*2, 365*3]\n",
    "\n",
    "FINAL_C = np.zeros([OUT_ITERATION, len(EVAL_TIMES), num_Event])\n",
    "FINAL_B = np.zeros([OUT_ITERATION, len(EVAL_TIMES), num_Event])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:70: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:72: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/utils_network.py:22: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/utils_network.py:29: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:219: The name tf.nn.raw_rnn is deprecated. Please use tf.compat.v1.nn.raw_rnn instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py36_prostate_temporal/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py36_prostate_temporal/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py36_prostate_temporal/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py36_prostate_temporal/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/utils_network.py:121: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:16: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/miniconda3/envs/py36_prostate_temporal/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:1230: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:19: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:267: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:267: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/space/Dropbox/Programming/wsl_repos/prostate_temporal/models/model.py:272: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "MAIN TRAINING ...\n",
      "|| Epoch:00100 | TR_Loss_tte:72.1599 |  TR_Loss_mle:67.5888 || VA_Loss_tte:5.5278 || VA_Loss_mle:2.5697 ||\n",
      "saved...\n",
      "[[0.20491065 0.26024828 0.22055476 0.35275747 0.35385224 0.35703417\n",
      "  0.34230306 0.34230306 0.34230306 0.34230306]\n",
      " [0.72699846 0.63605032 0.53450501 0.46524969 0.43385434 0.47101775\n",
      "  0.47101775 0.47101775 0.47101775 0.46488416]\n",
      " [0.45882353 0.44204098 0.43487439 0.42646875 0.4113663  0.4113663\n",
      "  0.4113663  0.4113663  0.4113663  0.41059877]\n",
      " [0.47748837 0.47485108 0.47160822 0.46570886 0.46570886 0.46570886\n",
      "  0.46570886 0.46570886 0.44139693 0.48129386]]\n",
      "|| Epoch:00200 | TR_Loss_tte:6.6956 |  TR_Loss_mle:3.4407 || VA_Loss_tte:5.2962 || VA_Loss_mle:2.5484 ||\n",
      "saved...\n",
      "[[0.20491065 0.26024828 0.22055476 0.35049549 0.34912533 0.35304369\n",
      "  0.34150233 0.34150233 0.34150233 0.34150233]\n",
      " [0.74307146 0.66537873 0.52510577 0.48563488 0.45764386 0.47461518\n",
      "  0.47461518 0.47461518 0.47461518 0.47461518]\n",
      " [0.47058824 0.46377562 0.45929649 0.45404297 0.44460394 0.44460394\n",
      "  0.44460394 0.44460394 0.44460394 0.44421862]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00300 | TR_Loss_tte:6.1547 |  TR_Loss_mle:3.2806 || VA_Loss_tte:4.7234 || VA_Loss_mle:2.5696 ||\n",
      "saved...\n",
      "[[0.20491065 0.25612853 0.21712582 0.34641166 0.34963239 0.35347176\n",
      "  0.34187563 0.34187563 0.34187563 0.34187563]\n",
      " [0.74628605 0.66804495 0.53050912 0.49229018 0.47317934 0.49076585\n",
      "  0.49076585 0.49076585 0.49076585 0.49076585]\n",
      " [0.48235294 0.47826537 0.47557789 0.47242578 0.46676236 0.46676236\n",
      "  0.46676236 0.46676236 0.46676236 0.4434449 ]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00400 | TR_Loss_tte:5.7803 |  TR_Loss_mle:3.3570 || VA_Loss_tte:4.3794 || VA_Loss_mle:2.5778 ||\n",
      "saved...\n",
      "[[0.2138032  0.25957988 0.21999844 0.35671612 0.35743975 0.36006275\n",
      "  0.34762333 0.34762333 0.34762333 0.34762333]\n",
      " [0.74789335 0.67071117 0.5316668  0.49309595 0.47384462 0.49133731\n",
      "  0.49076585 0.49076585 0.49076585 0.49076585]\n",
      " [0.48823529 0.48551025 0.47557789 0.47242578 0.46676236 0.46676236\n",
      "  0.46676236 0.46676236 0.46676236 0.4434449 ]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00500 | TR_Loss_tte:5.2572 |  TR_Loss_mle:3.1545 || VA_Loss_tte:4.1220 || VA_Loss_mle:2.5791 ||\n",
      "saved...\n",
      "[[0.21824948 0.26130555 0.22143475 0.36040532 0.36036888 0.36058839\n",
      "  0.34808172 0.34808172 0.34808172 0.34808172]\n",
      " [0.74789335 0.67192035 0.5316668  0.49309595 0.47384462 0.49133731\n",
      "  0.49133731 0.49133731 0.49133731 0.49076585]\n",
      " [0.48823529 0.48551025 0.4837186  0.48161719 0.46676236 0.46676236\n",
      "  0.46676236 0.46676236 0.46676236 0.4434449 ]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00600 | TR_Loss_tte:5.2724 |  TR_Loss_mle:3.3187 || VA_Loss_tte:4.0035 || VA_Loss_mle:2.5705 ||\n",
      "saved...\n",
      "[[0.21824948 0.25699997 0.21785113 0.36547163 0.36430926 0.36213537\n",
      "  0.35210995 0.35210995 0.35210995 0.35210995]\n",
      " [0.74789335 0.67071117 0.5316668  0.49309595 0.47317934 0.49076585\n",
      "  0.49076585 0.49076585 0.49076585 0.49076585]\n",
      " [0.48823529 0.48551025 0.4837186  0.47242578 0.46676236 0.46676236\n",
      "  0.46676236 0.46676236 0.46676236 0.4434449 ]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00700 | TR_Loss_tte:4.9554 |  TR_Loss_mle:3.0843 || VA_Loss_tte:3.9351 || VA_Loss_mle:2.5583 ||\n",
      "saved...\n",
      "[[0.25404889 0.26244539 0.22238345 0.37485004 0.37294909 0.36942914\n",
      "  0.36650805 0.36650805 0.36650805 0.36650805]\n",
      " [0.74628605 0.66804495 0.53050912 0.49229018 0.47317934 0.49371905\n",
      "  0.49371905 0.49371905 0.49371905 0.49371905]\n",
      " [0.48235294 0.47826537 0.47557789 0.47242578 0.46676236 0.46676236\n",
      "  0.46676236 0.46676236 0.46676236 0.4434449 ]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00800 | TR_Loss_tte:4.8995 |  TR_Loss_mle:3.0967 || VA_Loss_tte:3.8208 || VA_Loss_mle:2.5393 ||\n",
      "saved...\n",
      "[[0.28570763 0.25870647 0.22913514 0.38607172 0.39521329 0.39262183\n",
      "  0.38673335 0.38673335 0.38673335 0.38673335]\n",
      " [0.74467875 0.66804495 0.53050912 0.48563488 0.4654116  0.48564371\n",
      "  0.48564371 0.48564371 0.48564371 0.48564371]\n",
      " [0.48235294 0.47826537 0.47557789 0.46323438 0.44460394 0.44460394\n",
      "  0.44460394 0.44460394 0.44460394 0.44421862]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:00900 | TR_Loss_tte:4.7660 |  TR_Loss_mle:3.0244 || VA_Loss_tte:3.7083 || VA_Loss_mle:2.5133 ||\n",
      "saved...\n",
      "[[0.41678888 0.28150064 0.27375264 0.40440864 0.43234745 0.42007632\n",
      "  0.41897947 0.41897947 0.41897947 0.41897947]\n",
      " [0.73408527 0.65716136 0.51729209 0.47730196 0.45744201 0.47561768\n",
      "  0.47421459 0.47367594 0.47367594 0.47367594]\n",
      " [0.46470588 0.45653074 0.45115579 0.44485156 0.47225484 0.47049438\n",
      "  0.46873392 0.46873392 0.46873392 0.46328148]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.47713924\n",
      "  0.47713924 0.47713924 0.45030067 0.48956167]]\n",
      "|| Epoch:01000 | TR_Loss_tte:4.6658 |  TR_Loss_mle:2.9962 || VA_Loss_tte:3.6184 || VA_Loss_mle:2.4770 ||\n",
      "saved...\n",
      "[[0.57968168 0.33146761 0.36860477 0.46403865 0.50414225 0.48280048\n",
      "  0.49779098 0.49779098 0.49779098 0.49779098]\n",
      " [0.73247797 0.65449514 0.51188874 0.46475088 0.47146129 0.49076003\n",
      "  0.49183733 0.49183733 0.50113769 0.49554274]\n",
      " [0.46470588 0.45653074 0.45115579 0.44485156 0.47929668 0.46997793\n",
      "  0.46997793 0.46997793 0.46997793 0.46426425]\n",
      " [0.48499225 0.48323405 0.48107215 0.47713924 0.47713924 0.46570886\n",
      "  0.46570886 0.46570886 0.44139693 0.48129386]]\n",
      "|| Epoch:01100 | TR_Loss_tte:4.6003 |  TR_Loss_mle:2.9606 || VA_Loss_tte:3.5161 || VA_Loss_mle:2.4140 ||\n",
      "saved...\n",
      "[[0.69229021 0.35817031 0.44606636 0.55048406 0.56501637 0.53664064\n",
      "  0.54755511 0.54755511 0.54755511 0.54755511]\n",
      " [0.71110824 0.62239115 0.51790037 0.49993953 0.45702117 0.50132704\n",
      "  0.50132704 0.50132704 0.50132704 0.49735702]\n",
      " [0.43529412 0.42030635 0.41419847 0.40463466 0.4391507  0.4391507\n",
      "  0.4391507  0.4391507  0.4391507  0.43991052]\n",
      " [0.47748837 0.47485108 0.47160822 0.46570886 0.46570886 0.46570886\n",
      "  0.46570886 0.46570886 0.44139693 0.48129386]]\n",
      "|| Epoch:01200 | TR_Loss_tte:4.4912 |  TR_Loss_mle:2.8939 || VA_Loss_tte:3.2905 || VA_Loss_mle:2.2426 ||\n",
      "saved...\n",
      "[[0.73660015 0.37782532 0.46242558 0.59183874 0.59608535 0.5719988\n",
      "  0.565527   0.565527   0.565527   0.565527  ]\n",
      " [0.68300559 0.71198482 0.65037542 0.58960427 0.52980346 0.55682934\n",
      "  0.55682934 0.55682934 0.55682934 0.55682934]\n",
      " [0.4        0.50198316 0.52982727 0.49130573 0.50957583 0.50957583\n",
      "  0.50957583 0.50957583 0.53414945 0.5149602 ]\n",
      " [0.57203529 0.59754128 0.56276711 0.58194509 0.58194509 0.58194509\n",
      "  0.58194509 0.58194509 0.52803601 0.56174495]]\n",
      "|| Epoch:01300 | TR_Loss_tte:4.1244 |  TR_Loss_mle:2.5746 || VA_Loss_tte:2.9389 || VA_Loss_mle:1.9164 ||\n",
      "saved...\n",
      "[[0.74986257 0.45477925 0.53042117 0.64253795 0.64016849 0.61539061\n",
      "  0.61355022 0.61355022 0.61355022 0.61355022]\n",
      " [0.71105547 0.75639951 0.75454796 0.66451368 0.59669527 0.61781306\n",
      "  0.62409988 0.62393527 0.62409988 0.62393527]\n",
      " [0.90588235 0.74328693 0.66693283 0.58153848 0.57426859 0.57426859\n",
      "  0.57426859 0.57426859 0.57426859 0.53254952]\n",
      " [0.65751121 0.70545091 0.62598521 0.63061779 0.63061779 0.63061779\n",
      "  0.63061779 0.63061779 0.58909454 0.61844251]]\n",
      "|| Epoch:01400 | TR_Loss_tte:3.8005 |  TR_Loss_mle:2.2092 || VA_Loss_tte:2.5383 || VA_Loss_mle:1.5068 ||\n",
      "saved...\n",
      "[[0.80766414 0.54825908 0.60822611 0.67065389 0.657749   0.63640882\n",
      "  0.64206244 0.64206244 0.64206244 0.64206244]\n",
      " [0.71480673 0.75630751 0.76421957 0.7631074  0.73243043 0.73087963\n",
      "  0.73087963 0.73087963 0.73087963 0.73087963]\n",
      " [0.82352941 0.83752829 0.8233303  0.78336795 0.76274205 0.76274205\n",
      "  0.76274205 0.76274205 0.76274205 0.66316586]\n",
      " [0.78227545 0.80135376 0.75814873 0.73708435 0.73708435 0.73708435\n",
      "  0.73708435 0.73708435 0.66763342 0.69137194]]\n",
      "|| Epoch:01500 | TR_Loss_tte:3.4542 |  TR_Loss_mle:1.9230 || VA_Loss_tte:2.4351 || VA_Loss_mle:1.4582 ||\n",
      "saved...\n",
      "[[0.82544924 0.58835281 0.64356957 0.6947134  0.66856344 0.6455384\n",
      "  0.65511553 0.65511553 0.65511553 0.65511553]\n",
      " [0.73565143 0.7778172  0.79021199 0.76781992 0.73196337 0.73313758\n",
      "  0.73313758 0.73313758 0.73313758 0.73313758]\n",
      " [0.84705882 0.79830767 0.74029414 0.69669423 0.69750845 0.69750845\n",
      "  0.69750845 0.69750845 0.69750845 0.63984102]\n",
      " [0.76689184 0.7653823  0.72232354 0.7150205  0.7150205  0.7150205\n",
      "  0.7150205  0.7150205  0.6504467  0.67541274]]\n",
      "|| Epoch:01600 | TR_Loss_tte:3.3241 |  TR_Loss_mle:1.8754 || VA_Loss_tte:2.3076 || VA_Loss_mle:1.3920 ||\n",
      "saved...\n",
      "[[0.84331074 0.67269426 0.71376846 0.72089061 0.68375836 0.65819837\n",
      "  0.66870151 0.66870151 0.66870151 0.66870151]\n",
      " [0.69974524 0.75080489 0.76262271 0.74157789 0.72468364 0.72688452\n",
      "  0.72688452 0.72688452 0.72688452 0.72688452]\n",
      " [0.84705882 0.80563404 0.73196291 0.72150675 0.72801054 0.72801054\n",
      "  0.72801054 0.72801054 0.72801054 0.69214803]\n",
      " [0.7868424  0.78130816 0.75222034 0.7574092  0.7574092  0.7574092\n",
      "  0.7574092  0.7574092  0.71316058 0.73364742]]\n",
      "|| Epoch:01700 | TR_Loss_tte:3.3097 |  TR_Loss_mle:1.8790 || VA_Loss_tte:2.2344 || VA_Loss_mle:1.3557 ||\n",
      "saved...\n",
      "[[0.8524325  0.7132981  0.75150922 0.7522061  0.7118136  0.68366219\n",
      "  0.69090736 0.69090736 0.69090736 0.69090736]\n",
      " [0.702485   0.76160828 0.7762212  0.7472829  0.72939399 0.73078354\n",
      "  0.73078354 0.73078354 0.73078354 0.73078354]\n",
      " [0.88235294 0.82664886 0.74435371 0.72878641 0.73406617 0.73406617\n",
      "  0.73406617 0.73406617 0.73406617 0.72931614]\n",
      " [0.80619292 0.80013067 0.76932357 0.76749636 0.76749636 0.76749636\n",
      "  0.76749636 0.76749636 0.75510668 0.77259762]]\n",
      "|| Epoch:01800 | TR_Loss_tte:3.1837 |  TR_Loss_mle:1.7768 || VA_Loss_tte:2.1890 || VA_Loss_mle:1.3454 ||\n",
      "saved...\n",
      "[[0.8524325  0.74526551 0.77811628 0.7708715  0.73165191 0.7041364\n",
      "  0.70327025 0.70327025 0.70327025 0.70327025]\n",
      " [0.71273621 0.76932029 0.77908754 0.74576991 0.72814479 0.72380413\n",
      "  0.72380413 0.72380413 0.72380413 0.72380413]\n",
      " [0.89411765 0.83983402 0.76110608 0.74707721 0.7492815  0.7492815\n",
      "  0.7492815  0.7492815  0.7492815  0.70895224]\n",
      " [0.83422498 0.83027775 0.79852392 0.79878564 0.79878564 0.79878564\n",
      "  0.79878564 0.79878564 0.74539091 0.76357578]]\n",
      "|| Epoch:01900 | TR_Loss_tte:3.1357 |  TR_Loss_mle:1.7987 || VA_Loss_tte:2.1473 || VA_Loss_mle:1.3445 ||\n",
      "saved...\n",
      "[[0.86584773 0.74690994 0.77948496 0.7789941  0.74092294 0.71797208\n",
      "  0.71278992 0.71278992 0.71278992 0.71278992]\n",
      " [0.71239394 0.76323462 0.77738953 0.74634207 0.73079618 0.72608159\n",
      "  0.72608159 0.72608159 0.72608159 0.72608159]\n",
      " [0.87058824 0.83423712 0.74895651 0.73776636 0.7415362  0.7415362\n",
      "  0.7415362  0.7415362  0.7415362  0.7028334 ]\n",
      " [0.82914846 0.82565423 0.80733828 0.80244608 0.80244608 0.80244608\n",
      "  0.80244608 0.80244608 0.74824222 0.76622344]]\n",
      "|| Epoch:02000 | TR_Loss_tte:3.0902 |  TR_Loss_mle:1.7611 || VA_Loss_tte:2.1373 || VA_Loss_mle:1.3564 ||\n",
      "saved...\n",
      "[[0.86600053 0.76591519 0.79530337 0.79105238 0.75461698 0.72513545\n",
      "  0.72426175 0.72426175 0.72426175 0.72426175]\n",
      " [0.73509946 0.78323013 0.78547756 0.74237599 0.70896414 0.71028174\n",
      "  0.71028174 0.71028174 0.71028174 0.71028174]\n",
      " [0.87058824 0.81620691 0.72336105 0.69014642 0.70526885 0.70526885\n",
      "  0.70526885 0.70526885 0.70526885 0.70656605]\n",
      " [0.80553827 0.79685321 0.76415402 0.7670136  0.7670136  0.7670136\n",
      "  0.7670136  0.7670136  0.75473064 0.77224844]]\n",
      "|| Epoch:02100 | TR_Loss_tte:2.9521 |  TR_Loss_mle:1.6688 || VA_Loss_tte:2.1117 || VA_Loss_mle:1.3548 ||\n",
      "saved...\n",
      "[[0.87941576 0.76700212 0.79423531 0.78116761 0.7547825  0.72722232\n",
      "  0.72085664 0.72085664 0.72085664 0.72085664]\n",
      " [0.72376598 0.77178984 0.77739409 0.73472058 0.71447633 0.71206334\n",
      "  0.71206334 0.71206334 0.71206334 0.71206334]\n",
      " [0.82352941 0.78695214 0.7120703  0.68442353 0.69698731 0.69698731\n",
      "  0.69698731 0.69698731 0.69698731 0.70002357]\n",
      " [0.77243669 0.77705599 0.73393875 0.73846327 0.73846327 0.73846327\n",
      "  0.73846327 0.73846327 0.73249125 0.75159744]]\n",
      "|| Epoch:02200 | TR_Loss_tte:3.0186 |  TR_Loss_mle:1.7512 || VA_Loss_tte:2.0807 || VA_Loss_mle:1.3492 ||\n",
      "saved...\n",
      "[[0.87067602 0.77607404 0.80770423 0.79389473 0.77050063 0.74438589\n",
      "  0.73837001 0.73837001 0.73837001 0.73837001]\n",
      " [0.71205167 0.77463348 0.78500365 0.75514964 0.72860186 0.72419674\n",
      "  0.72419674 0.72419674 0.72419674 0.72419674]\n",
      " [0.83529412 0.8167053  0.76523798 0.74426432 0.74676629 0.74676629\n",
      "  0.74676629 0.74676629 0.74676629 0.70696521]\n",
      " [0.82339779 0.83131397 0.8053763  0.79707745 0.79707745 0.79707745\n",
      "  0.79707745 0.79707745 0.74406031 0.76234021]]\n",
      "|| Epoch:02300 | TR_Loss_tte:2.8985 |  TR_Loss_mle:1.6625 || VA_Loss_tte:2.0886 || VA_Loss_mle:1.3547 ||\n",
      "|| Epoch:02400 | TR_Loss_tte:2.9826 |  TR_Loss_mle:1.7494 || VA_Loss_tte:2.0720 || VA_Loss_mle:1.3449 ||\n",
      "saved...\n",
      "[[0.90655183 0.77488131 0.80473877 0.7929708  0.7713551  0.74722201\n",
      "  0.73816408 0.73816408 0.73816408 0.73816408]\n",
      " [0.74105381 0.77896734 0.78500741 0.74961509 0.72839021 0.72401493\n",
      "  0.72401493 0.72401493 0.72401493 0.72401493]\n",
      " [0.76470588 0.78069971 0.72261082 0.70051832 0.71037587 0.71037587\n",
      "  0.71037587 0.71037587 0.71037587 0.71060063]\n",
      " [0.78249423 0.78703779 0.76292575 0.76224697 0.76224697 0.76224697\n",
      "  0.76224697 0.76224697 0.75101766 0.76880064]]\n",
      "|| Epoch:02500 | TR_Loss_tte:2.9201 |  TR_Loss_mle:1.6950 || VA_Loss_tte:2.0155 || VA_Loss_mle:1.3214 ||\n",
      "saved...\n",
      "[[0.90202915 0.78792858 0.81559824 0.79556234 0.77785695 0.7544904\n",
      "  0.73914416 0.73914416 0.73914416 0.73914416]\n",
      " [0.70853336 0.76907256 0.78538571 0.76851906 0.74399832 0.72856223\n",
      "  0.72856223 0.72856223 0.72856223 0.72856223]\n",
      " [0.82352941 0.83633183 0.79183199 0.77657322 0.77012176 0.77012176\n",
      "  0.77012176 0.77012176 0.77012176 0.7254162 ]\n",
      " [0.81977637 0.83975271 0.82396839 0.81214503 0.81214503 0.81214503\n",
      "  0.81214503 0.81214503 0.75579726 0.77323888]]\n",
      "|| Epoch:02600 | TR_Loss_tte:2.9301 |  TR_Loss_mle:1.7198 || VA_Loss_tte:2.0437 || VA_Loss_mle:1.3512 ||\n",
      "|| Epoch:02700 | TR_Loss_tte:2.7727 |  TR_Loss_mle:1.6210 || VA_Loss_tte:1.9989 || VA_Loss_mle:1.3328 ||\n",
      "saved...\n",
      "[[0.89720086 0.76425586 0.79194955 0.76980786 0.75081946 0.72988576\n",
      "  0.71514177 0.71514177 0.71514177 0.71514177]\n",
      " [0.72406969 0.76910423 0.77601532 0.7357901  0.7129928  0.70488264\n",
      "  0.70488264 0.70488264 0.70488264 0.70488264]\n",
      " [0.82352941 0.79253879 0.7219738  0.70607596 0.71499904 0.71499904\n",
      "  0.71499904 0.71499904 0.71499904 0.71425296]\n",
      " [0.77898782 0.78466458 0.75795044 0.75816475 0.75816475 0.75816475\n",
      "  0.75816475 0.75816475 0.74783779 0.76584789]]\n",
      "|| Epoch:02800 | TR_Loss_tte:2.9077 |  TR_Loss_mle:1.7404 || VA_Loss_tte:1.9775 || VA_Loss_mle:1.3249 ||\n",
      "saved...\n",
      "[[0.89298379 0.75677969 0.78769973 0.77615612 0.76360638 0.74440716\n",
      "  0.73838856 0.73838856 0.73838856 0.73838856]\n",
      " [0.7166908  0.76063899 0.77816059 0.74484958 0.70627331 0.7079704\n",
      "  0.7079704  0.7079704  0.7079704  0.7079704 ]\n",
      " [0.83529412 0.82709332 0.76699876 0.75193604 0.75314807 0.75314807\n",
      "  0.75314807 0.75314807 0.75314807 0.74439097]\n",
      " [0.82967952 0.82950451 0.80133371 0.79733223 0.79733223 0.79733223\n",
      "  0.79733223 0.79733223 0.77834745 0.79417847]]\n",
      "|| Epoch:02900 | TR_Loss_tte:2.8485 |  TR_Loss_mle:1.7046 || VA_Loss_tte:1.9570 || VA_Loss_mle:1.3238 ||\n",
      "saved...\n",
      "[[0.91076889 0.79318287 0.81602602 0.77172703 0.75706806 0.73516085\n",
      "  0.71706275 0.71706275 0.71706275 0.71706275]\n",
      " [0.72981434 0.7617696  0.76466378 0.72788923 0.70192386 0.68651518\n",
      "  0.68651518 0.68651518 0.68651518 0.68651518]\n",
      " [0.67058824 0.7728433  0.71719366 0.71036945 0.71857061 0.71857061\n",
      "  0.71857061 0.71857061 0.71857061 0.71707453]\n",
      " [0.79320682 0.79850524 0.77753606 0.77404753 0.77404753 0.77404753\n",
      "  0.77404753 0.77404753 0.76020974 0.77733621]]\n",
      "|| Epoch:03000 | TR_Loss_tte:2.8578 |  TR_Loss_mle:1.7254 || VA_Loss_tte:1.9378 || VA_Loss_mle:1.3202 ||\n",
      "saved...\n",
      "[[0.9109217  0.77881125 0.80603699 0.77738536 0.76450905 0.7451692\n",
      "  0.73637392 0.73637392 0.73637392 0.73637392]\n",
      " [0.74071154 0.78162394 0.79152538 0.75590571 0.7197598  0.71955488\n",
      "  0.71955488 0.71955488 0.71955488 0.71955488]\n",
      " [0.77647059 0.80315633 0.75773186 0.74174468 0.74467031 0.74467031\n",
      "  0.74467031 0.74467031 0.74467031 0.73769348]\n",
      " [0.82465561 0.82666921 0.78944683 0.78400739 0.78400739 0.78400739\n",
      "  0.78400739 0.78400739 0.76796802 0.78454037]]\n",
      "|| Epoch:03100 | TR_Loss_tte:2.7415 |  TR_Loss_mle:1.6381 || VA_Loss_tte:1.9436 || VA_Loss_mle:1.3323 ||\n",
      "|| Epoch:03200 | TR_Loss_tte:2.7486 |  TR_Loss_mle:1.6541 || VA_Loss_tte:1.9017 || VA_Loss_mle:1.3076 ||\n",
      "saved...\n",
      "[[0.91076889 0.76059653 0.79284929 0.76142747 0.74924385 0.73033514\n",
      "  0.72611698 0.72611698 0.72611698 0.72611698]\n",
      " [0.71157682 0.76553399 0.78008813 0.74240801 0.71372374 0.71437008\n",
      "  0.71437008 0.71437008 0.71437008 0.71437008]\n",
      " [0.8        0.80686283 0.75472759 0.73961619 0.74642064 0.74642064\n",
      "  0.74642064 0.74642064 0.74642064 0.73907625]\n",
      " [0.79673115 0.80836253 0.76933618 0.77126554 0.77126554 0.77126554\n",
      "  0.77126554 0.77126554 0.7580427  0.77532394]]\n",
      "|| Epoch:03300 | TR_Loss_tte:2.7292 |  TR_Loss_mle:1.6760 || VA_Loss_tte:1.9089 || VA_Loss_mle:1.3190 ||\n",
      "|| Epoch:03400 | TR_Loss_tte:2.7352 |  TR_Loss_mle:1.6577 || VA_Loss_tte:1.8821 || VA_Loss_mle:1.3076 ||\n",
      "saved...\n",
      "[[0.89275458 0.7702933  0.80092009 0.77276616 0.75757019 0.73330234\n",
      "  0.72870455 0.72870455 0.72870455 0.72870455]\n",
      " [0.73962927 0.78372383 0.79471139 0.7636604  0.72616248 0.72210138\n",
      "  0.72210138 0.72210138 0.72210138 0.72210138]\n",
      " [0.77647059 0.82336589 0.77907087 0.74393606 0.74649323 0.74649323\n",
      "  0.74649323 0.74649323 0.74649323 0.7391336 ]\n",
      " [0.85278787 0.84542046 0.78226777 0.77811701 0.77811701 0.77811701\n",
      "  0.77811701 0.77811701 0.76337968 0.78027974]]\n",
      "|| Epoch:03500 | TR_Loss_tte:2.6671 |  TR_Loss_mle:1.6264 || VA_Loss_tte:1.9003 || VA_Loss_mle:1.3217 ||\n",
      "|| Epoch:03600 | TR_Loss_tte:2.7179 |  TR_Loss_mle:1.6671 || VA_Loss_tte:1.8810 || VA_Loss_mle:1.3231 ||\n",
      "saved...\n",
      "[[0.91981425 0.77665227 0.8081855  0.77641969 0.76204695 0.74114355\n",
      "  0.73018415 0.73018415 0.73018415 0.73018415]\n",
      " [0.74057896 0.78735238 0.80314401 0.7727625  0.72931968 0.72481332\n",
      "  0.72481332 0.72481332 0.72481332 0.72481332]\n",
      " [0.81176471 0.86177665 0.81180627 0.78957771 0.77759401 0.77759401\n",
      "  0.77759401 0.77759401 0.77759401 0.76370343]\n",
      " [0.87038811 0.86289437 0.82306324 0.81534834 0.81534834 0.81534834\n",
      "  0.81534834 0.81534834 0.79238117 0.80720986]]\n",
      "|| Epoch:03700 | TR_Loss_tte:2.6515 |  TR_Loss_mle:1.6215 || VA_Loss_tte:1.8804 || VA_Loss_mle:1.3211 ||\n",
      "saved...\n",
      "[[0.91513876 0.77858572 0.80387655 0.75719428 0.74922161 0.7220249\n",
      "  0.71351164 0.71351164 0.71351164 0.71351164]\n",
      " [0.74619104 0.77700392 0.78217606 0.75318159 0.71078607 0.70889353\n",
      "  0.70889353 0.70889353 0.70889353 0.70889353]\n",
      " [0.64705882 0.77103013 0.73075275 0.69234523 0.70357703 0.70357703\n",
      "  0.70357703 0.70357703 0.70357703 0.7052295 ]\n",
      " [0.80659101 0.80821162 0.73262897 0.74453202 0.74453202 0.74453202\n",
      "  0.74453202 0.74453202 0.73721852 0.75598708]]\n",
      "|| Epoch:03800 | TR_Loss_tte:2.6513 |  TR_Loss_mle:1.6371 || VA_Loss_tte:1.8663 || VA_Loss_mle:1.3149 ||\n",
      "saved...\n",
      "[[0.91513876 0.77063677 0.80317869 0.76782714 0.75547626 0.72747272\n",
      "  0.72094162 0.72094162 0.72094162 0.72094162]\n",
      " [0.7285995  0.78125429 0.7936391  0.76642208 0.72645127 0.72234945\n",
      "  0.72234945 0.72234945 0.72234945 0.72234945]\n",
      " [0.76470588 0.80397681 0.75751728 0.73107694 0.7393172  0.7393172\n",
      "  0.7393172  0.7393172  0.7393172  0.73346448]\n",
      " [0.80232934 0.81406912 0.75339278 0.75818406 0.75818406 0.75818406\n",
      "  0.75818406 0.75818406 0.74785283 0.76586186]]\n",
      "|| Epoch:03900 | TR_Loss_tte:2.6806 |  TR_Loss_mle:1.6579 || VA_Loss_tte:1.8997 || VA_Loss_mle:1.3338 ||\n",
      "|| Epoch:04000 | TR_Loss_tte:2.6288 |  TR_Loss_mle:1.6385 || VA_Loss_tte:1.8647 || VA_Loss_mle:1.3252 ||\n",
      "saved...\n",
      "[[0.893213   0.75419228 0.79146438 0.7855731  0.77052155 0.74000637\n",
      "  0.73455083 0.73455083 0.73455083 0.73455083]\n",
      " [0.72772057 0.78059306 0.79461584 0.75805672 0.71281985 0.71359368\n",
      "  0.71359368 0.71359368 0.71359368 0.71359368]\n",
      " [0.83529412 0.82121372 0.75960438 0.7219859  0.72840912 0.72840912\n",
      "  0.72840912 0.72840912 0.72840912 0.72484702]\n",
      " [0.80795331 0.8042804  0.74928154 0.74766739 0.74766739 0.74766739\n",
      "  0.74766739 0.74766739 0.73966083 0.75825495]]\n",
      "|| Epoch:04100 | TR_Loss_tte:2.5763 |  TR_Loss_mle:1.5909 || VA_Loss_tte:1.8738 || VA_Loss_mle:1.3292 ||\n",
      "|| Epoch:04200 | TR_Loss_tte:2.5950 |  TR_Loss_mle:1.6035 || VA_Loss_tte:1.8608 || VA_Loss_mle:1.3243 ||\n",
      "saved...\n",
      "[[0.90632262 0.76576232 0.80109433 0.77773082 0.76452225 0.73494174\n",
      "  0.72745502 0.72745502 0.72745502 0.72745502]\n",
      " [0.72199917 0.77628884 0.80121245 0.77520127 0.73369983 0.72857574\n",
      "  0.72857574 0.72857574 0.72857574 0.72857574]\n",
      " [0.71764706 0.85483886 0.79589125 0.7638969  0.76327308 0.76327308\n",
      "  0.76327308 0.76327308 0.76327308 0.72000569]\n",
      " [0.84285862 0.83442863 0.77457942 0.77199588 0.77199588 0.77199588\n",
      "  0.77199588 0.77199588 0.75861161 0.77585221]]\n",
      "|| Epoch:04300 | TR_Loss_tte:2.6507 |  TR_Loss_mle:1.6427 || VA_Loss_tte:1.8619 || VA_Loss_mle:1.3216 ||\n",
      "|| Epoch:04400 | TR_Loss_tte:2.5953 |  TR_Loss_mle:1.6292 || VA_Loss_tte:1.8499 || VA_Loss_mle:1.3182 ||\n",
      "saved...\n",
      "[[0.90210555 0.75843416 0.7969677  0.7791798  0.7653458  0.73563698\n",
      "  0.7280613  0.7280613  0.7280613  0.7280613 ]\n",
      " [0.73228895 0.7752876  0.7918959  0.76668762 0.71994598 0.7167616\n",
      "  0.7167616  0.7167616  0.7167616  0.7167616 ]\n",
      " [0.65882353 0.80051922 0.76975306 0.72343646 0.72961578 0.72961578\n",
      "  0.72961578 0.72961578 0.72961578 0.72580029]\n",
      " [0.81566052 0.83452188 0.76834414 0.76669274 0.76669274 0.76669274\n",
      "  0.76669274 0.76669274 0.7544807  0.77201635]]\n",
      "|| Epoch:04500 | TR_Loss_tte:2.5480 |  TR_Loss_mle:1.5813 || VA_Loss_tte:1.8482 || VA_Loss_mle:1.3210 ||\n",
      "saved...\n",
      "[[0.91023407 0.75109197 0.78099301 0.73759012 0.73813204 0.71477782\n",
      "  0.71255015 0.71255015 0.71255015 0.71255015]\n",
      " [0.73462461 0.76830246 0.77053434 0.74956206 0.70381483 0.70585864\n",
      "  0.70585864 0.70585864 0.70585864 0.70585864]\n",
      " [0.61176471 0.7773091  0.73773767 0.6873317  0.68936959 0.68936959\n",
      "  0.68936959 0.68936959 0.68936959 0.66579536]\n",
      " [0.75624167 0.79203743 0.71971236 0.71983431 0.71983431 0.71983431\n",
      "  0.71983431 0.71983431 0.68828512 0.71054864]]\n",
      "|| Epoch:04600 | TR_Loss_tte:2.5798 |  TR_Loss_mle:1.6113 || VA_Loss_tte:1.8641 || VA_Loss_mle:1.3386 ||\n",
      "|| Epoch:04700 | TR_Loss_tte:2.5294 |  TR_Loss_mle:1.5685 || VA_Loss_tte:1.8893 || VA_Loss_mle:1.3545 ||\n",
      "|| Epoch:04800 | TR_Loss_tte:2.5606 |  TR_Loss_mle:1.6224 || VA_Loss_tte:1.8504 || VA_Loss_mle:1.3213 ||\n",
      "|| Epoch:04900 | TR_Loss_tte:2.5754 |  TR_Loss_mle:1.6265 || VA_Loss_tte:1.8599 || VA_Loss_mle:1.3312 ||\n",
      "|| Epoch:05000 | TR_Loss_tte:2.6062 |  TR_Loss_mle:1.6477 || VA_Loss_tte:1.8508 || VA_Loss_mle:1.3147 ||\n",
      "|| Epoch:05100 | TR_Loss_tte:2.5198 |  TR_Loss_mle:1.5987 || VA_Loss_tte:1.8407 || VA_Loss_mle:1.3160 ||\n",
      "saved...\n",
      "[[0.86124865 0.74090049 0.77842866 0.74378276 0.73759971 0.70831937\n",
      "  0.70691803 0.70691803 0.70691803 0.70691803]\n",
      " [0.7439647  0.76950086 0.79131622 0.76197406 0.71150865 0.71246739\n",
      "  0.71246739 0.71246739 0.71246739 0.71246739]\n",
      " [0.57647059 0.82665087 0.76799269 0.70794837 0.71321098 0.71321098\n",
      "  0.71321098 0.71321098 0.71321098 0.71284039]\n",
      " [0.86063744 0.85200331 0.75703684 0.74632576 0.74632576 0.74632576\n",
      "  0.74632576 0.74632576 0.70892074 0.7297104 ]]\n",
      "|| Epoch:05200 | TR_Loss_tte:2.5282 |  TR_Loss_mle:1.6111 || VA_Loss_tte:1.8640 || VA_Loss_mle:1.3316 ||\n",
      "|| Epoch:05300 | TR_Loss_tte:2.5657 |  TR_Loss_mle:1.6250 || VA_Loss_tte:1.8968 || VA_Loss_mle:1.3673 ||\n",
      "|| Epoch:05400 | TR_Loss_tte:2.4958 |  TR_Loss_mle:1.5724 || VA_Loss_tte:1.8651 || VA_Loss_mle:1.3359 ||\n",
      "|| Epoch:05500 | TR_Loss_tte:2.5674 |  TR_Loss_mle:1.6389 || VA_Loss_tte:1.8507 || VA_Loss_mle:1.3227 ||\n",
      "|| Epoch:05600 | TR_Loss_tte:2.5069 |  TR_Loss_mle:1.6028 || VA_Loss_tte:1.8511 || VA_Loss_mle:1.3253 ||\n",
      "|| Epoch:05700 | TR_Loss_tte:2.5032 |  TR_Loss_mle:1.5865 || VA_Loss_tte:1.8566 || VA_Loss_mle:1.3362 ||\n",
      "|| Epoch:05800 | TR_Loss_tte:2.4572 |  TR_Loss_mle:1.5641 || VA_Loss_tte:1.8770 || VA_Loss_mle:1.3565 ||\n",
      "|| Epoch:05900 | TR_Loss_tte:2.4805 |  TR_Loss_mle:1.5776 || VA_Loss_tte:1.8387 || VA_Loss_mle:1.3284 ||\n",
      "saved...\n",
      "[[0.84376916 0.7439276  0.7829209  0.76943318 0.7597215  0.72682702\n",
      "  0.72841607 0.72841607 0.72841607 0.72841607]\n",
      " [0.7471793  0.798146   0.80358727 0.78036219 0.72868225 0.72721898\n",
      "  0.72721898 0.72721898 0.72721898 0.72721898]\n",
      " [0.81176471 0.84980652 0.80681571 0.72701545 0.72924736 0.72924736\n",
      "  0.72924736 0.72924736 0.72924736 0.72550924]\n",
      " [0.80618592 0.8263003  0.71573094 0.686684   0.686684   0.686684\n",
      "  0.686684   0.686684   0.69215759 0.71414452]]\n",
      "|| Epoch:06000 | TR_Loss_tte:2.4989 |  TR_Loss_mle:1.5858 || VA_Loss_tte:1.8952 || VA_Loss_mle:1.3555 ||\n",
      "|| Epoch:06100 | TR_Loss_tte:2.5357 |  TR_Loss_mle:1.6314 || VA_Loss_tte:1.8664 || VA_Loss_mle:1.3426 ||\n",
      "|| Epoch:06200 | TR_Loss_tte:2.5315 |  TR_Loss_mle:1.6434 || VA_Loss_tte:1.8726 || VA_Loss_mle:1.3449 ||\n",
      "|| Epoch:06300 | TR_Loss_tte:2.3880 |  TR_Loss_mle:1.5191 || VA_Loss_tte:1.8667 || VA_Loss_mle:1.3426 ||\n",
      "|| Epoch:06400 | TR_Loss_tte:2.4600 |  TR_Loss_mle:1.5799 || VA_Loss_tte:1.8466 || VA_Loss_mle:1.3223 ||\n",
      "|| Epoch:06500 | TR_Loss_tte:2.4338 |  TR_Loss_mle:1.5649 || VA_Loss_tte:1.9080 || VA_Loss_mle:1.3669 ||\n",
      "|| Epoch:06600 | TR_Loss_tte:2.4688 |  TR_Loss_mle:1.5994 || VA_Loss_tte:1.8916 || VA_Loss_mle:1.3677 ||\n",
      "|| Epoch:06700 | TR_Loss_tte:2.5113 |  TR_Loss_mle:1.6306 || VA_Loss_tte:1.8743 || VA_Loss_mle:1.3395 ||\n",
      "|| Epoch:06800 | TR_Loss_tte:2.4553 |  TR_Loss_mle:1.5895 || VA_Loss_tte:1.8970 || VA_Loss_mle:1.3491 ||\n",
      "|| Epoch:06900 | TR_Loss_tte:2.4217 |  TR_Loss_mle:1.5605 || VA_Loss_tte:1.9126 || VA_Loss_mle:1.3739 ||\n",
      "|| Epoch:07000 | TR_Loss_tte:2.3530 |  TR_Loss_mle:1.4809 || VA_Loss_tte:1.9308 || VA_Loss_mle:1.3749 ||\n",
      "|| Epoch:07100 | TR_Loss_tte:2.4380 |  TR_Loss_mle:1.5811 || VA_Loss_tte:1.9219 || VA_Loss_mle:1.3643 ||\n",
      "|| Epoch:07200 | TR_Loss_tte:2.3468 |  TR_Loss_mle:1.5063 || VA_Loss_tte:1.9281 || VA_Loss_mle:1.3684 ||\n",
      "|| Epoch:07300 | TR_Loss_tte:2.4127 |  TR_Loss_mle:1.5510 || VA_Loss_tte:1.9108 || VA_Loss_mle:1.3506 ||\n",
      "|| Epoch:07400 | TR_Loss_tte:2.4562 |  TR_Loss_mle:1.6163 || VA_Loss_tte:1.9517 || VA_Loss_mle:1.3746 ||\n",
      "|| Epoch:07500 | TR_Loss_tte:2.4178 |  TR_Loss_mle:1.5883 || VA_Loss_tte:1.9631 || VA_Loss_mle:1.4052 ||\n",
      "|| Epoch:07600 | TR_Loss_tte:2.4067 |  TR_Loss_mle:1.5571 || VA_Loss_tte:1.9003 || VA_Loss_mle:1.3499 ||\n",
      "|| Epoch:07700 | TR_Loss_tte:2.3538 |  TR_Loss_mle:1.5226 || VA_Loss_tte:1.9279 || VA_Loss_mle:1.3744 ||\n",
      "|| Epoch:07800 | TR_Loss_tte:2.3441 |  TR_Loss_mle:1.5272 || VA_Loss_tte:1.9301 || VA_Loss_mle:1.3474 ||\n",
      "|| Epoch:07900 | TR_Loss_tte:2.3219 |  TR_Loss_mle:1.5058 || VA_Loss_tte:1.9788 || VA_Loss_mle:1.3864 ||\n",
      "|| Epoch:08000 | TR_Loss_tte:2.3597 |  TR_Loss_mle:1.5330 || VA_Loss_tte:1.9906 || VA_Loss_mle:1.3715 ||\n",
      "|| Epoch:08100 | TR_Loss_tte:2.3333 |  TR_Loss_mle:1.5099 || VA_Loss_tte:1.9578 || VA_Loss_mle:1.3711 ||\n",
      "|| Epoch:08200 | TR_Loss_tte:2.3193 |  TR_Loss_mle:1.5044 || VA_Loss_tte:1.9472 || VA_Loss_mle:1.3587 ||\n",
      "|| Epoch:08300 | TR_Loss_tte:2.3022 |  TR_Loss_mle:1.5045 || VA_Loss_tte:1.9805 || VA_Loss_mle:1.3794 ||\n",
      "|| Epoch:08400 | TR_Loss_tte:2.2893 |  TR_Loss_mle:1.4949 || VA_Loss_tte:1.9467 || VA_Loss_mle:1.3649 ||\n",
      "|| Epoch:08500 | TR_Loss_tte:2.3009 |  TR_Loss_mle:1.4977 || VA_Loss_tte:1.9593 || VA_Loss_mle:1.3615 ||\n",
      "|| Epoch:08600 | TR_Loss_tte:2.3887 |  TR_Loss_mle:1.5834 || VA_Loss_tte:2.0050 || VA_Loss_mle:1.4009 ||\n",
      "|| Epoch:08700 | TR_Loss_tte:2.2575 |  TR_Loss_mle:1.4533 || VA_Loss_tte:1.9944 || VA_Loss_mle:1.3655 ||\n",
      "|| Epoch:08800 | TR_Loss_tte:2.2976 |  TR_Loss_mle:1.4939 || VA_Loss_tte:2.0173 || VA_Loss_mle:1.3690 ||\n",
      "|| Epoch:08900 | TR_Loss_tte:2.3233 |  TR_Loss_mle:1.5225 || VA_Loss_tte:2.0176 || VA_Loss_mle:1.3559 ||\n",
      "|| Epoch:09000 | TR_Loss_tte:2.3017 |  TR_Loss_mle:1.5075 || VA_Loss_tte:2.0007 || VA_Loss_mle:1.3881 ||\n",
      "|| Epoch:09100 | TR_Loss_tte:2.2862 |  TR_Loss_mle:1.4991 || VA_Loss_tte:2.0367 || VA_Loss_mle:1.3916 ||\n",
      "|| Epoch:09200 | TR_Loss_tte:2.3271 |  TR_Loss_mle:1.5245 || VA_Loss_tte:2.0622 || VA_Loss_mle:1.4124 ||\n",
      "|| Epoch:09300 | TR_Loss_tte:2.2651 |  TR_Loss_mle:1.4704 || VA_Loss_tte:2.0064 || VA_Loss_mle:1.3542 ||\n",
      "|| Epoch:09400 | TR_Loss_tte:2.3301 |  TR_Loss_mle:1.5320 || VA_Loss_tte:2.0146 || VA_Loss_mle:1.3675 ||\n",
      "|| Epoch:09500 | TR_Loss_tte:2.2434 |  TR_Loss_mle:1.4587 || VA_Loss_tte:2.0162 || VA_Loss_mle:1.3511 ||\n",
      "|| Epoch:09600 | TR_Loss_tte:2.2077 |  TR_Loss_mle:1.4480 || VA_Loss_tte:2.0304 || VA_Loss_mle:1.3669 ||\n",
      "|| Epoch:09700 | TR_Loss_tte:2.1786 |  TR_Loss_mle:1.4086 || VA_Loss_tte:2.1190 || VA_Loss_mle:1.4527 ||\n",
      "|| Epoch:09800 | TR_Loss_tte:2.2912 |  TR_Loss_mle:1.5057 || VA_Loss_tte:2.1030 || VA_Loss_mle:1.4237 ||\n",
      "|| Epoch:09900 | TR_Loss_tte:2.3105 |  TR_Loss_mle:1.5328 || VA_Loss_tte:2.0366 || VA_Loss_mle:1.3875 ||\n",
      "|| Epoch:10000 | TR_Loss_tte:2.2714 |  TR_Loss_mle:1.5090 || VA_Loss_tte:2.0650 || VA_Loss_mle:1.4098 ||\n",
      "|| Epoch:10100 | TR_Loss_tte:2.2459 |  TR_Loss_mle:1.4924 || VA_Loss_tte:2.0985 || VA_Loss_mle:1.3925 ||\n",
      "|| Epoch:10200 | TR_Loss_tte:2.3069 |  TR_Loss_mle:1.5339 || VA_Loss_tte:2.0894 || VA_Loss_mle:1.3974 ||\n",
      "|| Epoch:10300 | TR_Loss_tte:2.3018 |  TR_Loss_mle:1.5323 || VA_Loss_tte:2.0943 || VA_Loss_mle:1.3827 ||\n",
      "|| Epoch:10400 | TR_Loss_tte:2.2373 |  TR_Loss_mle:1.4823 || VA_Loss_tte:2.0877 || VA_Loss_mle:1.4329 ||\n",
      "|| Epoch:10500 | TR_Loss_tte:2.2152 |  TR_Loss_mle:1.4552 || VA_Loss_tte:2.0784 || VA_Loss_mle:1.3688 ||\n",
      "|| Epoch:10600 | TR_Loss_tte:2.2541 |  TR_Loss_mle:1.4915 || VA_Loss_tte:2.0843 || VA_Loss_mle:1.3728 ||\n",
      "|| Epoch:10700 | TR_Loss_tte:2.2163 |  TR_Loss_mle:1.4492 || VA_Loss_tte:2.0774 || VA_Loss_mle:1.3893 ||\n",
      "|| Epoch:10800 | TR_Loss_tte:2.2148 |  TR_Loss_mle:1.4691 || VA_Loss_tte:2.1145 || VA_Loss_mle:1.4116 ||\n",
      "|| Epoch:10900 | TR_Loss_tte:2.3077 |  TR_Loss_mle:1.5587 || VA_Loss_tte:2.0773 || VA_Loss_mle:1.3878 ||\n",
      "model trained...\n",
      "INFO:tensorflow:Restoring parameters from ./STRATCANS_v1_new2_MICE/p1.0/itr0/models/model_tte\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "MAIN TRAINING ...\n",
      "|| Epoch:00100 | TR_Loss_tte:67.3778 |  TR_Loss_mle:62.9846 || VA_Loss_tte:5.9811 || VA_Loss_mle:2.9174 ||\n",
      "saved...\n",
      "[[0.19020229 0.22979716 0.29023669 0.29023669 0.29023669 0.31053892\n",
      "  0.28526966 0.27108553 0.24125184 0.24125184]\n",
      " [0.73922192 0.63327313 0.63327313 0.63327313 0.57456707 0.58761355\n",
      "  0.59592106 0.55895517 0.55895517 0.55895517]\n",
      " [0.43613147 0.43613147 0.43613147 0.42411878 0.49091422 0.45322834\n",
      "  0.43774366 0.43774366 0.43774366 0.54090069]\n",
      " [0.         0.         0.47826087 0.47413297 0.46817485 0.46862462\n",
      "  0.46862462 0.46862462 0.47748795 0.47748795]]\n",
      "|| Epoch:00200 | TR_Loss_tte:6.5300 |  TR_Loss_mle:3.3908 || VA_Loss_tte:5.7971 || VA_Loss_mle:2.8999 ||\n",
      "saved...\n",
      "[[0.17829017 0.22535486 0.28712047 0.28712047 0.28712047 0.30786039\n",
      "  0.28424897 0.27027099 0.24052695 0.24052695]\n",
      " [0.61048682 0.53286934 0.57412669 0.57626823 0.54325618 0.5525617\n",
      "  0.58120293 0.55684711 0.55684711 0.53077334]\n",
      " [0.47445259 0.47445259 0.47445259 0.47195462 0.46502953 0.46299472\n",
      "  0.45962591 0.45962591 0.45962591 0.53614792]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.47748795]]\n",
      "|| Epoch:00300 | TR_Loss_tte:6.1912 |  TR_Loss_mle:3.2941 || VA_Loss_tte:5.5924 || VA_Loss_mle:2.9399 ||\n",
      "saved...\n",
      "[[0.17233411 0.22190192 0.28469828 0.28469828 0.28469828 0.30577841\n",
      "  0.28617791 0.27181033 0.24189688 0.24189688]\n",
      " [0.61333162 0.53711991 0.53711991 0.53711991 0.51497495 0.54138269\n",
      "  0.56747685 0.54480225 0.54480225 0.54480225]\n",
      " [0.47445259 0.47445259 0.47445259 0.47195462 0.46502953 0.46299472\n",
      "  0.45962591 0.45962591 0.45962591 0.45198763]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00400 | TR_Loss_tte:5.4825 |  TR_Loss_mle:3.0031 || VA_Loss_tte:4.7567 || VA_Loss_mle:2.9567 ||\n",
      "saved...\n",
      "[[0.17233411 0.22182653 0.28587114 0.28587114 0.28587114 0.30678653\n",
      "  0.28685413 0.27672405 0.24626983 0.24626983]\n",
      " [0.61617643 0.54137049 0.54137049 0.54137049 0.51497495 0.54138269\n",
      "  0.57504101 0.55504238 0.55504238 0.55504238]\n",
      " [0.48083944 0.48083944 0.48083944 0.47992726 0.46502953 0.46299472\n",
      "  0.45962591 0.45962591 0.45962591 0.45198763]\n",
      " [0.         0.         0.5        0.5        0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00500 | TR_Loss_tte:5.2733 |  TR_Loss_mle:3.1733 || VA_Loss_tte:4.4907 || VA_Loss_mle:2.9535 ||\n",
      "saved...\n",
      "[[0.16964539 0.21928728 0.28531561 0.28531561 0.28531561 0.30630903\n",
      "  0.28985932 0.30068579 0.26759452 0.26759452]\n",
      " [0.61617643 0.54137049 0.54137049 0.54137049 0.52040687 0.54798301\n",
      "  0.57504101 0.55504238 0.55504238 0.55504238]\n",
      " [0.48083944 0.48083944 0.48083944 0.47992726 0.4760711  0.47697543\n",
      "  0.45962591 0.45962591 0.45962591 0.45198763]\n",
      " [0.         0.         0.5        0.5        0.5        0.5\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00600 | TR_Loss_tte:5.0249 |  TR_Loss_mle:3.1056 || VA_Loss_tte:4.3958 || VA_Loss_mle:2.9305 ||\n",
      "saved...\n",
      "[[0.18473601 0.22548689 0.28837088 0.28837088 0.28837088 0.30893517\n",
      "  0.29191631 0.32297028 0.28742654 0.28742654]\n",
      " [0.61814517 0.5383073  0.5383073  0.5383073  0.51595041 0.54198346\n",
      "  0.58009556 0.55947784 0.55947784 0.54923771]\n",
      " [0.48083944 0.47445259 0.47445259 0.47195462 0.46502953 0.46299472\n",
      "  0.45962591 0.45962591 0.45962591 0.45198763]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00700 | TR_Loss_tte:4.9265 |  TR_Loss_mle:3.1033 || VA_Loss_tte:4.2978 || VA_Loss_mle:2.9044 ||\n",
      "saved...\n",
      "[[0.22624629 0.25689987 0.30659359 0.30659359 0.30659359 0.32459841\n",
      "  0.30427024 0.3672079  0.32679569 0.32679569]\n",
      " [0.61923784 0.54068207 0.53643149 0.53643149 0.51246942 0.54046461\n",
      "  0.57645443 0.55268023 0.55268023 0.55268023]\n",
      " [0.47445259 0.47445259 0.47445259 0.47195462 0.46502953 0.46299472\n",
      "  0.45962591 0.45962591 0.45962591 0.45198763]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00800 | TR_Loss_tte:4.8184 |  TR_Loss_mle:3.0409 || VA_Loss_tte:4.2034 || VA_Loss_mle:2.8842 ||\n",
      "saved...\n",
      "[[0.30747299 0.32174075 0.34703982 0.34703982 0.34703982 0.35936375\n",
      "  0.35419379 0.42769099 0.41299088 0.41299088]\n",
      " [0.62293088 0.53865884 0.53865884 0.53865884 0.51429923 0.54159156\n",
      "  0.57732446 0.5534437  0.5534437  0.55198673]\n",
      " [0.47445259 0.46806573 0.46806573 0.46398198 0.54333013 0.51794325\n",
      "  0.5060902  0.5060902  0.5060902  0.48702339]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:00900 | TR_Loss_tte:4.7434 |  TR_Loss_mle:3.0429 || VA_Loss_tte:4.1220 || VA_Loss_mle:2.8606 ||\n",
      "saved...\n",
      "[[0.44927109 0.42529278 0.41845455 0.41845455 0.41845455 0.42074791\n",
      "  0.40593602 0.49338578 0.47792948 0.47792948]\n",
      " [0.6163652  0.53134507 0.5270945  0.5270945  0.50285889 0.5351707\n",
      "  0.61680129 0.58808527 0.58361379 0.57914231]\n",
      " [0.45529203 0.45529203 0.45529203 0.45188187 0.53881863 0.59284984\n",
      "  0.56943096 0.56943096 0.56943096 0.53478461]\n",
      " [0.         0.         0.48913043 0.48706649 0.48408743 0.48157168\n",
      "  0.48157168 0.48157168 0.48677755 0.48677755]]\n",
      "|| Epoch:01000 | TR_Loss_tte:4.5613 |  TR_Loss_mle:2.9017 || VA_Loss_tte:4.0527 || VA_Loss_mle:2.8103 ||\n",
      "saved...\n",
      "[[0.58591467 0.51600521 0.47575567 0.47575567 0.47575567 0.46389404\n",
      "  0.45423932 0.56040346 0.56346642 0.56346642]\n",
      " [0.61024891 0.51412637 0.51073579 0.5098758  0.51318    0.52541706\n",
      "  0.62812823 0.5944224  0.5944224  0.5944224 ]\n",
      " [0.4468521  0.44046525 0.44046525 0.43460981 0.52172605 0.62673504\n",
      "  0.59354289 0.59354289 0.59354289 0.65837894]\n",
      " [0.         0.         0.47826087 0.47413297 0.46817485 0.46862462\n",
      "  0.46862462 0.46862462 0.47748795 0.47748795]]\n",
      "|| Epoch:01100 | TR_Loss_tte:4.4948 |  TR_Loss_mle:2.8898 || VA_Loss_tte:3.8088 || VA_Loss_mle:2.6143 ||\n",
      "saved...\n",
      "[[0.65871474 0.56824921 0.51757894 0.51757894 0.51757894 0.50594976\n",
      "  0.48802871 0.58736815 0.63277939 0.63277939]\n",
      " [0.65367594 0.57679501 0.61399035 0.61294437 0.58221264 0.5868524\n",
      "  0.676312   0.70856997 0.70856997 0.70856997]\n",
      " [0.4064779  0.40009105 0.40009105 0.38131972 0.46420531 0.60552882\n",
      "  0.65735396 0.65735396 0.65735396 0.7064948 ]\n",
      " [0.         0.         0.42391304 0.55475509 0.71379096 0.76712976\n",
      "  0.76712976 0.76712976 0.69166741 0.69166741]]\n",
      "|| Epoch:01200 | TR_Loss_tte:4.2374 |  TR_Loss_mle:2.6831 || VA_Loss_tte:3.4592 || VA_Loss_mle:2.2740 ||\n",
      "saved...\n",
      "[[0.68718174 0.58275421 0.53190687 0.53190687 0.53190687 0.51826526\n",
      "  0.51369386 0.60784952 0.65100672 0.65100672]\n",
      " [0.78454308 0.64509407 0.64509407 0.64509407 0.57845828 0.58623765\n",
      "  0.68056537 0.71248589 0.71248589 0.71248589]\n",
      " [0.53261269 0.53261269 0.53261269 0.47108851 0.49318122 0.62631616\n",
      "  0.67493166 0.67493166 0.67493166 0.71974902]\n",
      " [0.         0.         0.42391304 0.55475509 0.70625757 0.76100031\n",
      "  0.76100031 0.76100031 0.68726949 0.68726949]]\n",
      "|| Epoch:01300 | TR_Loss_tte:4.1435 |  TR_Loss_mle:2.6053 || VA_Loss_tte:3.2888 || VA_Loss_mle:2.1446 ||\n",
      "saved...\n",
      "[[0.72979101 0.62492921 0.57599445 0.57599445 0.57599445 0.55005376\n",
      "  0.54107683 0.62970175 0.67045405 0.67045405]\n",
      " [0.79917755 0.64282239 0.64114757 0.64114757 0.56357625 0.60347716\n",
      "  0.68893759 0.71983265 0.71874392 0.71709659]\n",
      " [0.5371741  0.5371741  0.5371741  0.46634595 0.47610782 0.60787598\n",
      "  0.65933871 0.65933871 0.65481491 0.70458026]\n",
      " [0.         0.         0.41304348 0.53789483 0.69570485 0.75241423\n",
      "  0.75241423 0.75241423 0.66093083 0.66093083]]\n",
      "|| Epoch:01400 | TR_Loss_tte:3.7029 |  TR_Loss_mle:2.1999 || VA_Loss_tte:2.8903 || VA_Loss_mle:1.7528 ||\n",
      "saved...\n",
      "[[0.7901172  0.67792239 0.62324355 0.62324355 0.62324355 0.60593335\n",
      "  0.60597647 0.68149312 0.71654565 0.71654565]\n",
      " [0.70404243 0.59164518 0.59164518 0.59164518 0.54036881 0.63082791\n",
      "  0.71499014 0.7426942  0.7426942  0.7426942 ]\n",
      " [0.63297689 0.63297689 0.63297689 0.53979346 0.64045347 0.72966649\n",
      "  0.76232435 0.76232435 0.76232435 0.75050859]\n",
      " [0.         0.         0.7826087  0.77972292 0.84706334 0.87556509\n",
      "  0.87556509 0.87556509 0.78964858 0.78964858]]\n",
      "|| Epoch:01500 | TR_Loss_tte:3.3975 |  TR_Loss_mle:1.9372 || VA_Loss_tte:2.7132 || VA_Loss_mle:1.6146 ||\n",
      "saved...\n",
      "[[0.79820977 0.68463987 0.61569841 0.61569841 0.61569841 0.61471483\n",
      "  0.61859259 0.69156107 0.71903191 0.71903191]\n",
      " [0.66004307 0.5840282  0.5840282  0.5840282  0.59231077 0.65208362\n",
      "  0.72680932 0.75306573 0.75306573 0.75306573]\n",
      " [0.67219962 0.67219962 0.67219962 0.72663817 0.75186819 0.80785331\n",
      "  0.82843886 0.82843886 0.82843886 0.73008594]\n",
      " [0.         0.         0.84782609 0.76866855 0.84014449 0.86993566\n",
      "  0.86993566 0.86993566 0.66454089 0.66454089]]\n",
      "|| Epoch:01600 | TR_Loss_tte:3.3466 |  TR_Loss_mle:1.9112 || VA_Loss_tte:2.6980 || VA_Loss_mle:1.6271 ||\n",
      "saved...\n",
      "[[0.81861541 0.69900951 0.62359886 0.62359886 0.62359886 0.61845225\n",
      "  0.62745507 0.69863353 0.73179971 0.73179971]\n",
      " [0.66689661 0.59531269 0.59531269 0.59531269 0.59770126 0.65505883\n",
      "  0.72910625 0.76228622 0.76228622 0.76228622]\n",
      " [0.64254605 0.64254605 0.64254605 0.69978438 0.71814417 0.7901926\n",
      "  0.82258758 0.82258758 0.82258758 0.72567386]\n",
      " [0.         0.         0.86956522 0.78144486 0.84814109 0.87644198\n",
      "  0.87644198 0.87644198 0.66920923 0.66920923]]\n",
      "|| Epoch:01700 | TR_Loss_tte:3.1751 |  TR_Loss_mle:1.7855 || VA_Loss_tte:2.6332 || VA_Loss_mle:1.5982 ||\n",
      "saved...\n",
      "[[0.8185842  0.69924654 0.6264884  0.6264884  0.6264884  0.63009606\n",
      "  0.63733437 0.70651743 0.73881597 0.73881597]\n",
      " [0.67260638 0.60074737 0.60074737 0.60074737 0.62156579 0.66734269\n",
      "  0.73858969 0.7706081  0.7706081  0.7706081 ]\n",
      " [0.61403061 0.61403061 0.61403061 0.68135645 0.70481559 0.78684467\n",
      "  0.81975658 0.81975658 0.81975658 0.72353919]\n",
      " [0.         0.         0.84782609 0.73559661 0.81944499 0.85309378\n",
      "  0.85309378 0.85309378 0.69281291 0.69281291]]\n",
      "|| Epoch:01800 | TR_Loss_tte:3.2242 |  TR_Loss_mle:1.8630 || VA_Loss_tte:2.5805 || VA_Loss_mle:1.5915 ||\n",
      "saved...\n",
      "[[0.82432658 0.71475977 0.63621295 0.63621295 0.63621295 0.64150812\n",
      "  0.6408878  0.70935315 0.74133961 0.74133961]\n",
      " [0.67983154 0.59677607 0.59677607 0.59677607 0.62606321 0.67959692\n",
      "  0.74345951 0.77488145 0.77488145 0.77488145]\n",
      " [0.61425824 0.61425824 0.61425824 0.71226494 0.72533915 0.79524178\n",
      "  0.82685715 0.82685715 0.82685715 0.72889327]\n",
      " [0.         0.         0.89130435 0.79570724 0.85706781 0.8837051\n",
      "  0.8837051  0.8837051  0.71477674 0.71477674]]\n",
      "|| Epoch:01900 | TR_Loss_tte:3.0642 |  TR_Loss_mle:1.7400 || VA_Loss_tte:2.5250 || VA_Loss_mle:1.5657 ||\n",
      "saved...\n",
      "[[0.8185842  0.71174811 0.63539399 0.63539399 0.63539399 0.6408042\n",
      "  0.65316306 0.71914908 0.75005747 0.75005747]\n",
      " [0.69643022 0.60189811 0.60189811 0.60189811 0.63027108 0.6857237\n",
      "  0.74818953 0.77903212 0.77903212 0.77903212]\n",
      " [0.60992446 0.60992446 0.60992446 0.7171546  0.72671885 0.79621001\n",
      "  0.82767588 0.82767588 0.82767588 0.69437294]\n",
      " [0.         0.         0.91304348 0.81031615 0.86621143 0.89114468\n",
      "  0.89114468 0.89114468 0.67975851 0.67975851]]\n",
      "|| Epoch:02000 | TR_Loss_tte:2.9367 |  TR_Loss_mle:1.6557 || VA_Loss_tte:2.4871 || VA_Loss_mle:1.5592 ||\n",
      "saved...\n",
      "[[0.84243966 0.73909113 0.66608206 0.66608206 0.66608206 0.66718196\n",
      "  0.68408456 0.74382513 0.77201786 0.77201786]\n",
      " [0.69806579 0.6094935  0.6094935  0.6094935  0.62875092 0.69371114\n",
      "  0.75435603 0.78444333 0.78444333 0.78444333]\n",
      " [0.60992446 0.60992446 0.60992446 0.69408357 0.72794141 0.80307354\n",
      "  0.83347966 0.83347966 0.83347966 0.73388688]\n",
      " [0.         0.         0.84782609 0.77591026 0.85221042 0.87975294\n",
      "  0.87975294 0.87975294 0.75229722 0.75229722]]\n",
      "|| Epoch:02100 | TR_Loss_tte:2.9907 |  TR_Loss_mle:1.7412 || VA_Loss_tte:2.4586 || VA_Loss_mle:1.5523 ||\n",
      "saved...\n",
      "[[0.83031385 0.73738608 0.66617968 0.66617968 0.66617968 0.67031924\n",
      "  0.69287632 0.75084116 0.77826176 0.77826176]\n",
      " [0.69161652 0.60388382 0.60388382 0.60388382 0.63578235 0.69006583\n",
      "  0.75154176 0.78197376 0.78197376 0.78197376]\n",
      " [0.62247054 0.62247054 0.62247054 0.73295278 0.74749432 0.81078948\n",
      "  0.84000423 0.84000423 0.84000423 0.70366896]\n",
      " [0.         0.         0.95652174 0.83805257 0.88357145 0.90526944\n",
      "  0.90526944 0.90526944 0.68989312 0.68989312]]\n",
      "|| Epoch:02200 | TR_Loss_tte:2.9891 |  TR_Loss_mle:1.7732 || VA_Loss_tte:2.4139 || VA_Loss_mle:1.5400 ||\n",
      "saved...\n",
      "[[0.84824445 0.74666121 0.6740477  0.6740477  0.6740477  0.66792204\n",
      "  0.6981408  0.75504234 0.78200058 0.78200058]\n",
      " [0.71939156 0.62588581 0.62588581 0.62588581 0.63445763 0.6900266\n",
      "  0.75151148 0.78194719 0.78194719 0.78194719]\n",
      " [0.62725957 0.62725957 0.62725957 0.71297662 0.7371971  0.80356326\n",
      "  0.83389376 0.83389376 0.83389376 0.69906145]\n",
      " [0.         0.         0.84782609 0.77704514 0.84538733 0.87420143\n",
      "  0.87420143 0.87420143 0.74831397 0.74831397]]\n",
      "|| Epoch:02300 | TR_Loss_tte:2.9416 |  TR_Loss_mle:1.7278 || VA_Loss_tte:2.3944 || VA_Loss_mle:1.5521 ||\n",
      "saved...\n",
      "[[0.8394172  0.73845909 0.67094934 0.67094934 0.67094934 0.66525886\n",
      "  0.6996799  0.75627057 0.78309364 0.78309364]\n",
      " [0.70621368 0.60942836 0.60942836 0.60942836 0.63257737 0.69550711\n",
      "  0.75574256 0.78566003 0.78566003 0.78566003]\n",
      " [0.61859202 0.61859202 0.61859202 0.72275596 0.74901439 0.8118562\n",
      "  0.84090625 0.84090625 0.84090625 0.70434911]\n",
      " [0.         0.         0.91304348 0.82724396 0.88433982 0.90589461\n",
      "  0.90589461 0.90589461 0.73069786 0.73069786]]\n",
      "|| Epoch:02400 | TR_Loss_tte:2.9050 |  TR_Loss_mle:1.7272 || VA_Loss_tte:2.3820 || VA_Loss_mle:1.5615 ||\n",
      "saved...\n",
      "[[0.84228839 0.73722767 0.66885977 0.66885977 0.66885977 0.66651616\n",
      "  0.70307275 0.75897814 0.78550324 0.78550324]\n",
      " [0.72807585 0.62958417 0.62958417 0.62958417 0.64137588 0.69851207\n",
      "  0.75806247 0.78769579 0.78769579 0.78769579]\n",
      " [0.61471349 0.61471349 0.61471349 0.71255914 0.74445447 0.81466181\n",
      "  0.84327865 0.84327865 0.84327865 0.706138  ]\n",
      " [0.         0.         0.89130435 0.80434441 0.87000715 0.89423302\n",
      "  0.89423302 0.89423302 0.76268677 0.76268677]]\n",
      "|| Epoch:02500 | TR_Loss_tte:2.8039 |  TR_Loss_mle:1.6704 || VA_Loss_tte:2.3382 || VA_Loss_mle:1.5482 ||\n",
      "saved...\n",
      "[[0.84210592 0.74770241 0.67865915 0.67865915 0.67865915 0.66883241\n",
      "  0.70462643 0.76021801 0.78660666 0.78660666]\n",
      " [0.71550162 0.61665975 0.61665975 0.61665975 0.63851812 0.69140604\n",
      "  0.75257644 0.78288171 0.78288171 0.78288171]\n",
      " [0.60604594 0.60604594 0.60604594 0.73002882 0.75127456 0.81944789\n",
      "  0.84732575 0.84732575 0.84732575 0.70918966]\n",
      " [0.         0.         0.82608696 0.78014771 0.85486261 0.87094833\n",
      "  0.87094833 0.87094833 0.70562367 0.70562367]]\n",
      "|| Epoch:02600 | TR_Loss_tte:2.7714 |  TR_Loss_mle:1.6577 || VA_Loss_tte:2.3182 || VA_Loss_mle:1.5497 ||\n",
      "saved...\n",
      "[[0.84473221 0.75603485 0.68586588 0.68586588 0.68586588 0.67502692\n",
      "  0.70290657 0.75884553 0.78538522 0.78538522]\n",
      " [0.72812938 0.6242758  0.6242758  0.6242758  0.63313497 0.69162588\n",
      "  0.75274616 0.78303064 0.78303064 0.78303064]\n",
      " [0.61494112 0.61494112 0.61494112 0.72808693 0.75896795 0.81884126\n",
      "  0.84681278 0.84681278 0.84681278 0.70880286]\n",
      " [0.         0.         0.86956522 0.78982144 0.85338393 0.88070776\n",
      "  0.88070776 0.88070776 0.71262612 0.71262612]]\n",
      "|| Epoch:02700 | TR_Loss_tte:2.7575 |  TR_Loss_mle:1.6513 || VA_Loss_tte:2.2958 || VA_Loss_mle:1.5279 ||\n",
      "saved...\n",
      "[[0.83923473 0.75081575 0.68601786 0.68601786 0.68601786 0.67821093\n",
      "  0.70788726 0.76282022 0.78892249 0.78892249]\n",
      " [0.73800975 0.63041553 0.63041553 0.63041553 0.64205886 0.69725069\n",
      "  0.75708865 0.78684124 0.78684124 0.78684124]\n",
      " [0.60604594 0.60604594 0.60604594 0.73002882 0.75145983 0.81957791\n",
      "  0.8474357  0.8474357  0.8474357  0.70927256]\n",
      " [0.         0.         0.82608696 0.79053153 0.86136177 0.88719882\n",
      "  0.88719882 0.88719882 0.71728351 0.71728351]]\n",
      "|| Epoch:02800 | TR_Loss_tte:2.7128 |  TR_Loss_mle:1.6364 || VA_Loss_tte:2.2799 || VA_Loss_mle:1.5428 ||\n",
      "saved...\n",
      "[[0.83639475 0.76011808 0.68873022 0.68873022 0.68873022 0.67748895\n",
      "  0.7129825  0.76688634 0.79254112 0.79254112]\n",
      " [0.72713725 0.61860779 0.61860779 0.61860779 0.62847859 0.68819742\n",
      "  0.75009931 0.78070798 0.78070798 0.78070798]\n",
      " [0.62292579 0.62292579 0.62292579 0.71786629 0.74422179 0.80849294\n",
      "  0.83806228 0.83806228 0.83806228 0.70220466]\n",
      " [0.         0.         0.7826087  0.77649487 0.85257632 0.88005065\n",
      "  0.88005065 0.88005065 0.71215465 0.71215465]]\n",
      "|| Epoch:02900 | TR_Loss_tte:2.7152 |  TR_Loss_mle:1.6195 || VA_Loss_tte:2.2775 || VA_Loss_mle:1.5389 ||\n",
      "saved...\n",
      "[[0.83920352 0.75596491 0.6832974  0.6832974  0.6832974  0.67587257\n",
      "  0.70982934 0.76437005 0.79030175 0.79030175]\n",
      " [0.74929391 0.6304316  0.6304316  0.6304316  0.63431213 0.6947223\n",
      "  0.75513667 0.78512836 0.78512836 0.78512836]\n",
      " [0.61904727 0.61904727 0.61904727 0.71535982 0.74142601 0.81253654\n",
      "  0.84148154 0.84148154 0.84148154 0.73992059]\n",
      " [0.         0.         0.73913043 0.74151674 0.83068376 0.86223806\n",
      "  0.86223806 0.86223806 0.73973017 0.73973017]]\n",
      "|| Epoch:03000 | TR_Loss_tte:2.7784 |  TR_Loss_mle:1.7006 || VA_Loss_tte:2.2486 || VA_Loss_mle:1.5267 ||\n",
      "saved...\n",
      "[[0.83346114 0.75999421 0.68748554 0.68748554 0.68748554 0.68252584\n",
      "  0.71684166 0.76996604 0.79528189 0.79528189]\n",
      " [0.73855677 0.63083579 0.63083579 0.63083579 0.64628408 0.70360412\n",
      "  0.76199365 0.79114547 0.79114547 0.79114547]\n",
      " [0.61471349 0.61471349 0.61471349 0.74332052 0.75082768 0.81913429\n",
      "  0.84706057 0.84706057 0.84706057 0.7089897 ]\n",
      " [0.         0.         0.82608696 0.75545234 0.83940595 0.84740969\n",
      "  0.84740969 0.84740969 0.68873453 0.68873453]]\n",
      "|| Epoch:03100 | TR_Loss_tte:2.6604 |  TR_Loss_mle:1.6060 || VA_Loss_tte:2.2798 || VA_Loss_mle:1.5666 ||\n",
      "|| Epoch:03200 | TR_Loss_tte:2.6356 |  TR_Loss_mle:1.6167 || VA_Loss_tte:2.2612 || VA_Loss_mle:1.5556 ||\n",
      "|| Epoch:03300 | TR_Loss_tte:2.6825 |  TR_Loss_mle:1.6577 || VA_Loss_tte:2.2415 || VA_Loss_mle:1.5314 ||\n",
      "saved...\n",
      "[[0.85698286 0.75872204 0.69646698 0.69646698 0.69646698 0.68413904\n",
      "  0.71537426 0.76879503 0.79423975 0.79423975]\n",
      " [0.72764155 0.63249119 0.63249119 0.63249119 0.63212417 0.69044267\n",
      "  0.74724195 0.7782006  0.7782006  0.7782006 ]\n",
      " [0.63136572 0.63136572 0.63136572 0.7156302  0.73732714 0.80365452\n",
      "  0.82488837 0.82488837 0.82488837 0.72740875]\n",
      " [0.         0.         0.80434783 0.77487369 0.84402824 0.86213309\n",
      "  0.86213309 0.86213309 0.73965485 0.73965485]]\n",
      "|| Epoch:03400 | TR_Loss_tte:2.6450 |  TR_Loss_mle:1.6387 || VA_Loss_tte:2.2006 || VA_Loss_mle:1.5053 ||\n",
      "saved...\n",
      "[[0.85444541 0.7710805  0.69744212 0.69744212 0.69744212 0.68803059\n",
      "  0.71750406 0.77049465 0.79575233 0.79575233]\n",
      " [0.7678402  0.65003651 0.65003651 0.65003651 0.65429795 0.70465979\n",
      "  0.76280866 0.78465575 0.78465575 0.78465575]\n",
      " [0.61904727 0.61904727 0.61904727 0.72305016 0.74864986 0.81760598\n",
      "  0.83668568 0.83668568 0.83668568 0.73630434]\n",
      " [0.         0.         0.73913043 0.69692609 0.79524139 0.8224383\n",
      "  0.8224383  0.8224383  0.71117357 0.71117357]]\n",
      "|| Epoch:03500 | TR_Loss_tte:2.6085 |  TR_Loss_mle:1.6026 || VA_Loss_tte:2.2632 || VA_Loss_mle:1.5722 ||\n",
      "|| Epoch:03600 | TR_Loss_tte:2.5896 |  TR_Loss_mle:1.6068 || VA_Loss_tte:2.2220 || VA_Loss_mle:1.5514 ||\n",
      "|| Epoch:03700 | TR_Loss_tte:2.6361 |  TR_Loss_mle:1.6491 || VA_Loss_tte:2.2250 || VA_Loss_mle:1.5550 ||\n",
      "|| Epoch:03800 | TR_Loss_tte:2.6095 |  TR_Loss_mle:1.6328 || VA_Loss_tte:2.1900 || VA_Loss_mle:1.5259 ||\n",
      "saved...\n",
      "[[0.82979763 0.75242656 0.69218667 0.69218667 0.69218667 0.68045992\n",
      "  0.71320186 0.7670614  0.79269691 0.79269691]\n",
      " [0.73698376 0.63142638 0.63142638 0.63142638 0.63900933 0.69877903\n",
      "  0.75826857 0.78067174 0.78067174 0.78067174]\n",
      " [0.62794245 0.62794245 0.62794245 0.73648897 0.74795795 0.81712042\n",
      "  0.8362751  0.8362751  0.8362751  0.73599475]\n",
      " [0.         0.         0.86956522 0.77437503 0.85124953 0.86800859\n",
      "  0.86800859 0.86800859 0.74387057 0.74387057]]\n",
      "|| Epoch:03900 | TR_Loss_tte:2.5264 |  TR_Loss_mle:1.5551 || VA_Loss_tte:2.1882 || VA_Loss_mle:1.5194 ||\n",
      "saved...\n",
      "[[0.83291371 0.76330521 0.69859217 0.69859217 0.69859217 0.68596574\n",
      "  0.71356953 0.76735481 0.79295803 0.79295803]\n",
      " [0.76501209 0.6485115  0.6485115  0.6485115  0.64916516 0.7055945\n",
      "  0.75893953 0.78846542 0.78846542 0.78846542]\n",
      " [0.62794245 0.62794245 0.62794245 0.72879862 0.72019764 0.79763924\n",
      "  0.82888443 0.82888443 0.82888443 0.73042192]\n",
      " [0.         0.         0.76086957 0.67497249 0.78903418 0.82835042\n",
      "  0.82835042 0.82835042 0.67505938 0.67505938]]\n",
      "|| Epoch:04000 | TR_Loss_tte:2.5416 |  TR_Loss_mle:1.5771 || VA_Loss_tte:2.1856 || VA_Loss_mle:1.5288 ||\n",
      "saved...\n",
      "[[0.85325693 0.77204147 0.70703613 0.70703613 0.70703613 0.69627707\n",
      "  0.72636107 0.77756274 0.79556887 0.79556887]\n",
      " [0.76877813 0.66255179 0.66255179 0.66255179 0.66069956 0.71213766\n",
      "  0.76858175 0.78972173 0.78972173 0.78972173]\n",
      " [0.64094378 0.64094378 0.64094378 0.76027169 0.77224293 0.82815715\n",
      "  0.84560772 0.84560772 0.84560772 0.74303188]\n",
      " [0.         0.         0.76086957 0.67183037 0.77143239 0.77017882\n",
      "  0.77017882 0.77017882 0.67367704 0.67367704]]\n",
      "|| Epoch:04100 | TR_Loss_tte:2.5165 |  TR_Loss_mle:1.5594 || VA_Loss_tte:2.1955 || VA_Loss_mle:1.5348 ||\n",
      "|| Epoch:04200 | TR_Loss_tte:2.5534 |  TR_Loss_mle:1.5997 || VA_Loss_tte:2.1999 || VA_Loss_mle:1.5487 ||\n",
      "|| Epoch:04300 | TR_Loss_tte:2.5165 |  TR_Loss_mle:1.5832 || VA_Loss_tte:2.2060 || VA_Loss_mle:1.5453 ||\n",
      "|| Epoch:04400 | TR_Loss_tte:2.4789 |  TR_Loss_mle:1.5447 || VA_Loss_tte:2.2030 || VA_Loss_mle:1.5573 ||\n",
      "|| Epoch:04500 | TR_Loss_tte:2.4317 |  TR_Loss_mle:1.5102 || VA_Loss_tte:2.1951 || VA_Loss_mle:1.5527 ||\n",
      "|| Epoch:04600 | TR_Loss_tte:2.5298 |  TR_Loss_mle:1.5879 || VA_Loss_tte:2.2241 || VA_Loss_mle:1.5752 ||\n",
      "|| Epoch:04700 | TR_Loss_tte:2.5211 |  TR_Loss_mle:1.6123 || VA_Loss_tte:2.2085 || VA_Loss_mle:1.5768 ||\n",
      "|| Epoch:04800 | TR_Loss_tte:2.4443 |  TR_Loss_mle:1.5302 || VA_Loss_tte:2.1590 || VA_Loss_mle:1.5357 ||\n",
      "saved...\n",
      "[[0.81461816 0.75262844 0.69368992 0.69368992 0.69368992 0.68480541\n",
      "  0.71866618 0.77142205 0.79657766 0.79657766]\n",
      " [0.81401801 0.68476722 0.68476722 0.68476722 0.67895002 0.72277466\n",
      "  0.78138453 0.80095638 0.80095638 0.80095638]\n",
      " [0.63638238 0.63638238 0.63638238 0.71118184 0.73682614 0.80930852\n",
      "  0.82966938 0.82966938 0.82966938 0.69587611]\n",
      " [0.         0.         0.65217391 0.69790637 0.80338834 0.82906696\n",
      "  0.82906696 0.82906696 0.6755735  0.6755735 ]]\n",
      "|| Epoch:04900 | TR_Loss_tte:2.5434 |  TR_Loss_mle:1.6162 || VA_Loss_tte:2.1844 || VA_Loss_mle:1.5597 ||\n",
      "|| Epoch:05000 | TR_Loss_tte:2.4956 |  TR_Loss_mle:1.6006 || VA_Loss_tte:2.2046 || VA_Loss_mle:1.5584 ||\n",
      "|| Epoch:05100 | TR_Loss_tte:2.4233 |  TR_Loss_mle:1.5206 || VA_Loss_tte:2.1993 || VA_Loss_mle:1.5680 ||\n",
      "|| Epoch:05200 | TR_Loss_tte:2.4891 |  TR_Loss_mle:1.6060 || VA_Loss_tte:2.2147 || VA_Loss_mle:1.5794 ||\n",
      "|| Epoch:05300 | TR_Loss_tte:2.4511 |  TR_Loss_mle:1.5500 || VA_Loss_tte:2.1777 || VA_Loss_mle:1.5630 ||\n",
      "|| Epoch:05400 | TR_Loss_tte:2.4257 |  TR_Loss_mle:1.5457 || VA_Loss_tte:2.1893 || VA_Loss_mle:1.5569 ||\n",
      "|| Epoch:05500 | TR_Loss_tte:2.4132 |  TR_Loss_mle:1.5281 || VA_Loss_tte:2.1928 || VA_Loss_mle:1.5776 ||\n",
      "|| Epoch:05600 | TR_Loss_tte:2.4279 |  TR_Loss_mle:1.5447 || VA_Loss_tte:2.2101 || VA_Loss_mle:1.5893 ||\n",
      "|| Epoch:05700 | TR_Loss_tte:2.4685 |  TR_Loss_mle:1.5744 || VA_Loss_tte:2.1752 || VA_Loss_mle:1.5519 ||\n",
      "|| Epoch:05800 | TR_Loss_tte:2.4419 |  TR_Loss_mle:1.5733 || VA_Loss_tte:2.1825 || VA_Loss_mle:1.5665 ||\n",
      "|| Epoch:05900 | TR_Loss_tte:2.4908 |  TR_Loss_mle:1.6109 || VA_Loss_tte:2.2062 || VA_Loss_mle:1.5826 ||\n",
      "|| Epoch:06000 | TR_Loss_tte:2.4690 |  TR_Loss_mle:1.5984 || VA_Loss_tte:2.1779 || VA_Loss_mle:1.5560 ||\n",
      "|| Epoch:06100 | TR_Loss_tte:2.3937 |  TR_Loss_mle:1.5462 || VA_Loss_tte:2.1927 || VA_Loss_mle:1.5741 ||\n",
      "|| Epoch:06200 | TR_Loss_tte:2.3635 |  TR_Loss_mle:1.5004 || VA_Loss_tte:2.1944 || VA_Loss_mle:1.5647 ||\n",
      "|| Epoch:06300 | TR_Loss_tte:2.3645 |  TR_Loss_mle:1.5180 || VA_Loss_tte:2.2450 || VA_Loss_mle:1.6220 ||\n",
      "|| Epoch:06400 | TR_Loss_tte:2.3634 |  TR_Loss_mle:1.5390 || VA_Loss_tte:2.2060 || VA_Loss_mle:1.5756 ||\n",
      "|| Epoch:06500 | TR_Loss_tte:2.4170 |  TR_Loss_mle:1.5624 || VA_Loss_tte:2.2251 || VA_Loss_mle:1.5874 ||\n",
      "|| Epoch:06600 | TR_Loss_tte:2.3405 |  TR_Loss_mle:1.4975 || VA_Loss_tte:2.2329 || VA_Loss_mle:1.5988 ||\n",
      "|| Epoch:06700 | TR_Loss_tte:2.3296 |  TR_Loss_mle:1.5052 || VA_Loss_tte:2.2436 || VA_Loss_mle:1.5957 ||\n",
      "|| Epoch:06800 | TR_Loss_tte:2.4903 |  TR_Loss_mle:1.6344 || VA_Loss_tte:2.2040 || VA_Loss_mle:1.5845 ||\n",
      "|| Epoch:06900 | TR_Loss_tte:2.3609 |  TR_Loss_mle:1.5368 || VA_Loss_tte:2.2930 || VA_Loss_mle:1.6496 ||\n",
      "|| Epoch:07000 | TR_Loss_tte:2.3382 |  TR_Loss_mle:1.5170 || VA_Loss_tte:2.2767 || VA_Loss_mle:1.6131 ||\n",
      "|| Epoch:07100 | TR_Loss_tte:2.3686 |  TR_Loss_mle:1.5491 || VA_Loss_tte:2.3096 || VA_Loss_mle:1.6578 ||\n",
      "|| Epoch:07200 | TR_Loss_tte:2.3265 |  TR_Loss_mle:1.5198 || VA_Loss_tte:2.3321 || VA_Loss_mle:1.6778 ||\n",
      "|| Epoch:07300 | TR_Loss_tte:2.3022 |  TR_Loss_mle:1.5079 || VA_Loss_tte:2.2811 || VA_Loss_mle:1.6280 ||\n",
      "|| Epoch:07400 | TR_Loss_tte:2.3586 |  TR_Loss_mle:1.5436 || VA_Loss_tte:2.3604 || VA_Loss_mle:1.6865 ||\n",
      "|| Epoch:07500 | TR_Loss_tte:2.3416 |  TR_Loss_mle:1.5492 || VA_Loss_tte:2.3335 || VA_Loss_mle:1.6499 ||\n",
      "|| Epoch:07600 | TR_Loss_tte:2.3071 |  TR_Loss_mle:1.5046 || VA_Loss_tte:2.2880 || VA_Loss_mle:1.6210 ||\n",
      "|| Epoch:07700 | TR_Loss_tte:2.3240 |  TR_Loss_mle:1.5203 || VA_Loss_tte:2.3489 || VA_Loss_mle:1.6729 ||\n",
      "|| Epoch:07800 | TR_Loss_tte:2.2974 |  TR_Loss_mle:1.5050 || VA_Loss_tte:2.3332 || VA_Loss_mle:1.6566 ||\n",
      "|| Epoch:07900 | TR_Loss_tte:2.2708 |  TR_Loss_mle:1.4843 || VA_Loss_tte:2.3776 || VA_Loss_mle:1.6639 ||\n",
      "|| Epoch:08000 | TR_Loss_tte:2.3389 |  TR_Loss_mle:1.5582 || VA_Loss_tte:2.3187 || VA_Loss_mle:1.6109 ||\n",
      "|| Epoch:08100 | TR_Loss_tte:2.3211 |  TR_Loss_mle:1.5377 || VA_Loss_tte:2.3444 || VA_Loss_mle:1.6320 ||\n",
      "|| Epoch:08200 | TR_Loss_tte:2.2852 |  TR_Loss_mle:1.5198 || VA_Loss_tte:2.3252 || VA_Loss_mle:1.6158 ||\n",
      "|| Epoch:08300 | TR_Loss_tte:2.2808 |  TR_Loss_mle:1.5011 || VA_Loss_tte:2.3432 || VA_Loss_mle:1.6436 ||\n",
      "|| Epoch:08400 | TR_Loss_tte:2.2642 |  TR_Loss_mle:1.4922 || VA_Loss_tte:2.4184 || VA_Loss_mle:1.7026 ||\n",
      "|| Epoch:08500 | TR_Loss_tte:2.2697 |  TR_Loss_mle:1.5030 || VA_Loss_tte:2.3569 || VA_Loss_mle:1.6441 ||\n",
      "|| Epoch:08600 | TR_Loss_tte:2.2610 |  TR_Loss_mle:1.5076 || VA_Loss_tte:2.3911 || VA_Loss_mle:1.6640 ||\n",
      "|| Epoch:08700 | TR_Loss_tte:2.2716 |  TR_Loss_mle:1.4878 || VA_Loss_tte:2.3761 || VA_Loss_mle:1.6427 ||\n",
      "|| Epoch:08800 | TR_Loss_tte:2.2507 |  TR_Loss_mle:1.4889 || VA_Loss_tte:2.4513 || VA_Loss_mle:1.7016 ||\n",
      "|| Epoch:08900 | TR_Loss_tte:2.1348 |  TR_Loss_mle:1.3901 || VA_Loss_tte:2.4564 || VA_Loss_mle:1.6980 ||\n",
      "|| Epoch:09000 | TR_Loss_tte:2.2252 |  TR_Loss_mle:1.4688 || VA_Loss_tte:2.4402 || VA_Loss_mle:1.6993 ||\n",
      "|| Epoch:09100 | TR_Loss_tte:2.2802 |  TR_Loss_mle:1.5309 || VA_Loss_tte:2.3679 || VA_Loss_mle:1.6404 ||\n",
      "|| Epoch:09200 | TR_Loss_tte:2.2529 |  TR_Loss_mle:1.5009 || VA_Loss_tte:2.4322 || VA_Loss_mle:1.7031 ||\n",
      "|| Epoch:09300 | TR_Loss_tte:2.2767 |  TR_Loss_mle:1.5169 || VA_Loss_tte:2.4597 || VA_Loss_mle:1.7118 ||\n",
      "|| Epoch:09400 | TR_Loss_tte:2.1844 |  TR_Loss_mle:1.4338 || VA_Loss_tte:2.4436 || VA_Loss_mle:1.6806 ||\n",
      "|| Epoch:09500 | TR_Loss_tte:2.2237 |  TR_Loss_mle:1.4762 || VA_Loss_tte:2.5555 || VA_Loss_mle:1.7585 ||\n",
      "|| Epoch:09600 | TR_Loss_tte:2.1441 |  TR_Loss_mle:1.4212 || VA_Loss_tte:2.4805 || VA_Loss_mle:1.6951 ||\n",
      "|| Epoch:09700 | TR_Loss_tte:2.1480 |  TR_Loss_mle:1.4300 || VA_Loss_tte:2.5205 || VA_Loss_mle:1.7518 ||\n",
      "|| Epoch:09800 | TR_Loss_tte:2.1814 |  TR_Loss_mle:1.4395 || VA_Loss_tte:2.4920 || VA_Loss_mle:1.7270 ||\n",
      "model trained...\n",
      "INFO:tensorflow:Restoring parameters from ./STRATCANS_v1_new2_MICE/p1.0/itr1/models/model_tte\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "MAIN TRAINING ...\n",
      "|| Epoch:00100 | TR_Loss_tte:87.3256 |  TR_Loss_mle:83.1022 || VA_Loss_tte:7.1704 || VA_Loss_mle:3.4969 ||\n",
      "saved...\n",
      "[[0.32290088 0.22610622 0.27071708 0.27400264 0.26744608 0.30517381\n",
      "  0.28954926 0.32222472 0.32222472 0.32222472]\n",
      " [0.53862089 0.54265877 0.48343331 0.4791826  0.50923377 0.58406746\n",
      "  0.56262285 0.56262285 0.56262285 0.56262285]\n",
      " [0.47514666 0.47320231 0.47362481 0.47340581 0.52516198 0.51622175\n",
      "  0.51622175 0.51622175 0.51622175 0.51622175]\n",
      " [0.49298977 0.49224374 0.49194709 0.55624118 0.54812858 0.54812858\n",
      "  0.54812858 0.54812858 0.54812858 0.54812858]]\n",
      "|| Epoch:00200 | TR_Loss_tte:6.0310 |  TR_Loss_mle:3.0419 || VA_Loss_tte:6.6058 || VA_Loss_mle:3.3660 ||\n",
      "saved...\n",
      "[[0.32275054 0.22595238 0.27058624 0.27831945 0.27110823 0.3085747\n",
      "  0.29074291 0.32332926 0.32332926 0.32332926]\n",
      " [0.53360907 0.54244583 0.50117563 0.47439801 0.50487757 0.55779973\n",
      "  0.54067609 0.54067609 0.55914286 0.55699474]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.55624118 0.54812858 0.54812858\n",
      "  0.54812858 0.54812858 0.54812858 0.54812858]]\n",
      "|| Epoch:00300 | TR_Loss_tte:5.5605 |  TR_Loss_mle:2.8436 || VA_Loss_tte:6.3546 || VA_Loss_mle:3.3799 ||\n",
      "saved...\n",
      "[[0.32815534 0.22975005 0.27287017 0.2846079  0.27644301 0.3135289\n",
      "  0.29291156 0.32872913 0.32872913 0.32872913]\n",
      " [0.54890305 0.52046922 0.49197118 0.47361788 0.50416729 0.56181958\n",
      "  0.54634578 0.54634578 0.54634578 0.54634578]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00400 | TR_Loss_tte:5.1447 |  TR_Loss_mle:2.6537 || VA_Loss_tte:5.7061 || VA_Loss_mle:3.3956 ||\n",
      "saved...\n",
      "[[0.33346159 0.23255302 0.27620022 0.28818289 0.27947583 0.31634536\n",
      "  0.29521138 0.33085728 0.33085728 0.33085728]\n",
      " [0.56037354 0.52916424 0.50202607 0.48503936 0.47808851 0.54261988\n",
      "  0.55250131 0.55250131 0.55309098 0.55309098]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00500 | TR_Loss_tte:4.8628 |  TR_Loss_mle:2.6827 || VA_Loss_tte:5.3126 || VA_Loss_mle:3.4072 ||\n",
      "saved...\n",
      "[[0.33547773 0.23485517 0.27815824 0.29330242 0.28511516 0.32158239\n",
      "  0.29948776 0.33481443 0.33481443 0.33481443]\n",
      " [0.56037354 0.53351175 0.50705352 0.4907501  0.48411697 0.54846596\n",
      "  0.5361742  0.5361742  0.5361742  0.53143337]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00600 | TR_Loss_tte:4.5606 |  TR_Loss_mle:2.6314 || VA_Loss_tte:5.1473 || VA_Loss_mle:3.4095 ||\n",
      "saved...\n",
      "[[0.33755424 0.23702162 0.27908938 0.29937033 0.29285523 0.32877029\n",
      "  0.30535715 0.34024568 0.34024568 0.34024568]\n",
      " [0.56419704 0.53351175 0.50705352 0.4907501  0.48411697 0.54846596\n",
      "  0.5361742  0.5361742  0.5361742  0.5361742 ]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00700 | TR_Loss_tte:4.4017 |  TR_Loss_mle:2.6053 || VA_Loss_tte:5.0277 || VA_Loss_mle:3.4086 ||\n",
      "saved...\n",
      "[[0.34286979 0.24392387 0.28401382 0.3086893  0.29946474 0.33358718\n",
      "  0.30929046 0.3472785  0.3472785  0.3472785 ]\n",
      " [0.56419704 0.53785926 0.50705352 0.4907501  0.48411697 0.54846596\n",
      "  0.5361742  0.5361742  0.5361742  0.5361742 ]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00800 | TR_Loss_tte:4.3449 |  TR_Loss_mle:2.6367 || VA_Loss_tte:4.9225 || VA_Loss_mle:3.4048 ||\n",
      "saved...\n",
      "[[0.36056484 0.25296592 0.29166967 0.32194515 0.31200647 0.3452342\n",
      "  0.31880101 0.35947222 0.35947222 0.35947222]\n",
      " [0.56419704 0.53785926 0.51208096 0.4907501  0.48411697 0.54846596\n",
      "  0.5361742  0.5361742  0.5361742  0.5361742 ]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:00900 | TR_Loss_tte:4.1603 |  TR_Loss_mle:2.5229 || VA_Loss_tte:4.8296 || VA_Loss_mle:3.3964 ||\n",
      "saved...\n",
      "[[0.37673152 0.26469668 0.30168152 0.34334294 0.33793635 0.36931426\n",
      "  0.33846397 0.3844536  0.3844536  0.3844536 ]\n",
      " [0.5656551  0.53899761 0.50790119 0.49143321 0.48473891 0.54895112\n",
      "  0.53661657 0.53661657 0.53661657 0.53661657]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:01000 | TR_Loss_tte:4.0748 |  TR_Loss_mle:2.5231 || VA_Loss_tte:4.7396 || VA_Loss_mle:3.3797 ||\n",
      "saved...\n",
      "[[0.39104744 0.28778748 0.31855169 0.37027072 0.37244615 0.40004107\n",
      "  0.36355442 0.4110642  0.4110642  0.4110642 ]\n",
      " [0.5618316  0.5346501  0.50790119 0.49143321 0.48473891 0.54895112\n",
      "  0.53187574 0.53187574 0.53128607 0.53187574]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:01100 | TR_Loss_tte:3.9943 |  TR_Loss_mle:2.4747 || VA_Loss_tte:4.6954 || VA_Loss_mle:3.3609 ||\n",
      "saved...\n",
      "[[0.40380226 0.31033709 0.33678453 0.39395219 0.40938675 0.43170416\n",
      "  0.38624261 0.43205875 0.43205875 0.43205875]\n",
      " [0.55800811 0.52595508 0.4978463  0.51744373 0.50759151 0.56498748\n",
      "  0.55123866 0.55123866 0.57284389 0.57284389]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:01200 | TR_Loss_tte:4.1231 |  TR_Loss_mle:2.6436 || VA_Loss_tte:4.6379 || VA_Loss_mle:3.3369 ||\n",
      "saved...\n",
      "[[0.43585876 0.34900618 0.36778125 0.42349851 0.45130276 0.47062992\n",
      "  0.41298446 0.45680436 0.45680436 0.45680436]\n",
      " [0.54653762 0.51291255 0.48276396 0.51265959 0.53639721 0.58235094\n",
      "  0.60314933 0.60314933 0.60314933 0.60314933]\n",
      " [0.48135999 0.47990173 0.4809991  0.48107442 0.53370283 0.52655863\n",
      "  0.52655863 0.52655863 0.52655863 0.52655863]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:01300 | TR_Loss_tte:3.8676 |  TR_Loss_mle:2.4356 || VA_Loss_tte:4.5734 || VA_Loss_mle:3.3204 ||\n",
      "saved...\n",
      "[[0.47274592 0.38504819 0.39752424 0.44849613 0.48287893 0.50127457\n",
      "  0.42601321 0.46886053 0.46886053 0.46886053]\n",
      " [0.54388636 0.51084262 0.48182253 0.51141747 0.55088878 0.59644637\n",
      "  0.62475845 0.62502809 0.6250766  0.6250766 ]\n",
      " [0.47514666 0.47320231 0.47362481 0.47340581 0.51662113 0.57206346\n",
      "  0.57206346 0.57206346 0.57206346 0.57206346]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]]\n",
      "|| Epoch:01400 | TR_Loss_tte:3.8659 |  TR_Loss_mle:2.4944 || VA_Loss_tte:3.9202 || VA_Loss_mle:2.6737 ||\n",
      "saved...\n",
      "[[0.57466425 0.45727436 0.46259995 0.50745925 0.54326957 0.55735699\n",
      "  0.46271764 0.50282502 0.50282502 0.50282502]\n",
      " [0.6463925  0.64601895 0.6651003  0.65461871 0.65939268 0.69247882\n",
      "  0.71959971 0.71959971 0.71959971 0.71959971]\n",
      " [0.43481782 0.6534523  0.66920613 0.63555831 0.6276229  0.67476678\n",
      "  0.67476678 0.67476678 0.67476678 0.67476678]\n",
      " [0.80499754 0.76066496 0.70373554 0.61443096 0.67004801 0.67004801\n",
      "  0.67004801 0.67004801 0.71723996 0.71723996]]\n",
      "|| Epoch:01500 | TR_Loss_tte:3.4894 |  TR_Loss_mle:2.1110 || VA_Loss_tte:3.7674 || VA_Loss_mle:2.5491 ||\n",
      "saved...\n",
      "[[0.60697473 0.50802601 0.50303084 0.54111963 0.58219475 0.5882209\n",
      "  0.49364233 0.53144123 0.53144123 0.53144123]\n",
      " [0.69685446 0.65196143 0.67328648 0.66204528 0.6657752  0.69788285\n",
      "  0.72353185 0.72321877 0.72321877 0.72321877]\n",
      " [0.65035194 0.73465008 0.72386653 0.6787928  0.64696212 0.69165761\n",
      "  0.69165761 0.69165761 0.69165761 0.69165761]\n",
      " [0.80499754 0.87891603 0.79682013 0.72973656 0.76872116 0.76872116\n",
      "  0.76872116 0.76872116 0.76872116 0.76872116]]\n",
      "|| Epoch:01600 | TR_Loss_tte:3.3131 |  TR_Loss_mle:1.9604 || VA_Loss_tte:3.3508 || VA_Loss_mle:2.1784 ||\n",
      "saved...\n",
      "[[0.66779263 0.60008864 0.58147032 0.60913367 0.64507892 0.63869239\n",
      "  0.54436212 0.57837486 0.57837486 0.57837486]\n",
      " [0.76677257 0.6906088  0.72761753 0.74565244 0.72863192 0.74920224\n",
      "  0.77132058 0.77132058 0.77132058 0.77132058]\n",
      " [0.69913307 0.77091948 0.81713255 0.70689676 0.66594047 0.70823325\n",
      "  0.70823325 0.70823325 0.70823325 0.70823325]\n",
      " [0.83099343 0.89644133 0.85474609 0.66920137 0.71691797 0.71524003\n",
      "  0.71524003 0.71524003 0.71524003 0.71524003]]\n",
      "|| Epoch:01700 | TR_Loss_tte:2.9971 |  TR_Loss_mle:1.6748 || VA_Loss_tte:3.0860 || VA_Loss_mle:1.9634 ||\n",
      "saved...\n",
      "[[0.68815865 0.62713217 0.59889878 0.62682114 0.660084   0.65130593\n",
      "  0.56419905 0.59673098 0.59673098 0.59673098]\n",
      " [0.79015601 0.69627378 0.74503449 0.76798471 0.73735813 0.74313797\n",
      "  0.76579114 0.76579114 0.76579114 0.76579114]\n",
      " [0.57764302 0.77512638 0.81668079 0.70651553 0.6642795  0.70678256\n",
      "  0.70678256 0.70678256 0.70678256 0.70678256]\n",
      " [0.92966937 0.95551318 0.94236366 0.76500156 0.79889929 0.79889929\n",
      "  0.79889929 0.79889929 0.79889929 0.79889929]]\n",
      "|| Epoch:01800 | TR_Loss_tte:2.9010 |  TR_Loss_mle:1.6128 || VA_Loss_tte:2.9539 || VA_Loss_mle:1.8801 ||\n",
      "saved...\n",
      "[[0.75347154 0.67031303 0.62181451 0.64006718 0.6739136  0.66018566\n",
      "  0.58441662 0.61543934 0.61543934 0.61543934]\n",
      " [0.82793682 0.7190854  0.76564012 0.78790859 0.75384004 0.74769759\n",
      "  0.76994863 0.76994863 0.76994863 0.76994863]\n",
      " [0.49860507 0.75003186 0.80699662 0.7128005  0.68599186 0.72574609\n",
      "  0.72574609 0.72574609 0.72574609 0.72574609]\n",
      " [0.9061751  0.94342424 0.93631567 0.77303631 0.80577506 0.80577506\n",
      "  0.80577506 0.80577506 0.80577506 0.80577506]]\n",
      "|| Epoch:01900 | TR_Loss_tte:2.8345 |  TR_Loss_mle:1.5837 || VA_Loss_tte:2.8364 || VA_Loss_mle:1.8202 ||\n",
      "saved...\n",
      "[[0.81881639 0.74340333 0.685906   0.70094685 0.73074535 0.71296308\n",
      "  0.64614054 0.67255566 0.67255566 0.67255566]\n",
      " [0.83506561 0.72747732 0.77428887 0.79487825 0.75355336 0.75887203\n",
      "  0.78013758 0.78013758 0.78013758 0.78013758]\n",
      " [0.49227604 0.74183102 0.79554733 0.72048739 0.69109004 0.73019882\n",
      "  0.73019882 0.73019882 0.73019882 0.73019882]\n",
      " [0.92959328 0.95132487 0.94265696 0.81487244 0.84157647 0.84157647\n",
      "  0.84157647 0.84157647 0.84157647 0.84157647]]\n",
      "|| Epoch:02000 | TR_Loss_tte:2.7584 |  TR_Loss_mle:1.5435 || VA_Loss_tte:2.8183 || VA_Loss_mle:1.8354 ||\n",
      "saved...\n",
      "[[0.8154869  0.74925994 0.68082637 0.6950346  0.72443353 0.70710154\n",
      "  0.65076879 0.67683842 0.67683842 0.67683842]\n",
      " [0.83113523 0.71623445 0.76951652 0.79601042 0.75126801 0.75371763\n",
      "  0.77543775 0.77543775 0.77543775 0.77543775]\n",
      " [0.49273882 0.74774387 0.80242237 0.7291804  0.69523708 0.73382084\n",
      "  0.73382084 0.73382084 0.73382084 0.73382084]\n",
      " [0.88260474 0.92299783 0.91261085 0.80946132 0.83694589 0.83694589\n",
      "  0.83694589 0.83694589 0.83694589 0.83694589]]\n",
      "|| Epoch:02100 | TR_Loss_tte:2.6790 |  TR_Loss_mle:1.5114 || VA_Loss_tte:2.7290 || VA_Loss_mle:1.7944 ||\n",
      "saved...\n",
      "[[0.81523418 0.76828269 0.69244823 0.71782386 0.74376669 0.7197711\n",
      "  0.67120151 0.69574586 0.69574586 0.69574586]\n",
      " [0.82902385 0.70779947 0.76323544 0.78597076 0.74544341 0.75297097\n",
      "  0.77475694 0.77475694 0.77475694 0.77475694]\n",
      " [0.49906785 0.76167707 0.81438814 0.74216929 0.7332414  0.76701372\n",
      "  0.76701372 0.76701372 0.76701372 0.76701372]\n",
      " [0.90609901 0.93923593 0.91833413 0.82554796 0.85071208 0.85071208\n",
      "  0.85071208 0.85071208 0.85071208 0.85071208]]\n",
      "|| Epoch:02200 | TR_Loss_tte:2.6898 |  TR_Loss_mle:1.5459 || VA_Loss_tte:2.6502 || VA_Loss_mle:1.7591 ||\n",
      "saved...\n",
      "[[0.82292331 0.78743966 0.70881079 0.74177011 0.76408138 0.73863656\n",
      "  0.68660639 0.71000079 0.71000079 0.71000079]\n",
      " [0.833732   0.71008752 0.76493923 0.79398112 0.75273655 0.75676165\n",
      "  0.77821331 0.77821331 0.77821331 0.77821331]\n",
      " [0.51103174 0.76061095 0.81058945 0.73318088 0.72489765 0.7597263\n",
      "  0.7597263  0.7597263  0.7597263  0.7597263 ]\n",
      " [0.94368984 0.95027991 0.93085334 0.81151453 0.83870293 0.83870293\n",
      "  0.83870293 0.83870293 0.83870293 0.83870293]]\n",
      "|| Epoch:02300 | TR_Loss_tte:2.6254 |  TR_Loss_mle:1.5018 || VA_Loss_tte:2.6628 || VA_Loss_mle:1.7934 ||\n",
      "|| Epoch:02400 | TR_Loss_tte:2.5913 |  TR_Loss_mle:1.4927 || VA_Loss_tte:2.6501 || VA_Loss_mle:1.7997 ||\n",
      "saved...\n",
      "[[0.83342366 0.81217673 0.73268836 0.7620137  0.78125493 0.75194277\n",
      "  0.71160593 0.73313415 0.73313415 0.73313415]\n",
      " [0.83391398 0.71310658 0.76358775 0.78625467 0.74735998 0.74497372\n",
      "  0.76746498 0.76746498 0.76746498 0.76746498]\n",
      " [0.49227604 0.74756337 0.78981983 0.72721983 0.72427374 0.75918138\n",
      "  0.75918138 0.75918138 0.75918138 0.75918138]\n",
      " [0.93436823 0.92888684 0.91733755 0.83424587 0.85815535 0.85815535\n",
      "  0.85815535 0.85815535 0.85815535 0.85815535]]\n",
      "|| Epoch:02500 | TR_Loss_tte:2.5108 |  TR_Loss_mle:1.4222 || VA_Loss_tte:2.5996 || VA_Loss_mle:1.7720 ||\n",
      "saved...\n",
      "[[0.82224084 0.81089943 0.73545541 0.76158543 0.77959542 0.74643836\n",
      "  0.71376885 0.73513561 0.73513561 0.73513561]\n",
      " [0.84220512 0.71546722 0.76176542 0.78810481 0.74572831 0.73920046\n",
      "  0.76220088 0.76220088 0.76220088 0.76220088]\n",
      " [0.51172591 0.74664363 0.78923604 0.71805294 0.71302023 0.7493526\n",
      "  0.7493526  0.7493526  0.7493526  0.7493526 ]\n",
      " [0.91580109 0.91103483 0.899354   0.82910787 0.85375848 0.85375848\n",
      "  0.85375848 0.85375848 0.85375848 0.85375848]]\n",
      "|| Epoch:02600 | TR_Loss_tte:2.5052 |  TR_Loss_mle:1.4415 || VA_Loss_tte:2.5221 || VA_Loss_mle:1.7238 ||\n",
      "saved...\n",
      "[[0.82446099 0.81961152 0.74019998 0.76986827 0.78532593 0.75968664\n",
      "  0.73121393 0.75127844 0.75127844 0.75127844]\n",
      " [0.81915165 0.70163188 0.75266286 0.78076943 0.73904972 0.71514576\n",
      "  0.74026761 0.74026761 0.74026761 0.74026761]\n",
      " [0.53611648 0.7676213  0.79942914 0.72665452 0.70456088 0.74196422\n",
      "  0.74196422 0.74196422 0.74196422 0.74196422]\n",
      " [0.92997375 0.89343239 0.88888068 0.8271834  0.85211162 0.85211162\n",
      "  0.85211162 0.85211162 0.85211162 0.85211162]]\n",
      "|| Epoch:02700 | TR_Loss_tte:2.5438 |  TR_Loss_mle:1.4863 || VA_Loss_tte:2.5354 || VA_Loss_mle:1.7539 ||\n",
      "|| Epoch:02800 | TR_Loss_tte:2.4698 |  TR_Loss_mle:1.4409 || VA_Loss_tte:2.4949 || VA_Loss_mle:1.7237 ||\n",
      "saved...\n",
      "[[0.80488088 0.81096693 0.73291676 0.76574575 0.77275525 0.74933383\n",
      "  0.73821499 0.75775688 0.75775688 0.75775688]\n",
      " [0.84798727 0.72563401 0.77413556 0.78811736 0.75403011 0.74121271\n",
      "  0.76403567 0.76403567 0.76403567 0.76403567]\n",
      " [0.5119573  0.76670429 0.79572509 0.74376874 0.73758684 0.77080901\n",
      "  0.77080901 0.77080901 0.77080901 0.77080901]\n",
      " [0.93459651 0.9290043  0.9320517  0.86581558 0.88517123 0.88517123\n",
      "  0.88517123 0.88517123 0.88517123 0.88517123]]\n",
      "|| Epoch:02900 | TR_Loss_tte:2.4595 |  TR_Loss_mle:1.4374 || VA_Loss_tte:2.5282 || VA_Loss_mle:1.7718 ||\n",
      "|| Epoch:03000 | TR_Loss_tte:2.4773 |  TR_Loss_mle:1.4578 || VA_Loss_tte:2.5650 || VA_Loss_mle:1.8171 ||\n",
      "|| Epoch:03100 | TR_Loss_tte:2.4428 |  TR_Loss_mle:1.4459 || VA_Loss_tte:2.4937 || VA_Loss_mle:1.7511 ||\n",
      "saved...\n",
      "[[0.80298807 0.81575705 0.73597561 0.7682106  0.77743867 0.74971986\n",
      "  0.73503938 0.75481832 0.75481832 0.75481832]\n",
      " [0.85229837 0.7345001  0.78433727 0.79799782 0.76302591 0.75237872\n",
      "  0.77421692 0.77421692 0.77421692 0.77421692]\n",
      " [0.52438397 0.76323098 0.80288647 0.74113771 0.73866817 0.77175344\n",
      "  0.77175344 0.77175344 0.77175344 0.77175344]\n",
      " [0.90670776 0.91880336 0.92751906 0.87625429 0.8941042  0.8941042\n",
      "  0.8941042  0.8941042  0.8941042  0.8941042 ]]\n",
      "|| Epoch:03200 | TR_Loss_tte:2.3746 |  TR_Loss_mle:1.3605 || VA_Loss_tte:2.4962 || VA_Loss_mle:1.7702 ||\n",
      "|| Epoch:03300 | TR_Loss_tte:2.3561 |  TR_Loss_mle:1.3981 || VA_Loss_tte:2.4540 || VA_Loss_mle:1.7373 ||\n",
      "saved...\n",
      "[[0.81392157 0.81584615 0.73419386 0.76410985 0.77266363 0.74528547\n",
      "  0.7330018  0.75293285 0.75293285 0.75293285]\n",
      " [0.84269619 0.72011535 0.77604488 0.789656   0.75543099 0.73625835\n",
      "  0.75550951 0.75550951 0.75550951 0.75550951]\n",
      " [0.53048161 0.77416134 0.80358027 0.73883176 0.72231425 0.75171531\n",
      "  0.75171531 0.75171531 0.75171531 0.75171531]\n",
      " [0.92535099 0.9076504  0.90760244 0.83479509 0.85862535 0.85862535\n",
      "  0.85862535 0.85862535 0.85862535 0.85862535]]\n",
      "|| Epoch:03400 | TR_Loss_tte:2.3400 |  TR_Loss_mle:1.3765 || VA_Loss_tte:2.4802 || VA_Loss_mle:1.7663 ||\n",
      "|| Epoch:03500 | TR_Loss_tte:2.3741 |  TR_Loss_mle:1.4205 || VA_Loss_tte:2.4730 || VA_Loss_mle:1.7651 ||\n",
      "|| Epoch:03600 | TR_Loss_tte:2.3749 |  TR_Loss_mle:1.3916 || VA_Loss_tte:2.4521 || VA_Loss_mle:1.7570 ||\n",
      "saved...\n",
      "[[0.80452367 0.81504961 0.74000046 0.77503418 0.78063504 0.7526882\n",
      "  0.74095405 0.76029147 0.76029147 0.76029147]\n",
      " [0.85516887 0.73941522 0.7892166  0.79861119 0.76192629 0.75113244\n",
      "  0.76907183 0.76907183 0.76907183 0.76907183]\n",
      " [0.53657926 0.77663241 0.80202673 0.74330363 0.7360555  0.76371688\n",
      "  0.76371688 0.76371688 0.76371688 0.76371688]\n",
      " [0.92088041 0.90949923 0.9163963  0.84486828 0.86724551 0.86724551\n",
      "  0.86724551 0.86724551 0.86724551 0.86724551]]\n",
      "|| Epoch:03700 | TR_Loss_tte:2.3425 |  TR_Loss_mle:1.4057 || VA_Loss_tte:2.4750 || VA_Loss_mle:1.7923 ||\n",
      "|| Epoch:03800 | TR_Loss_tte:2.3226 |  TR_Loss_mle:1.3780 || VA_Loss_tte:2.4258 || VA_Loss_mle:1.7455 ||\n",
      "saved...\n",
      "[[0.7926348  0.81651952 0.73939311 0.77632259 0.78302425 0.75358587\n",
      "  0.74327046 0.76243496 0.76243496 0.76243496]\n",
      " [0.84533165 0.73173498 0.7823171  0.79968852 0.76456524 0.75772818\n",
      "  0.77508588 0.77508588 0.77508588 0.77508588]\n",
      " [0.53048161 0.77712034 0.81170242 0.75146859 0.74606604 0.7667054\n",
      "  0.7667054  0.7667054  0.7667054  0.7667054 ]\n",
      " [0.91618156 0.90708144 0.90714578 0.85504161 0.86939471 0.86939471\n",
      "  0.86939471 0.86939471 0.86939471 0.86939471]]\n",
      "|| Epoch:03900 | TR_Loss_tte:2.2522 |  TR_Loss_mle:1.3261 || VA_Loss_tte:2.4111 || VA_Loss_mle:1.7397 ||\n",
      "saved...\n",
      "[[0.79804057 0.81474452 0.73791803 0.77603176 0.77888894 0.74974557\n",
      "  0.74359479 0.76273508 0.76273508 0.76273508]\n",
      " [0.85241101 0.73592505 0.78541778 0.80218724 0.7684983  0.76269469\n",
      "  0.77961438 0.77961438 0.77961438 0.77961438]\n",
      " [0.53657926 0.77371981 0.815788   0.75491626 0.75459483 0.78566374\n",
      "  0.78566374 0.78566374 0.78566374 0.78566374]\n",
      " [0.90663166 0.91461504 0.91684745 0.85373068 0.87482954 0.87482954\n",
      "  0.87482954 0.87482954 0.87482954 0.87482954]]\n",
      "|| Epoch:04000 | TR_Loss_tte:2.3555 |  TR_Loss_mle:1.4033 || VA_Loss_tte:2.4372 || VA_Loss_mle:1.7683 ||\n",
      "|| Epoch:04100 | TR_Loss_tte:2.2287 |  TR_Loss_mle:1.3194 || VA_Loss_tte:2.3986 || VA_Loss_mle:1.7355 ||\n",
      "saved...\n",
      "[[0.7871472  0.81197008 0.73191242 0.76852191 0.76992561 0.74274277\n",
      "  0.73787654 0.7574437  0.7574437  0.7574437 ]\n",
      " [0.84344591 0.72198696 0.77517485 0.78895488 0.75645071 0.74914809\n",
      "  0.75924503 0.75924503 0.75924503 0.75924503]\n",
      " [0.53657926 0.76831236 0.79986777 0.75015602 0.7451861  0.7601822\n",
      "  0.7601822  0.7601822  0.7601822  0.7601822 ]\n",
      " [0.87010614 0.8750751  0.88511143 0.8267365  0.84517251 0.84517251\n",
      "  0.84517251 0.84517251 0.84517251 0.84517251]]\n",
      "|| Epoch:04200 | TR_Loss_tte:2.3594 |  TR_Loss_mle:1.4463 || VA_Loss_tte:2.4171 || VA_Loss_mle:1.7655 ||\n",
      "|| Epoch:04300 | TR_Loss_tte:2.3454 |  TR_Loss_mle:1.4314 || VA_Loss_tte:2.3721 || VA_Loss_mle:1.7160 ||\n",
      "saved...\n",
      "[[0.77864108 0.812244   0.7321108  0.77218639 0.77433055 0.74551237\n",
      "  0.73826131 0.75779974 0.75779974 0.75779974]\n",
      " [0.87139864 0.74658606 0.79695608 0.80484811 0.77092093 0.76268604\n",
      "  0.7796065  0.7796065  0.7796065  0.7796065 ]\n",
      " [0.53048161 0.774347   0.80057612 0.74786235 0.7492837  0.77527035\n",
      "  0.77527035 0.77527035 0.77527035 0.77527035]\n",
      " [0.90238937 0.89583554 0.89446449 0.84770586 0.86967378 0.86967378\n",
      "  0.86967378 0.86967378 0.86967378 0.86967378]]\n",
      "|| Epoch:04400 | TR_Loss_tte:2.2662 |  TR_Loss_mle:1.3648 || VA_Loss_tte:2.4489 || VA_Loss_mle:1.7879 ||\n",
      "|| Epoch:04500 | TR_Loss_tte:2.2957 |  TR_Loss_mle:1.3882 || VA_Loss_tte:2.3921 || VA_Loss_mle:1.7596 ||\n",
      "|| Epoch:04600 | TR_Loss_tte:2.3623 |  TR_Loss_mle:1.4600 || VA_Loss_tte:2.3891 || VA_Loss_mle:1.7439 ||\n",
      "|| Epoch:04700 | TR_Loss_tte:2.2517 |  TR_Loss_mle:1.3642 || VA_Loss_tte:2.4254 || VA_Loss_mle:1.7781 ||\n",
      "|| Epoch:04800 | TR_Loss_tte:2.2294 |  TR_Loss_mle:1.3430 || VA_Loss_tte:2.4678 || VA_Loss_mle:1.8272 ||\n",
      "|| Epoch:04900 | TR_Loss_tte:2.2778 |  TR_Loss_mle:1.4060 || VA_Loss_tte:2.4574 || VA_Loss_mle:1.8045 ||\n",
      "|| Epoch:05000 | TR_Loss_tte:2.2584 |  TR_Loss_mle:1.3640 || VA_Loss_tte:2.4158 || VA_Loss_mle:1.7882 ||\n",
      "|| Epoch:05100 | TR_Loss_tte:2.2777 |  TR_Loss_mle:1.3903 || VA_Loss_tte:2.4791 || VA_Loss_mle:1.8526 ||\n",
      "|| Epoch:05200 | TR_Loss_tte:2.2836 |  TR_Loss_mle:1.4126 || VA_Loss_tte:2.4123 || VA_Loss_mle:1.7897 ||\n",
      "|| Epoch:05300 | TR_Loss_tte:2.2407 |  TR_Loss_mle:1.3588 || VA_Loss_tte:2.4333 || VA_Loss_mle:1.8111 ||\n",
      "|| Epoch:05400 | TR_Loss_tte:2.2247 |  TR_Loss_mle:1.3569 || VA_Loss_tte:2.4426 || VA_Loss_mle:1.8087 ||\n",
      "|| Epoch:05500 | TR_Loss_tte:2.2223 |  TR_Loss_mle:1.3539 || VA_Loss_tte:2.4912 || VA_Loss_mle:1.8425 ||\n",
      "|| Epoch:05600 | TR_Loss_tte:2.3254 |  TR_Loss_mle:1.4687 || VA_Loss_tte:2.3854 || VA_Loss_mle:1.7559 ||\n",
      "|| Epoch:05700 | TR_Loss_tte:2.1781 |  TR_Loss_mle:1.3208 || VA_Loss_tte:2.4531 || VA_Loss_mle:1.8275 ||\n",
      "|| Epoch:05800 | TR_Loss_tte:2.2331 |  TR_Loss_mle:1.3674 || VA_Loss_tte:2.4303 || VA_Loss_mle:1.7850 ||\n",
      "|| Epoch:05900 | TR_Loss_tte:2.1700 |  TR_Loss_mle:1.3377 || VA_Loss_tte:2.4122 || VA_Loss_mle:1.7907 ||\n",
      "|| Epoch:06000 | TR_Loss_tte:2.1953 |  TR_Loss_mle:1.3606 || VA_Loss_tte:2.4059 || VA_Loss_mle:1.7717 ||\n",
      "|| Epoch:06100 | TR_Loss_tte:2.1543 |  TR_Loss_mle:1.3215 || VA_Loss_tte:2.4236 || VA_Loss_mle:1.8057 ||\n",
      "|| Epoch:06200 | TR_Loss_tte:2.1771 |  TR_Loss_mle:1.3496 || VA_Loss_tte:2.4408 || VA_Loss_mle:1.8172 ||\n",
      "|| Epoch:06300 | TR_Loss_tte:2.1860 |  TR_Loss_mle:1.3803 || VA_Loss_tte:2.4258 || VA_Loss_mle:1.8079 ||\n",
      "|| Epoch:06400 | TR_Loss_tte:2.1581 |  TR_Loss_mle:1.3145 || VA_Loss_tte:2.4077 || VA_Loss_mle:1.7814 ||\n",
      "|| Epoch:06500 | TR_Loss_tte:2.1568 |  TR_Loss_mle:1.3589 || VA_Loss_tte:2.4137 || VA_Loss_mle:1.7926 ||\n",
      "|| Epoch:06600 | TR_Loss_tte:2.1366 |  TR_Loss_mle:1.3323 || VA_Loss_tte:2.4887 || VA_Loss_mle:1.8435 ||\n",
      "|| Epoch:06700 | TR_Loss_tte:2.1027 |  TR_Loss_mle:1.3137 || VA_Loss_tte:2.4250 || VA_Loss_mle:1.8022 ||\n",
      "|| Epoch:06800 | TR_Loss_tte:2.1888 |  TR_Loss_mle:1.3936 || VA_Loss_tte:2.4534 || VA_Loss_mle:1.8178 ||\n",
      "|| Epoch:06900 | TR_Loss_tte:2.0849 |  TR_Loss_mle:1.2878 || VA_Loss_tte:2.5335 || VA_Loss_mle:1.9070 ||\n",
      "|| Epoch:07000 | TR_Loss_tte:2.1557 |  TR_Loss_mle:1.3618 || VA_Loss_tte:2.4221 || VA_Loss_mle:1.8034 ||\n",
      "|| Epoch:07100 | TR_Loss_tte:2.1037 |  TR_Loss_mle:1.3157 || VA_Loss_tte:2.4385 || VA_Loss_mle:1.7931 ||\n",
      "|| Epoch:07200 | TR_Loss_tte:2.1712 |  TR_Loss_mle:1.3897 || VA_Loss_tte:2.4318 || VA_Loss_mle:1.8053 ||\n",
      "|| Epoch:07300 | TR_Loss_tte:2.0944 |  TR_Loss_mle:1.3075 || VA_Loss_tte:2.4703 || VA_Loss_mle:1.8260 ||\n",
      "|| Epoch:07400 | TR_Loss_tte:2.1565 |  TR_Loss_mle:1.3549 || VA_Loss_tte:2.5446 || VA_Loss_mle:1.8997 ||\n",
      "|| Epoch:07500 | TR_Loss_tte:2.0933 |  TR_Loss_mle:1.3062 || VA_Loss_tte:2.4476 || VA_Loss_mle:1.8033 ||\n",
      "|| Epoch:07600 | TR_Loss_tte:2.0859 |  TR_Loss_mle:1.3073 || VA_Loss_tte:2.4883 || VA_Loss_mle:1.8295 ||\n",
      "|| Epoch:07700 | TR_Loss_tte:2.0587 |  TR_Loss_mle:1.2878 || VA_Loss_tte:2.4580 || VA_Loss_mle:1.7972 ||\n",
      "|| Epoch:07800 | TR_Loss_tte:2.0380 |  TR_Loss_mle:1.2627 || VA_Loss_tte:2.4425 || VA_Loss_mle:1.7809 ||\n",
      "|| Epoch:07900 | TR_Loss_tte:2.0575 |  TR_Loss_mle:1.2864 || VA_Loss_tte:2.5372 || VA_Loss_mle:1.8654 ||\n",
      "|| Epoch:08000 | TR_Loss_tte:2.1026 |  TR_Loss_mle:1.3333 || VA_Loss_tte:2.5451 || VA_Loss_mle:1.8494 ||\n",
      "|| Epoch:08100 | TR_Loss_tte:1.9699 |  TR_Loss_mle:1.2246 || VA_Loss_tte:2.5925 || VA_Loss_mle:1.9226 ||\n",
      "|| Epoch:08200 | TR_Loss_tte:1.9894 |  TR_Loss_mle:1.2474 || VA_Loss_tte:2.5428 || VA_Loss_mle:1.8651 ||\n",
      "|| Epoch:08300 | TR_Loss_tte:2.0250 |  TR_Loss_mle:1.2693 || VA_Loss_tte:2.5843 || VA_Loss_mle:1.8855 ||\n",
      "|| Epoch:08400 | TR_Loss_tte:2.0233 |  TR_Loss_mle:1.2657 || VA_Loss_tte:2.6064 || VA_Loss_mle:1.9034 ||\n",
      "|| Epoch:08500 | TR_Loss_tte:2.0619 |  TR_Loss_mle:1.3186 || VA_Loss_tte:2.5050 || VA_Loss_mle:1.8209 ||\n",
      "|| Epoch:08600 | TR_Loss_tte:1.9999 |  TR_Loss_mle:1.2500 || VA_Loss_tte:2.5563 || VA_Loss_mle:1.8366 ||\n",
      "|| Epoch:08700 | TR_Loss_tte:1.9472 |  TR_Loss_mle:1.2368 || VA_Loss_tte:2.5123 || VA_Loss_mle:1.7982 ||\n",
      "|| Epoch:08800 | TR_Loss_tte:2.0612 |  TR_Loss_mle:1.3163 || VA_Loss_tte:2.4828 || VA_Loss_mle:1.7675 ||\n",
      "|| Epoch:08900 | TR_Loss_tte:1.9819 |  TR_Loss_mle:1.2485 || VA_Loss_tte:2.5621 || VA_Loss_mle:1.8510 ||\n",
      "|| Epoch:09000 | TR_Loss_tte:2.0384 |  TR_Loss_mle:1.2889 || VA_Loss_tte:2.5428 || VA_Loss_mle:1.8025 ||\n",
      "|| Epoch:09100 | TR_Loss_tte:2.0342 |  TR_Loss_mle:1.2873 || VA_Loss_tte:2.5929 || VA_Loss_mle:1.8362 ||\n",
      "|| Epoch:09200 | TR_Loss_tte:2.0141 |  TR_Loss_mle:1.2722 || VA_Loss_tte:2.6349 || VA_Loss_mle:1.8903 ||\n",
      "|| Epoch:09300 | TR_Loss_tte:1.9795 |  TR_Loss_mle:1.2660 || VA_Loss_tte:2.5582 || VA_Loss_mle:1.8360 ||\n",
      "model trained...\n",
      "INFO:tensorflow:Restoring parameters from ./STRATCANS_v1_new2_MICE/p1.0/itr2/models/model_tte\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "MAIN TRAINING ...\n",
      "|| Epoch:00100 | TR_Loss_tte:71.0019 |  TR_Loss_mle:66.5601 || VA_Loss_tte:6.2732 || VA_Loss_mle:3.1557 ||\n",
      "saved...\n",
      "[[0.         0.28850943 0.25580471 0.2667008  0.22889731 0.23188436\n",
      "  0.24083264 0.24083264 0.24083264 0.24083264]\n",
      " [0.65035409 0.66547314 0.56116952 0.51633515 0.49546486 0.53670043\n",
      "  0.53670043 0.53181355 0.53181355 0.53181355]\n",
      " [0.4625     0.44971625 0.44410725 0.44042695 0.49201623 0.49201623\n",
      "  0.49201623 0.49201623 0.49201623 0.49201623]\n",
      " [0.47056346 0.46714316 0.46494038 0.45957332 0.45957332 0.45957332\n",
      "  0.45957332 0.45957332 0.45957332 0.40276839]]\n",
      "|| Epoch:00200 | TR_Loss_tte:6.5765 |  TR_Loss_mle:3.4080 || VA_Loss_tte:6.0224 || VA_Loss_mle:3.1400 ||\n",
      "saved...\n",
      "[[0.         0.28846653 0.25360062 0.26139009 0.22448209 0.2278687\n",
      "  0.23717118 0.23717118 0.23717118 0.23717118]\n",
      " [0.60022987 0.55956486 0.51979423 0.49056656 0.47130973 0.5146759\n",
      "  0.5146759  0.5146759  0.5146759  0.5146759 ]\n",
      " [0.475      0.47126643 0.46806129 0.46595826 0.50458076 0.50690283\n",
      "  0.50690283 0.50690283 0.50690283 0.49945953]\n",
      " [0.4779226  0.47535737 0.47370528 0.46967999 0.46967999 0.46967999\n",
      "  0.46967999 0.46967999 0.46967999 0.40276839]]\n",
      "|| Epoch:00300 | TR_Loss_tte:5.9060 |  TR_Loss_mle:3.1261 || VA_Loss_tte:5.3452 || VA_Loss_mle:3.1545 ||\n",
      "saved...\n",
      "[[0.         0.27813268 0.24722824 0.26077551 0.225829   0.22909371\n",
      "  0.23828815 0.23828815 0.23828815 0.23828815]\n",
      " [0.60608895 0.57250299 0.53477412 0.50819852 0.49057808 0.53224468\n",
      "  0.53224468 0.53224468 0.53224468 0.53224468]\n",
      " [0.48125    0.47844982 0.47604596 0.47446869 0.46093853 0.46093853\n",
      "  0.46093853 0.46093853 0.46093853 0.46093853]\n",
      " [0.48528173 0.48357158 0.48247019 0.47978666 0.46967999 0.46967999\n",
      "  0.46967999 0.46967999 0.46967999 0.40804745]]\n",
      "|| Epoch:00400 | TR_Loss_tte:5.4061 |  TR_Loss_mle:3.1019 || VA_Loss_tte:5.0446 || VA_Loss_mle:3.1588 ||\n",
      "saved...\n",
      "[[0.         0.28013116 0.25104864 0.26523895 0.23139765 0.23200376\n",
      "  0.23440782 0.23440782 0.23440782 0.23440782]\n",
      " [0.60608895 0.57250299 0.53477412 0.50819852 0.49668324 0.5345445\n",
      "  0.5345445  0.5345445  0.5345445  0.52897784]\n",
      " [0.48125    0.47844982 0.47604596 0.47446869 0.4707039  0.4707039\n",
      "  0.4707039  0.4707039  0.4707039  0.46093853]\n",
      " [0.48528173 0.48357158 0.48247019 0.47978666 0.47978666 0.47978666\n",
      "  0.47978666 0.47978666 0.47978666 0.40804745]]\n",
      "|| Epoch:00500 | TR_Loss_tte:5.2368 |  TR_Loss_mle:3.2111 || VA_Loss_tte:4.9404 || VA_Loss_mle:3.1558 ||\n",
      "saved...\n",
      "[[0.         0.28588876 0.25580735 0.27845515 0.24610103 0.24537653\n",
      "  0.24660106 0.24660106 0.24660106 0.24660106]\n",
      " [0.60608895 0.57250299 0.53477412 0.50819852 0.49668324 0.5345445\n",
      "  0.5345445  0.5345445  0.5345445  0.52897784]\n",
      " [0.48125    0.47844982 0.47604596 0.47446869 0.4707039  0.4707039\n",
      "  0.4707039  0.4707039  0.4707039  0.46093853]\n",
      " [0.48528173 0.48357158 0.48247019 0.47978666 0.47978666 0.47978666\n",
      "  0.47978666 0.47978666 0.47978666 0.40804745]]\n",
      "|| Epoch:00600 | TR_Loss_tte:4.9659 |  TR_Loss_mle:3.1013 || VA_Loss_tte:4.7728 || VA_Loss_mle:3.1465 ||\n",
      "saved...\n",
      "[[0.         0.30167305 0.27319048 0.29563354 0.26383479 0.25935079\n",
      "  0.25934272 0.25934272 0.25934272 0.25934272]\n",
      " [0.60608895 0.57250299 0.53477412 0.50819852 0.49057808 0.52897784\n",
      "  0.52897784 0.52897784 0.52897784 0.52897784]\n",
      " [0.48125    0.47844982 0.47604596 0.47446869 0.46093853 0.46093853\n",
      "  0.46093853 0.46093853 0.46093853 0.46093853]\n",
      " [0.48528173 0.48357158 0.48247019 0.47978666 0.46967999 0.46967999\n",
      "  0.46967999 0.46967999 0.46967999 0.40804745]]\n",
      "|| Epoch:00700 | TR_Loss_tte:4.9406 |  TR_Loss_mle:3.1921 || VA_Loss_tte:4.6496 || VA_Loss_mle:3.1277 ||\n",
      "saved...\n",
      "[[0.         0.3238321  0.29584241 0.31707126 0.30501859 0.29680762\n",
      "  0.29349567 0.29349567 0.29349567 0.29349567]\n",
      " [0.68860321 0.64042762 0.56737622 0.53357725 0.51868796 0.5546083\n",
      "  0.55403021 0.55403021 0.54875426 0.54875426]\n",
      " [0.475      0.47126643 0.46806129 0.46595826 0.46093853 0.46093853\n",
      "  0.46093853 0.46093853 0.46093853 0.46093853]\n",
      " [0.4779226  0.47535737 0.47370528 0.46967999 0.46967999 0.46967999\n",
      "  0.46967999 0.46967999 0.46967999 0.40804745]]\n",
      "|| Epoch:00800 | TR_Loss_tte:4.8371 |  TR_Loss_mle:3.1628 || VA_Loss_tte:4.5091 || VA_Loss_mle:3.0987 ||\n",
      "saved...\n",
      "[[0.         0.35978273 0.33206184 0.37278967 0.38399191 0.36647949\n",
      "  0.35702215 0.35702215 0.35702215 0.35702215]\n",
      " [0.68969187 0.63673748 0.56151885 0.52525556 0.50896468 0.54085578\n",
      "  0.54085578 0.54046671 0.54046671 0.54046671]\n",
      " [0.475      0.47126643 0.46806129 0.46595826 0.50458076 0.50458076\n",
      "  0.49790229 0.49713746 0.49713746 0.49713746]\n",
      " [0.4779226  0.47535737 0.47370528 0.46967999 0.46967999 0.46967999\n",
      "  0.45957332 0.45957332 0.45957332 0.40276839]]\n",
      "|| Epoch:00900 | TR_Loss_tte:4.6335 |  TR_Loss_mle:3.0288 || VA_Loss_tte:4.3700 || VA_Loss_mle:3.0473 ||\n",
      "saved...\n",
      "[[0.         0.36428828 0.33144844 0.42447968 0.48087934 0.45244432\n",
      "  0.42887079 0.42887079 0.42887079 0.42887079]\n",
      " [0.67328037 0.6898435  0.58871463 0.54268759 0.54147814 0.57164987\n",
      "  0.57242021 0.57242021 0.56877523 0.56877523]\n",
      " [0.44375    0.52370607 0.4986753  0.48767638 0.53334098 0.53334098\n",
      "  0.53334098 0.53334098 0.53334098 0.53334098]\n",
      " [0.47056346 0.46714316 0.46494038 0.45957332 0.45957332 0.45957332\n",
      "  0.45957332 0.45957332 0.45957332 0.40276839]]\n",
      "|| Epoch:01000 | TR_Loss_tte:4.5016 |  TR_Loss_mle:2.9438 || VA_Loss_tte:4.0997 || VA_Loss_mle:2.8538 ||\n",
      "saved...\n",
      "[[0.         0.40339428 0.39846813 0.5185383  0.58989395 0.55159353\n",
      "  0.52254141 0.52254141 0.52254141 0.52254141]\n",
      " [0.65188742 0.66079206 0.6594498  0.68261724 0.64562174 0.66707906\n",
      "  0.66707906 0.66707906 0.66707906 0.66707906]\n",
      " [0.40625    0.66414999 0.72584988 0.680587   0.69277392 0.69277392\n",
      "  0.69277392 0.69277392 0.69277392 0.69277392]\n",
      " [0.7078996  0.77130974 0.72194357 0.67867412 0.67867412 0.67867412\n",
      "  0.67867412 0.67867412 0.67867412 0.51721211]]\n",
      "|| Epoch:01100 | TR_Loss_tte:4.0712 |  TR_Loss_mle:2.6058 || VA_Loss_tte:3.7024 || VA_Loss_mle:2.4787 ||\n",
      "saved...\n",
      "[[0.         0.45297775 0.4611357  0.53244625 0.60331459 0.56595433\n",
      "  0.53236867 0.53236867 0.53236867 0.53236867]\n",
      " [0.63807733 0.65233176 0.68684501 0.70641233 0.6651088  0.68214393\n",
      "  0.68186218 0.68186218 0.68058752 0.68030576]\n",
      " [0.4        0.66397575 0.73109512 0.68732762 0.69866932 0.69866932\n",
      "  0.69866932 0.69866932 0.69866932 0.69534973]\n",
      " [0.72378618 0.78481959 0.73166111 0.68705485 0.68705485 0.68705485\n",
      "  0.68705485 0.68705485 0.68705485 0.52158965]]\n",
      "|| Epoch:01200 | TR_Loss_tte:4.0030 |  TR_Loss_mle:2.5263 || VA_Loss_tte:3.2011 || VA_Loss_mle:1.9495 ||\n",
      "saved...\n",
      "[[0.         0.57439179 0.62220737 0.5905088  0.64361588 0.61984586\n",
      "  0.5880404  0.5880404  0.5880404  0.5880404 ]\n",
      " [0.7465444  0.74497528 0.72634845 0.77089752 0.76146504 0.76943758\n",
      "  0.76943758 0.76943758 0.76943758 0.76943758]\n",
      " [0.7875     0.75117732 0.81400978 0.81979475 0.79594961 0.79594961\n",
      "  0.79594961 0.79594961 0.79594961 0.79594961]\n",
      " [0.72967397 0.80721178 0.81110957 0.76066895 0.76066895 0.76066895\n",
      "  0.76066895 0.76066895 0.76066895 0.72275856]]\n",
      "|| Epoch:01300 | TR_Loss_tte:3.4357 |  TR_Loss_mle:1.9599 || VA_Loss_tte:2.9077 || VA_Loss_mle:1.7414 ||\n",
      "saved...\n",
      "[[0.         0.62703246 0.66571535 0.61497075 0.65757646 0.62176971\n",
      "  0.59632825 0.59632825 0.59632825 0.59632825]\n",
      " [0.81950091 0.82262359 0.82568049 0.85348016 0.84088361 0.83858421\n",
      "  0.83858421 0.83858421 0.83858421 0.83858421]\n",
      " [0.8125     0.86766142 0.89991768 0.88460084 0.84798541 0.84798541\n",
      "  0.84798541 0.84798541 0.84798541 0.84798541]\n",
      " [0.8897498  0.91408901 0.89848605 0.8513105  0.8513105  0.8513105\n",
      "  0.8513105  0.8513105  0.8513105  0.77010369]]\n",
      "|| Epoch:01400 | TR_Loss_tte:3.2359 |  TR_Loss_mle:1.8487 || VA_Loss_tte:2.8105 || VA_Loss_mle:1.6971 ||\n",
      "saved...\n",
      "[[0.         0.66706042 0.69879881 0.7188915  0.76150967 0.72276138\n",
      "  0.6982125  0.6982125  0.6982125  0.6982125 ]\n",
      " [0.83540374 0.84227334 0.85574269 0.87687911 0.85570104 0.84882784\n",
      "  0.84882784 0.84882784 0.84882784 0.84882784]\n",
      " [0.7875     0.88153144 0.90766531 0.88492273 0.85755522 0.85755522\n",
      "  0.85755522 0.85755522 0.85755522 0.85755522]\n",
      " [0.90220232 0.92565031 0.9083222  0.87507863 0.87507863 0.87507863\n",
      "  0.87507863 0.87507863 0.87507863 0.78251859]]\n",
      "|| Epoch:01500 | TR_Loss_tte:3.2377 |  TR_Loss_mle:1.8631 || VA_Loss_tte:2.7456 || VA_Loss_mle:1.6712 ||\n",
      "saved...\n",
      "[[0.         0.69330605 0.71832242 0.7359519  0.77569334 0.74212549\n",
      "  0.72566913 0.72566913 0.72566913 0.72566913]\n",
      " [0.85783924 0.86515376 0.87747357 0.89653987 0.87358257 0.87166582\n",
      "  0.87166582 0.87166582 0.87166582 0.87166582]\n",
      " [0.8125     0.87635338 0.90136483 0.86669403 0.84625639 0.84625639\n",
      "  0.84625639 0.84625639 0.84625639 0.84625639]\n",
      " [0.91121103 0.9319814  0.90305018 0.88072194 0.88072194 0.88072194\n",
      "  0.88072194 0.88072194 0.88072194 0.78546628]]\n",
      "|| Epoch:01600 | TR_Loss_tte:3.2010 |  TR_Loss_mle:1.8822 || VA_Loss_tte:2.7542 || VA_Loss_mle:1.7128 ||\n",
      "|| Epoch:01700 | TR_Loss_tte:3.0925 |  TR_Loss_mle:1.8101 || VA_Loss_tte:2.6525 || VA_Loss_mle:1.6614 ||\n",
      "saved...\n",
      "[[0.         0.7017606  0.72747882 0.76510935 0.80312254 0.77569114\n",
      "  0.7693415  0.7693415  0.7693415  0.7693415 ]\n",
      " [0.83230889 0.85272725 0.86855179 0.89071663 0.86828632 0.86356987\n",
      "  0.86356987 0.86356987 0.86356987 0.86356987]\n",
      " [0.8375     0.87062191 0.90458269 0.87906024 0.86171612 0.86171612\n",
      "  0.86171612 0.86171612 0.86171612 0.86171612]\n",
      " [0.9054522  0.93355422 0.9185995  0.89922723 0.89922723 0.89922723\n",
      "  0.89922723 0.89922723 0.89922723 0.79513221]]\n",
      "|| Epoch:01800 | TR_Loss_tte:3.0885 |  TR_Loss_mle:1.8340 || VA_Loss_tte:2.6240 || VA_Loss_mle:1.6699 ||\n",
      "saved...\n",
      "[[0.         0.73711377 0.75669853 0.80517857 0.83643528 0.80814387\n",
      "  0.80219859 0.80219859 0.80219859 0.80219859]\n",
      " [0.83306872 0.8555239  0.8712246  0.89293875 0.87246201 0.86737725\n",
      "  0.86737725 0.86737725 0.86737725 0.86737725]\n",
      " [0.825      0.88790507 0.91484753 0.88794837 0.86948975 0.86948975\n",
      "  0.86948975 0.86948975 0.86948975 0.86948975]\n",
      " [0.9143748  0.93327517 0.9112565  0.89289438 0.89289438 0.89289438\n",
      "  0.89289438 0.89289438 0.89289438 0.79182435]]\n",
      "|| Epoch:01900 | TR_Loss_tte:2.9960 |  TR_Loss_mle:1.7898 || VA_Loss_tte:2.6259 || VA_Loss_mle:1.6862 ||\n",
      "|| Epoch:02000 | TR_Loss_tte:2.9318 |  TR_Loss_mle:1.7258 || VA_Loss_tte:2.5397 || VA_Loss_mle:1.6226 ||\n",
      "saved...\n",
      "[[0.         0.74812986 0.77230935 0.79931956 0.83156421 0.81664163\n",
      "  0.81321365 0.81321365 0.81321365 0.81321365]\n",
      " [0.8366548  0.85415055 0.84932998 0.87473601 0.85590655 0.85228206\n",
      "  0.85228206 0.85228206 0.85228206 0.85228206]\n",
      " [0.7875     0.83453146 0.87796571 0.85920642 0.86292835 0.86292835\n",
      "  0.86292835 0.86292835 0.86292835 0.86292835]\n",
      " [0.86487423 0.90503706 0.9014432  0.90481119 0.90481119 0.90481119\n",
      "  0.90481119 0.90481119 0.90481119 0.7980489 ]]\n",
      "|| Epoch:02100 | TR_Loss_tte:2.9638 |  TR_Loss_mle:1.7931 || VA_Loss_tte:2.5257 || VA_Loss_mle:1.6233 ||\n",
      "saved...\n",
      "[[0.         0.77653761 0.7957886  0.83650017 0.86247542 0.84691016\n",
      "  0.84081235 0.84081235 0.84081235 0.84081235]\n",
      " [0.84484546 0.85658292 0.87174696 0.89337303 0.87285699 0.86773739\n",
      "  0.86773739 0.86773739 0.86773739 0.86773739]\n",
      " [0.7875     0.86563173 0.9009024  0.87906686 0.87565432 0.87565432\n",
      "  0.87565432 0.87565432 0.87565432 0.87565432]\n",
      " [0.8997107  0.92624443 0.90527486 0.89283064 0.89283064 0.89283064\n",
      "  0.89283064 0.89283064 0.89283064 0.79179106]]\n",
      "|| Epoch:02200 | TR_Loss_tte:2.8778 |  TR_Loss_mle:1.7179 || VA_Loss_tte:2.5221 || VA_Loss_mle:1.6369 ||\n",
      "saved...\n",
      "[[0.         0.77626238 0.79556112 0.82811437 0.85550363 0.84056929\n",
      "  0.83503078 0.83503078 0.83503078 0.83503078]\n",
      " [0.84294537 0.8506752  0.86717144 0.88956904 0.86724257 0.86261819\n",
      "  0.86261819 0.86261819 0.86261819 0.86261819]\n",
      " [0.7375     0.8776934  0.90979797 0.87718943 0.87865645 0.87865645\n",
      "  0.87865645 0.87865645 0.87865645 0.87865645]\n",
      " [0.89716771 0.92118247 0.89030984 0.89011436 0.89011436 0.89011436\n",
      "  0.89011436 0.89011436 0.89011436 0.79037226]]\n",
      "|| Epoch:02300 | TR_Loss_tte:2.8907 |  TR_Loss_mle:1.7412 || VA_Loss_tte:2.5135 || VA_Loss_mle:1.6458 ||\n",
      "saved...\n",
      "[[0.         0.77182557 0.79189406 0.8233792  0.85156691 0.84129817\n",
      "  0.83569537 0.83569537 0.83569537 0.83569537]\n",
      " [0.84331661 0.85748795 0.86922954 0.8912801  0.87095346 0.86600176\n",
      "  0.86600176 0.86600176 0.86600176 0.86600176]\n",
      " [0.725      0.86597477 0.90115539 0.86970599 0.87211137 0.87211137\n",
      "  0.87211137 0.87211137 0.87211137 0.87211137]\n",
      " [0.89128416 0.92032247 0.88957817 0.89457838 0.89457838 0.89457838\n",
      "  0.89457838 0.89457838 0.89457838 0.79270396]]\n",
      "|| Epoch:02400 | TR_Loss_tte:2.8663 |  TR_Loss_mle:1.7348 || VA_Loss_tte:2.5228 || VA_Loss_mle:1.6658 ||\n",
      "|| Epoch:02500 | TR_Loss_tte:2.8614 |  TR_Loss_mle:1.7623 || VA_Loss_tte:2.4413 || VA_Loss_mle:1.6098 ||\n",
      "saved...\n",
      "[[0.         0.78665832 0.81282801 0.83554309 0.86167972 0.84834114\n",
      "  0.84211711 0.84211711 0.84211711 0.84211711]\n",
      " [0.83471093 0.85471256 0.86514303 0.88788266 0.87217281 0.86711356\n",
      "  0.86711356 0.86711356 0.86711356 0.86711356]\n",
      " [0.8125     0.87269716 0.90322108 0.88746118 0.88299606 0.88299606\n",
      "  0.88299606 0.88299606 0.88299606 0.88299606]\n",
      " [0.88653104 0.9169821  0.91515863 0.92173488 0.92173488 0.92173488\n",
      "  0.92173488 0.92173488 0.92173488 0.80688872]]\n",
      "|| Epoch:02600 | TR_Loss_tte:2.8110 |  TR_Loss_mle:1.7243 || VA_Loss_tte:2.4558 || VA_Loss_mle:1.6415 ||\n",
      "|| Epoch:02700 | TR_Loss_tte:2.7738 |  TR_Loss_mle:1.6914 || VA_Loss_tte:2.4070 || VA_Loss_mle:1.6065 ||\n",
      "saved...\n",
      "[[0.         0.79731286 0.82163408 0.84180645 0.86688696 0.85307714\n",
      "  0.84643537 0.84643537 0.84643537 0.84643537]\n",
      " [0.83615875 0.84940328 0.86780427 0.89009516 0.87633976 0.87091296\n",
      "  0.87091296 0.87091296 0.87091296 0.87091296]\n",
      " [0.7625     0.89106871 0.91966235 0.90169737 0.90473543 0.90473543\n",
      "  0.90473543 0.90473543 0.90473543 0.90473543]\n",
      " [0.92299089 0.94260515 0.92985268 0.93440752 0.93440752 0.93440752\n",
      "  0.93440752 0.93440752 0.93440752 0.88962345]]\n",
      "|| Epoch:02800 | TR_Loss_tte:2.7843 |  TR_Loss_mle:1.7229 || VA_Loss_tte:2.4233 || VA_Loss_mle:1.6266 ||\n",
      "|| Epoch:02900 | TR_Loss_tte:2.7003 |  TR_Loss_mle:1.6440 || VA_Loss_tte:2.4157 || VA_Loss_mle:1.6412 ||\n",
      "|| Epoch:03000 | TR_Loss_tte:2.8029 |  TR_Loss_mle:1.7644 || VA_Loss_tte:2.4656 || VA_Loss_mle:1.7050 ||\n",
      "|| Epoch:03100 | TR_Loss_tte:2.7015 |  TR_Loss_mle:1.6824 || VA_Loss_tte:2.4012 || VA_Loss_mle:1.6500 ||\n",
      "saved...\n",
      "[[0.         0.77786224 0.80122069 0.83261019 0.86083552 0.849728\n",
      "  0.84338165 0.84338165 0.84338165 0.84338165]\n",
      " [0.84508922 0.85461575 0.85153378 0.87656821 0.85757294 0.85706831\n",
      "  0.85706831 0.85706831 0.85706831 0.85706831]\n",
      " [0.75       0.86226782 0.89842148 0.87691868 0.87841965 0.87841965\n",
      "  0.87841965 0.87841965 0.87841965 0.87841965]\n",
      " [0.90296959 0.93180951 0.91356233 0.91526314 0.91526314 0.91526314\n",
      "  0.91526314 0.91526314 0.91526314 0.80350831]]\n",
      "|| Epoch:03200 | TR_Loss_tte:2.6763 |  TR_Loss_mle:1.6814 || VA_Loss_tte:2.3989 || VA_Loss_mle:1.6531 ||\n",
      "saved...\n",
      "[[0.         0.7867858  0.81510202 0.84116858 0.86635664 0.85259481\n",
      "  0.84599559 0.84599559 0.84599559 0.84599559]\n",
      " [0.8503474  0.86980488 0.87668186 0.89402381 0.87991288 0.87417092\n",
      "  0.87417092 0.87417092 0.87417092 0.87417092]\n",
      " [0.7875     0.87948106 0.90326101 0.88430245 0.88952169 0.88952169\n",
      "  0.88952169 0.88952169 0.88952169 0.88952169]\n",
      " [0.90289562 0.92239791 0.91266071 0.92467562 0.92467562 0.92467562\n",
      "  0.92467562 0.92467562 0.92467562 0.88454015]]\n",
      "|| Epoch:03300 | TR_Loss_tte:2.7213 |  TR_Loss_mle:1.7203 || VA_Loss_tte:2.3699 || VA_Loss_mle:1.6321 ||\n",
      "saved...\n",
      "[[0.         0.79818282 0.82235312 0.85072129 0.87429857 0.85766336\n",
      "  0.85061707 0.85061707 0.85061707 0.85061707]\n",
      " [0.84352579 0.85115492 0.86036285 0.88390852 0.86209431 0.85792403\n",
      "  0.85792403 0.85792403 0.85792403 0.85792403]\n",
      " [0.775      0.86639364 0.90146431 0.88913334 0.88445855 0.88445855\n",
      "  0.88445855 0.88445855 0.88445855 0.88445855]\n",
      " [0.91510182 0.94033572 0.93502748 0.92868039 0.92868039 0.92868039\n",
      "  0.92868039 0.92868039 0.92868039 0.81051659]]\n",
      "|| Epoch:03400 | TR_Loss_tte:2.6236 |  TR_Loss_mle:1.6481 || VA_Loss_tte:2.4478 || VA_Loss_mle:1.7064 ||\n",
      "|| Epoch:03500 | TR_Loss_tte:2.6641 |  TR_Loss_mle:1.6875 || VA_Loss_tte:2.3667 || VA_Loss_mle:1.6477 ||\n",
      "saved...\n",
      "[[0.         0.81136003 0.81589509 0.85073735 0.87431192 0.85336617\n",
      "  0.84996576 0.84996576 0.84996576 0.84996576]\n",
      " [0.82387087 0.83274132 0.8661651  0.88873238 0.86432696 0.85995976\n",
      "  0.85995976 0.85995976 0.85995976 0.85995976]\n",
      " [0.75       0.88904278 0.91568659 0.88548159 0.88590884 0.88590884\n",
      "  0.88590884 0.88590884 0.88590884 0.88590884]\n",
      " [0.92353458 0.94345207 0.91636202 0.91258265 0.91258265 0.91258265\n",
      "  0.91258265 0.91258265 0.91258265 0.8021082 ]]\n",
      "|| Epoch:03600 | TR_Loss_tte:2.5991 |  TR_Loss_mle:1.6217 || VA_Loss_tte:2.3656 || VA_Loss_mle:1.6411 ||\n",
      "saved...\n",
      "[[0.         0.80950539 0.81869949 0.85357212 0.8766687  0.859819\n",
      "  0.85584942 0.85584942 0.85584942 0.85584942]\n",
      " [0.82592775 0.83444135 0.87259389 0.89248301 0.87420218 0.86896393\n",
      "  0.86896393 0.86896393 0.86896393 0.86896393]\n",
      " [0.7        0.89040603 0.916692   0.89593209 0.89504893 0.89504893\n",
      "  0.89504893 0.89504893 0.89504893 0.89504893]\n",
      " [0.91342312 0.93307118 0.91463574 0.91109386 0.91109386 0.91109386\n",
      "  0.91109386 0.91109386 0.91109386 0.80133055]]\n",
      "|| Epoch:03700 | TR_Loss_tte:2.6616 |  TR_Loss_mle:1.6883 || VA_Loss_tte:2.3549 || VA_Loss_mle:1.6467 ||\n",
      "saved...\n",
      "[[0.         0.81075663 0.8284082  0.8575281  0.87995762 0.86711963\n",
      "  0.85923925 0.85923925 0.85923925 0.85923925]\n",
      " [0.83937919 0.84989636 0.87401193 0.89525608 0.87241497 0.86733436\n",
      "  0.86733436 0.86733436 0.86733436 0.86733436]\n",
      " [0.7625     0.89886899 0.9229335  0.89814317 0.89233862 0.89233862\n",
      "  0.89233862 0.89233862 0.89233862 0.89233862]\n",
      " [0.918498   0.93991248 0.9204562  0.91611362 0.91611362 0.91611362\n",
      "  0.91611362 0.91611362 0.91611362 0.88006793]]\n",
      "|| Epoch:03800 | TR_Loss_tte:2.6480 |  TR_Loss_mle:1.6943 || VA_Loss_tte:2.3802 || VA_Loss_mle:1.6507 ||\n",
      "|| Epoch:03900 | TR_Loss_tte:2.5764 |  TR_Loss_mle:1.6523 || VA_Loss_tte:2.3900 || VA_Loss_mle:1.6825 ||\n",
      "|| Epoch:04000 | TR_Loss_tte:2.6268 |  TR_Loss_mle:1.6716 || VA_Loss_tte:2.3864 || VA_Loss_mle:1.6776 ||\n",
      "|| Epoch:04100 | TR_Loss_tte:2.5893 |  TR_Loss_mle:1.6302 || VA_Loss_tte:2.3833 || VA_Loss_mle:1.6806 ||\n",
      "|| Epoch:04200 | TR_Loss_tte:2.6238 |  TR_Loss_mle:1.6731 || VA_Loss_tte:2.3586 || VA_Loss_mle:1.6567 ||\n",
      "|| Epoch:04300 | TR_Loss_tte:2.5216 |  TR_Loss_mle:1.6172 || VA_Loss_tte:2.3705 || VA_Loss_mle:1.6612 ||\n",
      "|| Epoch:04400 | TR_Loss_tte:2.6104 |  TR_Loss_mle:1.6682 || VA_Loss_tte:2.3584 || VA_Loss_mle:1.6528 ||\n",
      "|| Epoch:04500 | TR_Loss_tte:2.6114 |  TR_Loss_mle:1.6886 || VA_Loss_tte:2.3471 || VA_Loss_mle:1.6520 ||\n",
      "saved...\n",
      "[[0.         0.80620154 0.8203061  0.8479808  0.87361433 0.85919571\n",
      "  0.85528111 0.85528111 0.85528111 0.85528111]\n",
      " [0.83926839 0.84763615 0.87128827 0.89299168 0.87681949 0.87135038\n",
      "  0.87135038 0.87135038 0.87135038 0.87135038]\n",
      " [0.7625     0.88869683 0.91791308 0.90018271 0.90341069 0.90341069\n",
      "  0.90341069 0.90341069 0.90341069 0.90341069]\n",
      " [0.91260366 0.93858008 0.93353381 0.93758226 0.93758226 0.93758226\n",
      "  0.93758226 0.93758226 0.93758226 0.89128173]]\n",
      "|| Epoch:04600 | TR_Loss_tte:2.5681 |  TR_Loss_mle:1.6491 || VA_Loss_tte:2.3980 || VA_Loss_mle:1.6804 ||\n",
      "|| Epoch:04700 | TR_Loss_tte:2.5386 |  TR_Loss_mle:1.6336 || VA_Loss_tte:2.3549 || VA_Loss_mle:1.6477 ||\n",
      "|| Epoch:04800 | TR_Loss_tte:2.5756 |  TR_Loss_mle:1.6784 || VA_Loss_tte:2.3981 || VA_Loss_mle:1.6934 ||\n",
      "|| Epoch:04900 | TR_Loss_tte:2.5438 |  TR_Loss_mle:1.6416 || VA_Loss_tte:2.4023 || VA_Loss_mle:1.7073 ||\n",
      "|| Epoch:05000 | TR_Loss_tte:2.5183 |  TR_Loss_mle:1.6344 || VA_Loss_tte:2.3523 || VA_Loss_mle:1.6551 ||\n",
      "|| Epoch:05100 | TR_Loss_tte:2.4638 |  TR_Loss_mle:1.5930 || VA_Loss_tte:2.3616 || VA_Loss_mle:1.6676 ||\n",
      "|| Epoch:05200 | TR_Loss_tte:2.4849 |  TR_Loss_mle:1.6094 || VA_Loss_tte:2.3938 || VA_Loss_mle:1.6901 ||\n",
      "|| Epoch:05300 | TR_Loss_tte:2.4960 |  TR_Loss_mle:1.6074 || VA_Loss_tte:2.3861 || VA_Loss_mle:1.6828 ||\n",
      "|| Epoch:05400 | TR_Loss_tte:2.4858 |  TR_Loss_mle:1.5993 || VA_Loss_tte:2.4185 || VA_Loss_mle:1.7248 ||\n",
      "|| Epoch:05500 | TR_Loss_tte:2.4796 |  TR_Loss_mle:1.6199 || VA_Loss_tte:2.4530 || VA_Loss_mle:1.7446 ||\n",
      "|| Epoch:05600 | TR_Loss_tte:2.4847 |  TR_Loss_mle:1.6258 || VA_Loss_tte:2.4092 || VA_Loss_mle:1.7069 ||\n",
      "|| Epoch:05700 | TR_Loss_tte:2.4398 |  TR_Loss_mle:1.5874 || VA_Loss_tte:2.4120 || VA_Loss_mle:1.6927 ||\n",
      "|| Epoch:05800 | TR_Loss_tte:2.4481 |  TR_Loss_mle:1.6127 || VA_Loss_tte:2.3961 || VA_Loss_mle:1.6845 ||\n",
      "|| Epoch:05900 | TR_Loss_tte:2.4598 |  TR_Loss_mle:1.5992 || VA_Loss_tte:2.4119 || VA_Loss_mle:1.7116 ||\n",
      "|| Epoch:06000 | TR_Loss_tte:2.4565 |  TR_Loss_mle:1.6083 || VA_Loss_tte:2.3777 || VA_Loss_mle:1.6764 ||\n",
      "|| Epoch:06100 | TR_Loss_tte:2.4701 |  TR_Loss_mle:1.6457 || VA_Loss_tte:2.4060 || VA_Loss_mle:1.6905 ||\n",
      "|| Epoch:06200 | TR_Loss_tte:2.3721 |  TR_Loss_mle:1.5458 || VA_Loss_tte:2.4142 || VA_Loss_mle:1.7110 ||\n",
      "|| Epoch:06300 | TR_Loss_tte:2.4743 |  TR_Loss_mle:1.6294 || VA_Loss_tte:2.4219 || VA_Loss_mle:1.7155 ||\n",
      "|| Epoch:06400 | TR_Loss_tte:2.4328 |  TR_Loss_mle:1.6139 || VA_Loss_tte:2.4256 || VA_Loss_mle:1.7122 ||\n",
      "|| Epoch:06500 | TR_Loss_tte:2.4828 |  TR_Loss_mle:1.6531 || VA_Loss_tte:2.4519 || VA_Loss_mle:1.7433 ||\n",
      "|| Epoch:06600 | TR_Loss_tte:2.4365 |  TR_Loss_mle:1.6113 || VA_Loss_tte:2.4721 || VA_Loss_mle:1.7421 ||\n",
      "|| Epoch:06700 | TR_Loss_tte:2.4061 |  TR_Loss_mle:1.5986 || VA_Loss_tte:2.4858 || VA_Loss_mle:1.7671 ||\n",
      "|| Epoch:06800 | TR_Loss_tte:2.3644 |  TR_Loss_mle:1.5537 || VA_Loss_tte:2.4222 || VA_Loss_mle:1.6826 ||\n",
      "|| Epoch:06900 | TR_Loss_tte:2.4389 |  TR_Loss_mle:1.6168 || VA_Loss_tte:2.4786 || VA_Loss_mle:1.7515 ||\n",
      "|| Epoch:07000 | TR_Loss_tte:2.4135 |  TR_Loss_mle:1.5929 || VA_Loss_tte:2.5210 || VA_Loss_mle:1.7576 ||\n",
      "|| Epoch:07100 | TR_Loss_tte:2.3732 |  TR_Loss_mle:1.5792 || VA_Loss_tte:2.4599 || VA_Loss_mle:1.7219 ||\n",
      "|| Epoch:07200 | TR_Loss_tte:2.3785 |  TR_Loss_mle:1.5841 || VA_Loss_tte:2.4602 || VA_Loss_mle:1.7068 ||\n",
      "|| Epoch:07300 | TR_Loss_tte:2.3912 |  TR_Loss_mle:1.5995 || VA_Loss_tte:2.4964 || VA_Loss_mle:1.7579 ||\n",
      "|| Epoch:07400 | TR_Loss_tte:2.3526 |  TR_Loss_mle:1.5532 || VA_Loss_tte:2.5424 || VA_Loss_mle:1.7736 ||\n",
      "|| Epoch:07500 | TR_Loss_tte:2.4034 |  TR_Loss_mle:1.6077 || VA_Loss_tte:2.5827 || VA_Loss_mle:1.8141 ||\n",
      "|| Epoch:07600 | TR_Loss_tte:2.3088 |  TR_Loss_mle:1.5160 || VA_Loss_tte:2.5935 || VA_Loss_mle:1.8252 ||\n",
      "|| Epoch:07700 | TR_Loss_tte:2.2827 |  TR_Loss_mle:1.5014 || VA_Loss_tte:2.6134 || VA_Loss_mle:1.8128 ||\n",
      "|| Epoch:07800 | TR_Loss_tte:2.4075 |  TR_Loss_mle:1.6139 || VA_Loss_tte:2.5368 || VA_Loss_mle:1.7846 ||\n",
      "|| Epoch:07900 | TR_Loss_tte:2.3348 |  TR_Loss_mle:1.5524 || VA_Loss_tte:2.5827 || VA_Loss_mle:1.8157 ||\n",
      "|| Epoch:08000 | TR_Loss_tte:2.3964 |  TR_Loss_mle:1.5926 || VA_Loss_tte:2.6254 || VA_Loss_mle:1.8251 ||\n",
      "|| Epoch:08100 | TR_Loss_tte:2.2694 |  TR_Loss_mle:1.4820 || VA_Loss_tte:2.5946 || VA_Loss_mle:1.8218 ||\n",
      "|| Epoch:08200 | TR_Loss_tte:2.2808 |  TR_Loss_mle:1.5134 || VA_Loss_tte:2.5699 || VA_Loss_mle:1.7966 ||\n",
      "|| Epoch:08300 | TR_Loss_tte:2.4047 |  TR_Loss_mle:1.6329 || VA_Loss_tte:2.5742 || VA_Loss_mle:1.7804 ||\n",
      "|| Epoch:08400 | TR_Loss_tte:2.3444 |  TR_Loss_mle:1.5818 || VA_Loss_tte:2.6216 || VA_Loss_mle:1.8226 ||\n",
      "|| Epoch:08500 | TR_Loss_tte:2.2666 |  TR_Loss_mle:1.5001 || VA_Loss_tte:2.6667 || VA_Loss_mle:1.8347 ||\n",
      "|| Epoch:08600 | TR_Loss_tte:2.3399 |  TR_Loss_mle:1.5733 || VA_Loss_tte:2.6461 || VA_Loss_mle:1.8746 ||\n",
      "|| Epoch:08700 | TR_Loss_tte:2.2844 |  TR_Loss_mle:1.5430 || VA_Loss_tte:2.6599 || VA_Loss_mle:1.8623 ||\n",
      "|| Epoch:08800 | TR_Loss_tte:2.2782 |  TR_Loss_mle:1.5059 || VA_Loss_tte:2.6352 || VA_Loss_mle:1.8196 ||\n",
      "|| Epoch:08900 | TR_Loss_tte:2.3418 |  TR_Loss_mle:1.5809 || VA_Loss_tte:2.6802 || VA_Loss_mle:1.8616 ||\n",
      "|| Epoch:09000 | TR_Loss_tte:2.2644 |  TR_Loss_mle:1.5088 || VA_Loss_tte:2.6552 || VA_Loss_mle:1.8544 ||\n",
      "|| Epoch:09100 | TR_Loss_tte:2.2948 |  TR_Loss_mle:1.5338 || VA_Loss_tte:2.6646 || VA_Loss_mle:1.8386 ||\n",
      "|| Epoch:09200 | TR_Loss_tte:2.2713 |  TR_Loss_mle:1.5199 || VA_Loss_tte:2.7045 || VA_Loss_mle:1.8426 ||\n",
      "|| Epoch:09300 | TR_Loss_tte:2.2990 |  TR_Loss_mle:1.5575 || VA_Loss_tte:2.7006 || VA_Loss_mle:1.8819 ||\n",
      "|| Epoch:09400 | TR_Loss_tte:2.3146 |  TR_Loss_mle:1.5426 || VA_Loss_tte:2.8083 || VA_Loss_mle:1.9513 ||\n",
      "|| Epoch:09500 | TR_Loss_tte:2.2334 |  TR_Loss_mle:1.4941 || VA_Loss_tte:2.7177 || VA_Loss_mle:1.8717 ||\n",
      "model trained...\n",
      "INFO:tensorflow:Restoring parameters from ./STRATCANS_v1_new2_MICE/p1.0/itr3/models/model_tte\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "MAIN TRAINING ...\n",
      "|| Epoch:00100 | TR_Loss_tte:50.0859 |  TR_Loss_mle:46.0114 || VA_Loss_tte:5.5088 || VA_Loss_mle:2.5981 ||\n",
      "saved...\n",
      "[[0.1535854  0.16203415 0.16203415 0.28617231 0.27771265 0.3529334\n",
      "  0.35039682 0.35039682 0.30011081 0.30011081]\n",
      " [0.71042835 0.70889909 0.59106291 0.5100384  0.4638946  0.52932311\n",
      "  0.52932311 0.59405084 0.59405084 0.59405084]\n",
      " [0.         0.45179574 0.43884959 0.43033647 0.50165245 0.50165245\n",
      "  0.57321977 0.57321977 0.57321977 0.57321977]\n",
      " [0.4638468  0.45923306 0.45355764 0.44798641 0.44798641 0.43511651\n",
      "  0.43511651 0.43511651 0.43511651 0.58079485]]\n",
      "|| Epoch:00200 | TR_Loss_tte:6.5642 |  TR_Loss_mle:3.5258 || VA_Loss_tte:5.1540 || VA_Loss_mle:2.5822 ||\n",
      "saved...\n",
      "[[0.15356811 0.15873001 0.15873001 0.29105579 0.2832131  0.3551602\n",
      "  0.35233544 0.35233544 0.3016844  0.3016844 ]\n",
      " [0.72670371 0.72670371 0.53757422 0.4877371  0.45774975 0.52410562\n",
      "  0.52410562 0.58994422 0.5860005  0.5860005 ]\n",
      " [0.         0.4638468  0.45923306 0.45355764 0.44798641 0.44798641\n",
      "  0.43511651 0.43511651 0.43511651 0.43511651]\n",
      " [0.47589787 0.47282204 0.46903843 0.46532428 0.46532428 0.45331695\n",
      "  0.45331695 0.45331695 0.45331695 0.5849358 ]]\n",
      "|| Epoch:00300 | TR_Loss_tte:6.1057 |  TR_Loss_mle:3.3840 || VA_Loss_tte:4.6852 || VA_Loss_mle:2.5922 ||\n",
      "saved...\n",
      "[[0.17507837 0.1659306  0.1659306  0.29862621 0.28883678 0.35748284\n",
      "  0.35103912 0.35103912 0.30063217 0.30063217]\n",
      " [0.72823296 0.72823296 0.54173531 0.49305933 0.46421496 0.52959512\n",
      "  0.52410562 0.59032122 0.58994422 0.58994422]\n",
      " [0.         0.4638468  0.45923306 0.45355764 0.44798641 0.44798641\n",
      "  0.43511651 0.43511651 0.43511651 0.43511651]\n",
      " [0.47589787 0.47282204 0.46903843 0.46532428 0.46532428 0.45331695\n",
      "  0.45331695 0.45331695 0.45331695 0.59233708]]\n",
      "|| Epoch:00400 | TR_Loss_tte:5.5527 |  TR_Loss_mle:3.3485 || VA_Loss_tte:4.2518 || VA_Loss_mle:2.5877 ||\n",
      "saved...\n",
      "[[0.18434399 0.16888585 0.16888585 0.30716719 0.29495324 0.36223978\n",
      "  0.35849881 0.35849881 0.30668725 0.30668725]\n",
      " [0.72823296 0.72823296 0.54173531 0.49305933 0.45774975 0.52410562\n",
      "  0.52410562 0.58994422 0.58994422 0.58994422]\n",
      " [0.         0.4638468  0.45923306 0.45355764 0.44798641 0.44798641\n",
      "  0.43511651 0.43511651 0.43511651 0.43511651]\n",
      " [0.47589787 0.47282204 0.46903843 0.46532428 0.46532428 0.45331695\n",
      "  0.45331695 0.45331695 0.45331695 0.59233708]]\n",
      "|| Epoch:00500 | TR_Loss_tte:5.1982 |  TR_Loss_mle:3.2756 || VA_Loss_tte:4.0663 || VA_Loss_mle:2.5825 ||\n",
      "saved...\n",
      "[[0.20287524 0.17983419 0.17983419 0.34060793 0.31956646 0.37728007\n",
      "  0.36827426 0.36827426 0.31462206 0.31462206]\n",
      " [0.72823296 0.72823296 0.53757422 0.4877371  0.45774975 0.52023597\n",
      "  0.52023597 0.58689847 0.58689847 0.58689847]\n",
      " [0.         0.4638468  0.45923306 0.45355764 0.44798641 0.44798641\n",
      "  0.43511651 0.43511651 0.43511651 0.43511651]\n",
      " [0.47589787 0.47282204 0.46903843 0.46532428 0.46532428 0.45331695\n",
      "  0.45331695 0.45331695 0.45331695 0.59233708]]\n",
      "|| Epoch:00600 | TR_Loss_tte:4.9906 |  TR_Loss_mle:3.2476 || VA_Loss_tte:3.9292 || VA_Loss_mle:2.5699 ||\n",
      "saved...\n",
      "[[0.26764814 0.22063633 0.22063633 0.36925978 0.34272322 0.39323868\n",
      "  0.37884918 0.37884918 0.3153603  0.3153603 ]\n",
      " [0.72638424 0.72638424 0.53744175 0.48764777 0.45768416 0.51631062\n",
      "  0.51631062 0.58380888 0.58380888 0.58380888]\n",
      " [0.         0.4638468  0.45923306 0.45355764 0.44798641 0.44798641\n",
      "  0.43511651 0.43511651 0.43511651 0.43511651]\n",
      " [0.47589787 0.47282204 0.46903843 0.46532428 0.45665534 0.44678727\n",
      "  0.44678727 0.44678727 0.44678727 0.58819613]]\n",
      "|| Epoch:00700 | TR_Loss_tte:4.8842 |  TR_Loss_mle:3.2387 || VA_Loss_tte:3.8113 || VA_Loss_mle:2.5462 ||\n",
      "saved...\n",
      "[[0.36633158 0.29411188 0.29411188 0.41038795 0.38780092 0.41974121\n",
      "  0.40192185 0.40192185 0.33408855 0.33408855]\n",
      " [0.72857683 0.72857683 0.60763364 0.52994728 0.47494664 0.53096791\n",
      "  0.53096791 0.59534544 0.59311085 0.59311085]\n",
      " [0.         0.45179574 0.43884959 0.43355457 0.50428236 0.50428236\n",
      "  0.4775202  0.4775202  0.4775202  0.4775202 ]\n",
      " [0.4638468  0.45923306 0.45355764 0.44798641 0.44798641 0.44025758\n",
      "  0.44025758 0.44025758 0.44025758 0.58405518]]\n",
      "|| Epoch:00800 | TR_Loss_tte:4.6482 |  TR_Loss_mle:3.1010 || VA_Loss_tte:3.7177 || VA_Loss_mle:2.5115 ||\n",
      "saved...\n",
      "[[0.52973604 0.40890512 0.40890512 0.47534309 0.45971406 0.4669387\n",
      "  0.43637444 0.43637444 0.36205399 0.36205399]\n",
      " [0.72618165 0.72618165 0.59356303 0.50787757 0.44820276 0.5075208\n",
      "  0.50702537 0.57611064 0.57611064 0.59734754]\n",
      " [0.         0.4276936  0.41846612 0.41355151 0.48324879 0.48324879\n",
      "  0.55935761 0.55935761 0.55935761 0.55935761]\n",
      " [0.4638468  0.45923306 0.45355764 0.44798641 0.44798641 0.44025758\n",
      "  0.42858682 0.42858682 0.42858682 0.63762501]]\n",
      "|| Epoch:00900 | TR_Loss_tte:4.4950 |  TR_Loss_mle:3.0007 || VA_Loss_tte:3.3056 || VA_Loss_mle:2.1612 ||\n",
      "saved...\n",
      "[[0.68072292 0.52787823 0.52787823 0.54935761 0.57763778 0.54786885\n",
      "  0.50683089 0.50683089 0.44278057 0.44278057]\n",
      " [0.6855971  0.6855971  0.66639908 0.61868948 0.51620693 0.56600135\n",
      "  0.56600135 0.6229198  0.62230989 0.62230989]\n",
      " [0.         0.58891244 0.60332479 0.52693561 0.5665354  0.5665354\n",
      "  0.61180929 0.61180929 0.61180929 0.61180929]\n",
      " [0.59695129 0.71508627 0.69151575 0.63542086 0.63542086 0.57115634\n",
      "  0.57115634 0.57115634 0.57115634 0.7280388 ]]\n",
      "|| Epoch:01000 | TR_Loss_tte:3.8910 |  TR_Loss_mle:2.4284 || VA_Loss_tte:2.8238 || VA_Loss_mle:1.6527 ||\n",
      "saved...\n",
      "[[0.70219858 0.55362221 0.55362221 0.58123392 0.61744016 0.56153721\n",
      "  0.51873033 0.51873033 0.51520355 0.51520355]\n",
      " [0.79134268 0.79134268 0.69146254 0.70378991 0.58688701 0.61440568\n",
      "  0.61440568 0.65214685 0.65214685 0.65214685]\n",
      " [0.         0.67759204 0.71165462 0.68270872 0.65165675 0.65165675\n",
      "  0.67592509 0.67592509 0.67592509 0.67592509]\n",
      " [0.64525586 0.74934142 0.75496099 0.65914982 0.65914982 0.68156908\n",
      "  0.68156908 0.68156908 0.68156908 0.79805961]]\n",
      "|| Epoch:01100 | TR_Loss_tte:3.5884 |  TR_Loss_mle:2.1386 || VA_Loss_tte:2.5814 || VA_Loss_mle:1.4900 ||\n",
      "saved...\n",
      "[[0.69902933 0.57427759 0.57427759 0.6441227  0.66602978 0.59024389\n",
      "  0.57026889 0.57026889 0.58841985 0.58841985]\n",
      " [0.77181164 0.77181164 0.73165127 0.73916162 0.63576995 0.66365069\n",
      "  0.66365069 0.70864971 0.70864971 0.70864971]\n",
      " [0.         0.80291013 0.79139893 0.70951137 0.66887381 0.66887381\n",
      "  0.68889346 0.68889346 0.68889346 0.68889346]\n",
      " [0.72997488 0.7961613  0.78920933 0.72463139 0.72463139 0.75145596\n",
      "  0.75145596 0.75145596 0.75145596 0.84238   ]]\n",
      "|| Epoch:01200 | TR_Loss_tte:3.3375 |  TR_Loss_mle:1.9422 || VA_Loss_tte:2.4803 || VA_Loss_mle:1.4524 ||\n",
      "saved...\n",
      "[[0.77881834 0.65223197 0.65223197 0.70427135 0.71632944 0.62250728\n",
      "  0.61494874 0.61494874 0.65606889 0.65606889]\n",
      " [0.7701655  0.7701655  0.75901105 0.75171135 0.6552141  0.66855144\n",
      "  0.66855144 0.71250702 0.71250702 0.71250702]\n",
      " [0.         0.77481021 0.76821418 0.65184804 0.6451833  0.6451833\n",
      "  0.68133126 0.68133126 0.68133126 0.68133126]\n",
      " [0.74223556 0.80657271 0.79975366 0.77542817 0.77542817 0.79999969\n",
      "  0.79999969 0.79999969 0.79999969 0.87316514]]\n",
      "|| Epoch:01300 | TR_Loss_tte:3.2166 |  TR_Loss_mle:1.8695 || VA_Loss_tte:2.4729 || VA_Loss_mle:1.4927 ||\n",
      "saved...\n",
      "[[0.75410424 0.64790165 0.64790165 0.71175756 0.72189057 0.62442981\n",
      "  0.63321434 0.63321434 0.67089521 0.67089521]\n",
      " [0.77192173 0.77192173 0.76187842 0.75601669 0.65858536 0.67141392\n",
      "  0.67141392 0.71476005 0.71476005 0.71476005]\n",
      " [0.         0.74654779 0.74534159 0.62683796 0.62474453 0.62474453\n",
      "  0.6659362  0.6659362  0.6659362  0.6659362 ]\n",
      " [0.74972537 0.80738405 0.8100802  0.78386724 0.78386724 0.80635623\n",
      "  0.80635623 0.80635623 0.80635623 0.87719629]]\n",
      "|| Epoch:01400 | TR_Loss_tte:3.1460 |  TR_Loss_mle:1.8707 || VA_Loss_tte:2.3693 || VA_Loss_mle:1.4289 ||\n",
      "saved...\n",
      "[[0.82795253 0.74675718 0.74675718 0.78403842 0.78960646 0.67228936\n",
      "  0.67488006 0.67488006 0.71256112 0.71256112]\n",
      " [0.79497744 0.79497744 0.78283231 0.7698574  0.67162033 0.67861205\n",
      "  0.67861205 0.72929697 0.72929697 0.72929697]\n",
      " [0.         0.75433375 0.74262158 0.65130123 0.64473643 0.64473643\n",
      "  0.71184113 0.71184113 0.71184113 0.71184113]\n",
      " [0.73706372 0.81112622 0.81565683 0.78842458 0.78842458 0.8200711\n",
      "  0.8200711  0.8200711  0.8200711  0.88589389]]\n",
      "|| Epoch:01500 | TR_Loss_tte:3.0952 |  TR_Loss_mle:1.8438 || VA_Loss_tte:2.3259 || VA_Loss_mle:1.4176 ||\n",
      "saved...\n",
      "[[0.85251097 0.78342767 0.78342767 0.7952121  0.79580585 0.67916186\n",
      "  0.68418152 0.68418152 0.7279567  0.7279567 ]\n",
      " [0.78951156 0.78951156 0.77585193 0.75479611 0.64766963 0.66214555\n",
      "  0.66214555 0.71633641 0.71633641 0.71633641]\n",
      " [0.         0.71844786 0.71147719 0.59839101 0.62024363 0.62024363\n",
      "  0.69339247 0.69339247 0.69339247 0.69339247]\n",
      " [0.66873684 0.76670296 0.77039559 0.77018269 0.77018269 0.8063308\n",
      "  0.8063308  0.8063308  0.8063308  0.87718016]]\n",
      "|| Epoch:01600 | TR_Loss_tte:3.1234 |  TR_Loss_mle:1.8907 || VA_Loss_tte:2.2860 || VA_Loss_mle:1.4173 ||\n",
      "saved...\n",
      "[[0.85865923 0.80408884 0.80408884 0.80368227 0.80817259 0.69083088\n",
      "  0.691022   0.691022   0.73350916 0.73350916]\n",
      " [0.78949936 0.78949936 0.77570729 0.75944219 0.65129107 0.66522046\n",
      "  0.66522046 0.71875663 0.71875663 0.71875663]\n",
      " [0.         0.72589055 0.72294237 0.64463933 0.6533521  0.6533521\n",
      "  0.7183307  0.7183307  0.7183307  0.7183307 ]\n",
      " [0.71237985 0.80089401 0.80884647 0.80160555 0.80160555 0.82999939\n",
      "  0.82999939 0.82999939 0.82999939 0.89219015]]\n",
      "|| Epoch:01700 | TR_Loss_tte:2.9736 |  TR_Loss_mle:1.7817 || VA_Loss_tte:2.2378 || VA_Loss_mle:1.4186 ||\n",
      "saved...\n",
      "[[0.85254556 0.80903683 0.80903683 0.80655436 0.81240709 0.69822627\n",
      "  0.70077868 0.70077868 0.74142874 0.74142874]\n",
      " [0.80543086 0.80543086 0.77477946 0.7765752  0.66611305 0.68167521\n",
      "  0.68167521 0.73170795 0.73170795 0.73170795]\n",
      " [0.         0.72019466 0.74118082 0.66656502 0.69001678 0.69001678\n",
      "  0.74594757 0.74594757 0.74594757 0.74594757]\n",
      " [0.71600552 0.80288057 0.82559825 0.8199821  0.8199821  0.84384112\n",
      "  0.84384112 0.84384112 0.84384112 0.90096821]]\n",
      "|| Epoch:01800 | TR_Loss_tte:2.9756 |  TR_Loss_mle:1.8060 || VA_Loss_tte:2.1844 || VA_Loss_mle:1.3961 ||\n",
      "saved...\n",
      "[[0.8771213  0.83382824 0.83382824 0.81583264 0.81719849 0.70417946\n",
      "  0.70596143 0.70596143 0.74563561 0.74563561]\n",
      " [0.79123121 0.79123121 0.78118376 0.77025052 0.66658404 0.67820547\n",
      "  0.67820547 0.72897696 0.72897696 0.72897696]\n",
      " [0.         0.72156776 0.73045112 0.65607323 0.68144267 0.68144267\n",
      "  0.73948931 0.73948931 0.73948931 0.73948931]\n",
      " [0.69637811 0.78923521 0.80699403 0.80477834 0.80477834 0.83238922\n",
      "  0.83238922 0.83238922 0.83238922 0.89370572]]\n",
      "|| Epoch:01900 | TR_Loss_tte:2.8928 |  TR_Loss_mle:1.7473 || VA_Loss_tte:2.2361 || VA_Loss_mle:1.4588 ||\n",
      "|| Epoch:02000 | TR_Loss_tte:2.9318 |  TR_Loss_mle:1.8050 || VA_Loss_tte:2.1859 || VA_Loss_mle:1.4323 ||\n",
      "|| Epoch:02100 | TR_Loss_tte:2.8886 |  TR_Loss_mle:1.8145 || VA_Loss_tte:2.1497 || VA_Loss_mle:1.4074 ||\n",
      "saved...\n",
      "[[0.87398664 0.82568849 0.82568849 0.80755328 0.80894721 0.70426683\n",
      "  0.70603749 0.70603749 0.74569735 0.74569735]\n",
      " [0.79491646 0.79491646 0.76558295 0.77037377 0.67487204 0.68911233\n",
      "  0.68911233 0.7375616  0.7375616  0.7375616 ]\n",
      " [0.         0.6998807  0.72964933 0.70006432 0.71739312 0.71739312\n",
      "  0.76656821 0.76656821 0.76656821 0.76656821]\n",
      " [0.68689405 0.79391604 0.83171089 0.82029085 0.82029085 0.84407368\n",
      "  0.84407368 0.84407368 0.84407368 0.90111569]]\n",
      "|| Epoch:02200 | TR_Loss_tte:2.8921 |  TR_Loss_mle:1.8169 || VA_Loss_tte:2.1245 || VA_Loss_mle:1.4055 ||\n",
      "saved...\n",
      "[[0.88330415 0.84460669 0.84460669 0.82022381 0.82233323 0.73055876\n",
      "  0.72892681 0.72892681 0.76427679 0.76427679]\n",
      " [0.84703481 0.84703481 0.77990588 0.77528847 0.67315568 0.69152464\n",
      "  0.69152464 0.73946031 0.73946031 0.73946031]\n",
      " [0.         0.69118799 0.72978367 0.71302617 0.73735909 0.73735909\n",
      "  0.78160714 0.78160714 0.78160714 0.78160714]\n",
      " [0.64528342 0.75754613 0.81032529 0.81218736 0.81218736 0.82768774\n",
      "  0.82768774 0.82768774 0.82768774 0.89072416]]\n",
      "|| Epoch:02300 | TR_Loss_tte:2.8887 |  TR_Loss_mle:1.8260 || VA_Loss_tte:2.1464 || VA_Loss_mle:1.4241 ||\n",
      "|| Epoch:02400 | TR_Loss_tte:2.7708 |  TR_Loss_mle:1.7172 || VA_Loss_tte:2.1423 || VA_Loss_mle:1.4321 ||\n",
      "|| Epoch:02500 | TR_Loss_tte:2.8339 |  TR_Loss_mle:1.7825 || VA_Loss_tte:2.1073 || VA_Loss_mle:1.4091 ||\n",
      "saved...\n",
      "[[0.89864885 0.85069816 0.85069816 0.82227038 0.8196516  0.7124162\n",
      "  0.70981382 0.70981382 0.74876263 0.74876263]\n",
      " [0.80738427 0.80738427 0.79481308 0.7794411  0.67375235 0.68816161\n",
      "  0.68816161 0.73681331 0.73681331 0.73681331]\n",
      " [0.         0.7118558  0.73990427 0.67135842 0.70330732 0.70330732\n",
      "  0.75595839 0.75595839 0.75595839 0.75595839]\n",
      " [0.62590504 0.75600335 0.79051857 0.79600089 0.79600089 0.82577779\n",
      "  0.82577779 0.82577779 0.82577779 0.88951293]]\n",
      "|| Epoch:02600 | TR_Loss_tte:2.8630 |  TR_Loss_mle:1.8195 || VA_Loss_tte:2.0940 || VA_Loss_mle:1.4039 ||\n",
      "saved...\n",
      "[[0.90178351 0.85412723 0.85412723 0.82911667 0.82893928 0.73774753\n",
      "  0.73850361 0.73850361 0.77205035 0.77205035]\n",
      " [0.82092059 0.82092059 0.80264344 0.78975394 0.68111433 0.6982822\n",
      "  0.6982822  0.74477909 0.74477909 0.74477909]\n",
      " [0.         0.72369724 0.7529772  0.66357238 0.69694441 0.69694441\n",
      "  0.75116566 0.75116566 0.75116566 0.75116566]\n",
      " [0.64585745 0.78420004 0.80619984 0.80881595 0.80881595 0.83543046\n",
      "  0.83543046 0.83543046 0.83543046 0.89563439]]\n",
      "|| Epoch:02700 | TR_Loss_tte:2.7642 |  TR_Loss_mle:1.7452 || VA_Loss_tte:2.1285 || VA_Loss_mle:1.4429 ||\n",
      "|| Epoch:02800 | TR_Loss_tte:2.7902 |  TR_Loss_mle:1.7751 || VA_Loss_tte:2.1421 || VA_Loss_mle:1.4570 ||\n",
      "|| Epoch:02900 | TR_Loss_tte:2.7890 |  TR_Loss_mle:1.7774 || VA_Loss_tte:2.0976 || VA_Loss_mle:1.4280 ||\n",
      "|| Epoch:03000 | TR_Loss_tte:2.7406 |  TR_Loss_mle:1.7607 || VA_Loss_tte:2.0737 || VA_Loss_mle:1.4045 ||\n",
      "saved...\n",
      "[[0.88943511 0.8479318  0.8479318  0.82378717 0.82498026 0.73021493\n",
      "  0.72862748 0.72862748 0.76403382 0.76403382]\n",
      " [0.81884488 0.81884488 0.79210957 0.7853115  0.67785252 0.691643\n",
      "  0.691643   0.73955346 0.73955346 0.73955346]\n",
      " [0.         0.70377368 0.75314166 0.66368184 0.69234723 0.69234723\n",
      "  0.74770293 0.74770293 0.74770293 0.74770293]\n",
      " [0.59790993 0.77230222 0.78487966 0.79139265 0.79139265 0.81202458\n",
      "  0.81202458 0.81202458 0.81202458 0.88079101]]\n",
      "|| Epoch:03100 | TR_Loss_tte:2.6853 |  TR_Loss_mle:1.7086 || VA_Loss_tte:2.0914 || VA_Loss_mle:1.4280 ||\n",
      "|| Epoch:03200 | TR_Loss_tte:2.6686 |  TR_Loss_mle:1.6965 || VA_Loss_tte:2.1051 || VA_Loss_mle:1.4474 ||\n",
      "|| Epoch:03300 | TR_Loss_tte:2.6224 |  TR_Loss_mle:1.6665 || VA_Loss_tte:2.0730 || VA_Loss_mle:1.4255 ||\n",
      "saved...\n",
      "[[0.89866615 0.85590801 0.85590801 0.82515045 0.82599297 0.73135399\n",
      "  0.72961913 0.72961913 0.76483875 0.76483875]\n",
      " [0.81272786 0.81272786 0.80670202 0.7871691  0.66632433 0.68185461\n",
      "  0.68185461 0.73184915 0.73184915 0.73184915]\n",
      " [0.         0.71622571 0.74518983 0.59323391 0.63477569 0.63477569\n",
      "  0.70433842 0.70433842 0.70433842 0.70433842]\n",
      " [0.60141251 0.75984806 0.76142523 0.76753856 0.76753856 0.80433916\n",
      "  0.80433916 0.80433916 0.80433916 0.87591712]]\n",
      "|| Epoch:03400 | TR_Loss_tte:2.6901 |  TR_Loss_mle:1.7155 || VA_Loss_tte:2.1181 || VA_Loss_mle:1.4707 ||\n",
      "|| Epoch:03500 | TR_Loss_tte:2.7252 |  TR_Loss_mle:1.7630 || VA_Loss_tte:2.0508 || VA_Loss_mle:1.4043 ||\n",
      "saved...\n",
      "[[0.8924833  0.84908581 0.84908581 0.82282374 0.82426458 0.7318851\n",
      "  0.73339988 0.73339988 0.76790761 0.76790761]\n",
      " [0.80114496 0.80114496 0.80659601 0.77940418 0.65859089 0.67915792\n",
      "  0.67915792 0.72972662 0.72972662 0.72972662]\n",
      " [0.         0.7202235  0.74778147 0.59160855 0.63813405 0.63813405\n",
      "  0.70686803 0.70686803 0.70686803 0.70686803]\n",
      " [0.56594587 0.74491141 0.75456959 0.76193598 0.76193598 0.77955483\n",
      "  0.77955483 0.77955483 0.77955483 0.86019956]]\n",
      "|| Epoch:03600 | TR_Loss_tte:2.6772 |  TR_Loss_mle:1.7337 || VA_Loss_tte:2.0807 || VA_Loss_mle:1.4481 ||\n",
      "|| Epoch:03700 | TR_Loss_tte:2.6354 |  TR_Loss_mle:1.7192 || VA_Loss_tte:2.0409 || VA_Loss_mle:1.4120 ||\n",
      "saved...\n",
      "[[0.91413191 0.86608367 0.86608367 0.8325902  0.83362055 0.74103683\n",
      "  0.73804884 0.73804884 0.77168121 0.77168121]\n",
      " [0.7964525  0.7964525  0.8117655  0.79058354 0.67149381 0.68624392\n",
      "  0.68624392 0.73530392 0.73530392 0.73530392]\n",
      " [0.         0.72019466 0.75804413 0.59226735 0.63867243 0.63867243\n",
      "  0.70727356 0.70727356 0.70727356 0.70727356]\n",
      " [0.58193704 0.74219115 0.7329216  0.75361806 0.75361806 0.77328954\n",
      "  0.77328954 0.77328954 0.77328954 0.85622628]]\n",
      "|| Epoch:03800 | TR_Loss_tte:2.6615 |  TR_Loss_mle:1.7281 || VA_Loss_tte:2.0584 || VA_Loss_mle:1.4250 ||\n",
      "|| Epoch:03900 | TR_Loss_tte:2.6454 |  TR_Loss_mle:1.7182 || VA_Loss_tte:2.0641 || VA_Loss_mle:1.4388 ||\n",
      "|| Epoch:04000 | TR_Loss_tte:2.6334 |  TR_Loss_mle:1.7042 || VA_Loss_tte:2.0891 || VA_Loss_mle:1.4637 ||\n",
      "|| Epoch:04100 | TR_Loss_tte:2.5760 |  TR_Loss_mle:1.6614 || VA_Loss_tte:2.0924 || VA_Loss_mle:1.4627 ||\n",
      "|| Epoch:04200 | TR_Loss_tte:2.5675 |  TR_Loss_mle:1.6346 || VA_Loss_tte:2.0678 || VA_Loss_mle:1.4454 ||\n",
      "|| Epoch:04300 | TR_Loss_tte:2.5945 |  TR_Loss_mle:1.6917 || VA_Loss_tte:2.0544 || VA_Loss_mle:1.4321 ||\n",
      "|| Epoch:04400 | TR_Loss_tte:2.5473 |  TR_Loss_mle:1.6373 || VA_Loss_tte:2.0329 || VA_Loss_mle:1.4218 ||\n",
      "saved...\n",
      "[[0.87409041 0.83994087 0.83994087 0.81737149 0.81811342 0.72692544\n",
      "  0.72576371 0.72576371 0.76170927 0.76170927]\n",
      " [0.8038718  0.8038718  0.80986585 0.79994582 0.67283292 0.68738094\n",
      "  0.68738094 0.73619886 0.73619886 0.73619886]\n",
      " [0.         0.72392514 0.7477208  0.59209695 0.63853318 0.63853318\n",
      "  0.70716867 0.70716867 0.70716867 0.70716867]\n",
      " [0.62574254 0.75671669 0.73694649 0.76159392 0.76159392 0.77929718\n",
      "  0.77929718 0.77929718 0.77929718 0.86003616]]\n",
      "|| Epoch:04500 | TR_Loss_tte:2.5783 |  TR_Loss_mle:1.6810 || VA_Loss_tte:2.0583 || VA_Loss_mle:1.4496 ||\n",
      "|| Epoch:04600 | TR_Loss_tte:2.5535 |  TR_Loss_mle:1.6468 || VA_Loss_tte:2.0534 || VA_Loss_mle:1.4441 ||\n",
      "|| Epoch:04700 | TR_Loss_tte:2.5957 |  TR_Loss_mle:1.7022 || VA_Loss_tte:2.0737 || VA_Loss_mle:1.4608 ||\n",
      "|| Epoch:04800 | TR_Loss_tte:2.5184 |  TR_Loss_mle:1.6519 || VA_Loss_tte:2.0441 || VA_Loss_mle:1.4378 ||\n",
      "|| Epoch:04900 | TR_Loss_tte:2.6096 |  TR_Loss_mle:1.7219 || VA_Loss_tte:2.0668 || VA_Loss_mle:1.4676 ||\n",
      "|| Epoch:05000 | TR_Loss_tte:2.4840 |  TR_Loss_mle:1.6256 || VA_Loss_tte:2.0568 || VA_Loss_mle:1.4478 ||\n",
      "|| Epoch:05100 | TR_Loss_tte:2.5149 |  TR_Loss_mle:1.6594 || VA_Loss_tte:2.0864 || VA_Loss_mle:1.4795 ||\n",
      "|| Epoch:05200 | TR_Loss_tte:2.5285 |  TR_Loss_mle:1.6692 || VA_Loss_tte:2.1041 || VA_Loss_mle:1.5026 ||\n",
      "|| Epoch:05300 | TR_Loss_tte:2.4662 |  TR_Loss_mle:1.6135 || VA_Loss_tte:2.0472 || VA_Loss_mle:1.4448 ||\n",
      "|| Epoch:05400 | TR_Loss_tte:2.5447 |  TR_Loss_mle:1.7045 || VA_Loss_tte:2.0845 || VA_Loss_mle:1.4796 ||\n",
      "|| Epoch:05500 | TR_Loss_tte:2.5694 |  TR_Loss_mle:1.7223 || VA_Loss_tte:2.0882 || VA_Loss_mle:1.4816 ||\n",
      "|| Epoch:05600 | TR_Loss_tte:2.4829 |  TR_Loss_mle:1.6421 || VA_Loss_tte:2.0578 || VA_Loss_mle:1.4620 ||\n",
      "|| Epoch:05700 | TR_Loss_tte:2.5096 |  TR_Loss_mle:1.6788 || VA_Loss_tte:2.0993 || VA_Loss_mle:1.4879 ||\n",
      "|| Epoch:05800 | TR_Loss_tte:2.4527 |  TR_Loss_mle:1.6325 || VA_Loss_tte:2.0146 || VA_Loss_mle:1.4185 ||\n",
      "saved...\n",
      "[[0.88022137 0.84985738 0.84985738 0.81292773 0.81481238 0.72676064\n",
      "  0.72562023 0.72562023 0.76159281 0.76159281]\n",
      " [0.80373731 0.80373731 0.79054201 0.77361119 0.64326737 0.66614697\n",
      "  0.66614697 0.71948587 0.71948587 0.71948587]\n",
      " [0.         0.70679933 0.75070465 0.60695538 0.6506758  0.6506758\n",
      "  0.71631483 0.71631483 0.71631483 0.71631483]\n",
      " [0.64961678 0.80063309 0.76899793 0.7831004  0.7831004  0.78521432\n",
      "  0.78521432 0.78521432 0.78521432 0.86378865]]\n",
      "|| Epoch:05900 | TR_Loss_tte:2.5416 |  TR_Loss_mle:1.7054 || VA_Loss_tte:2.1211 || VA_Loss_mle:1.5093 ||\n",
      "|| Epoch:06000 | TR_Loss_tte:2.5219 |  TR_Loss_mle:1.6832 || VA_Loss_tte:2.1156 || VA_Loss_mle:1.5040 ||\n",
      "|| Epoch:06100 | TR_Loss_tte:2.4675 |  TR_Loss_mle:1.6334 || VA_Loss_tte:2.0950 || VA_Loss_mle:1.4953 ||\n",
      "|| Epoch:06200 | TR_Loss_tte:2.4872 |  TR_Loss_mle:1.6725 || VA_Loss_tte:2.0828 || VA_Loss_mle:1.4664 ||\n",
      "|| Epoch:06300 | TR_Loss_tte:2.5002 |  TR_Loss_mle:1.6906 || VA_Loss_tte:2.1955 || VA_Loss_mle:1.5605 ||\n",
      "|| Epoch:06400 | TR_Loss_tte:2.5021 |  TR_Loss_mle:1.6804 || VA_Loss_tte:2.1209 || VA_Loss_mle:1.5090 ||\n",
      "|| Epoch:06500 | TR_Loss_tte:2.4731 |  TR_Loss_mle:1.6507 || VA_Loss_tte:2.0807 || VA_Loss_mle:1.4740 ||\n",
      "|| Epoch:06600 | TR_Loss_tte:2.4847 |  TR_Loss_mle:1.6705 || VA_Loss_tte:2.1479 || VA_Loss_mle:1.5221 ||\n",
      "|| Epoch:06700 | TR_Loss_tte:2.5151 |  TR_Loss_mle:1.7153 || VA_Loss_tte:2.1054 || VA_Loss_mle:1.4880 ||\n",
      "|| Epoch:06800 | TR_Loss_tte:2.4652 |  TR_Loss_mle:1.6662 || VA_Loss_tte:2.1242 || VA_Loss_mle:1.5040 ||\n",
      "|| Epoch:06900 | TR_Loss_tte:2.3496 |  TR_Loss_mle:1.5473 || VA_Loss_tte:2.1447 || VA_Loss_mle:1.5183 ||\n",
      "|| Epoch:07000 | TR_Loss_tte:2.4103 |  TR_Loss_mle:1.6144 || VA_Loss_tte:2.1256 || VA_Loss_mle:1.4881 ||\n",
      "|| Epoch:07100 | TR_Loss_tte:2.3991 |  TR_Loss_mle:1.6202 || VA_Loss_tte:2.2193 || VA_Loss_mle:1.5711 ||\n",
      "|| Epoch:07200 | TR_Loss_tte:2.3945 |  TR_Loss_mle:1.6398 || VA_Loss_tte:2.1976 || VA_Loss_mle:1.5521 ||\n",
      "|| Epoch:07300 | TR_Loss_tte:2.4003 |  TR_Loss_mle:1.6209 || VA_Loss_tte:2.1390 || VA_Loss_mle:1.4973 ||\n",
      "|| Epoch:07400 | TR_Loss_tte:2.3996 |  TR_Loss_mle:1.6373 || VA_Loss_tte:2.2264 || VA_Loss_mle:1.5796 ||\n",
      "|| Epoch:07500 | TR_Loss_tte:2.4450 |  TR_Loss_mle:1.6636 || VA_Loss_tte:2.1001 || VA_Loss_mle:1.4609 ||\n",
      "|| Epoch:07600 | TR_Loss_tte:2.4564 |  TR_Loss_mle:1.6914 || VA_Loss_tte:2.2115 || VA_Loss_mle:1.5681 ||\n",
      "|| Epoch:07700 | TR_Loss_tte:2.4218 |  TR_Loss_mle:1.6477 || VA_Loss_tte:2.1781 || VA_Loss_mle:1.5266 ||\n",
      "|| Epoch:07800 | TR_Loss_tte:2.4322 |  TR_Loss_mle:1.6665 || VA_Loss_tte:2.2322 || VA_Loss_mle:1.5914 ||\n",
      "|| Epoch:07900 | TR_Loss_tte:2.4145 |  TR_Loss_mle:1.6412 || VA_Loss_tte:2.1915 || VA_Loss_mle:1.5342 ||\n",
      "|| Epoch:08000 | TR_Loss_tte:2.2877 |  TR_Loss_mle:1.5353 || VA_Loss_tte:2.1962 || VA_Loss_mle:1.5413 ||\n",
      "|| Epoch:08100 | TR_Loss_tte:2.3231 |  TR_Loss_mle:1.5833 || VA_Loss_tte:2.2315 || VA_Loss_mle:1.5563 ||\n",
      "|| Epoch:08200 | TR_Loss_tte:2.3850 |  TR_Loss_mle:1.6173 || VA_Loss_tte:2.2017 || VA_Loss_mle:1.5323 ||\n",
      "|| Epoch:08300 | TR_Loss_tte:2.4116 |  TR_Loss_mle:1.6526 || VA_Loss_tte:2.2412 || VA_Loss_mle:1.5687 ||\n",
      "|| Epoch:08400 | TR_Loss_tte:2.3496 |  TR_Loss_mle:1.5921 || VA_Loss_tte:2.2007 || VA_Loss_mle:1.5180 ||\n",
      "|| Epoch:08500 | TR_Loss_tte:2.3257 |  TR_Loss_mle:1.5746 || VA_Loss_tte:2.2390 || VA_Loss_mle:1.5692 ||\n",
      "|| Epoch:08600 | TR_Loss_tte:2.3796 |  TR_Loss_mle:1.6291 || VA_Loss_tte:2.2883 || VA_Loss_mle:1.6058 ||\n",
      "|| Epoch:08700 | TR_Loss_tte:2.3579 |  TR_Loss_mle:1.6197 || VA_Loss_tte:2.2582 || VA_Loss_mle:1.5809 ||\n",
      "|| Epoch:08800 | TR_Loss_tte:2.2940 |  TR_Loss_mle:1.5447 || VA_Loss_tte:2.3141 || VA_Loss_mle:1.6096 ||\n",
      "|| Epoch:08900 | TR_Loss_tte:2.3125 |  TR_Loss_mle:1.5758 || VA_Loss_tte:2.3382 || VA_Loss_mle:1.6432 ||\n",
      "|| Epoch:09000 | TR_Loss_tte:2.2915 |  TR_Loss_mle:1.5695 || VA_Loss_tte:2.2758 || VA_Loss_mle:1.5745 ||\n",
      "|| Epoch:09100 | TR_Loss_tte:2.3508 |  TR_Loss_mle:1.6356 || VA_Loss_tte:2.2451 || VA_Loss_mle:1.5512 ||\n",
      "|| Epoch:09200 | TR_Loss_tte:2.3068 |  TR_Loss_mle:1.5671 || VA_Loss_tte:2.2761 || VA_Loss_mle:1.5918 ||\n",
      "|| Epoch:09300 | TR_Loss_tte:2.2655 |  TR_Loss_mle:1.5415 || VA_Loss_tte:2.3541 || VA_Loss_mle:1.6337 ||\n",
      "|| Epoch:09400 | TR_Loss_tte:2.3387 |  TR_Loss_mle:1.6095 || VA_Loss_tte:2.2938 || VA_Loss_mle:1.6115 ||\n",
      "|| Epoch:09500 | TR_Loss_tte:2.3184 |  TR_Loss_mle:1.5775 || VA_Loss_tte:2.3576 || VA_Loss_mle:1.6379 ||\n",
      "|| Epoch:09600 | TR_Loss_tte:2.3311 |  TR_Loss_mle:1.5926 || VA_Loss_tte:2.3477 || VA_Loss_mle:1.6196 ||\n",
      "|| Epoch:09700 | TR_Loss_tte:2.3158 |  TR_Loss_mle:1.5806 || VA_Loss_tte:2.3123 || VA_Loss_mle:1.6015 ||\n",
      "|| Epoch:09800 | TR_Loss_tte:2.3610 |  TR_Loss_mle:1.6364 || VA_Loss_tte:2.3460 || VA_Loss_mle:1.6269 ||\n",
      "|| Epoch:09900 | TR_Loss_tte:2.2572 |  TR_Loss_mle:1.5450 || VA_Loss_tte:2.3831 || VA_Loss_mle:1.6660 ||\n",
      "|| Epoch:10000 | TR_Loss_tte:2.3393 |  TR_Loss_mle:1.6043 || VA_Loss_tte:2.3365 || VA_Loss_mle:1.6192 ||\n",
      "|| Epoch:10100 | TR_Loss_tte:2.3388 |  TR_Loss_mle:1.6107 || VA_Loss_tte:2.3498 || VA_Loss_mle:1.6144 ||\n",
      "|| Epoch:10200 | TR_Loss_tte:2.2145 |  TR_Loss_mle:1.5182 || VA_Loss_tte:2.3856 || VA_Loss_mle:1.6323 ||\n",
      "|| Epoch:10300 | TR_Loss_tte:2.3062 |  TR_Loss_mle:1.5915 || VA_Loss_tte:2.3579 || VA_Loss_mle:1.6251 ||\n",
      "|| Epoch:10400 | TR_Loss_tte:2.2982 |  TR_Loss_mle:1.5758 || VA_Loss_tte:2.4752 || VA_Loss_mle:1.7279 ||\n",
      "|| Epoch:10500 | TR_Loss_tte:2.3170 |  TR_Loss_mle:1.6044 || VA_Loss_tte:2.4709 || VA_Loss_mle:1.7256 ||\n",
      "|| Epoch:10600 | TR_Loss_tte:2.2136 |  TR_Loss_mle:1.5128 || VA_Loss_tte:2.5002 || VA_Loss_mle:1.7443 ||\n",
      "|| Epoch:10700 | TR_Loss_tte:2.3224 |  TR_Loss_mle:1.6079 || VA_Loss_tte:2.3968 || VA_Loss_mle:1.6580 ||\n",
      "|| Epoch:10800 | TR_Loss_tte:2.3349 |  TR_Loss_mle:1.6173 || VA_Loss_tte:2.3742 || VA_Loss_mle:1.6255 ||\n",
      "model trained...\n",
      "INFO:tensorflow:Restoring parameters from ./STRATCANS_v1_new2_MICE/p1.0/itr4/models/model_tte\n"
     ]
    }
   ],
   "source": [
    "PRED_TIMES = [0, 365, 365*2, 365*3]\n",
    "EVAL_TIMES = [365, 365*2, 365*3, 365*4, 365*5, 365*6, 365*7, 365*8, 365*9, 365*10]\n",
    "\n",
    "FINAL_RESULT1 = np.zeros([OUT_ITERATION, len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "FINAL_RESULT2 = np.zeros([OUT_ITERATION, len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "\n",
    "FINAL_RESULT1_pred = np.zeros([OUT_ITERATION, len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "FINAL_RESULT2_pred = np.zeros([OUT_ITERATION, len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "\n",
    "# for out_itr in range(1):\n",
    "#     out_itr = 0\n",
    "for out_itr in range(OUT_ITERATION):  \n",
    "    save_path = './{}/p{}/itr{}'.format(data_mode, p_weibull, out_itr)\n",
    "\n",
    "    if not os.path.exists(save_path + '/models/'):\n",
    "        os.makedirs(save_path + '/models/')\n",
    "\n",
    "    if not os.path.exists(save_path + '/results/'):\n",
    "        os.makedirs(save_path + '/results/')\n",
    "    \n",
    "    (tr_data_s,te_data_s, tr_data_t,te_data_t, tr_time,te_time, tr_tte,te_tte, tr_label,te_label, tr_tte_new,te_tte_new, tr_label_new,te_label_new) = train_test_split(\n",
    "        data_xs, data_xt, data_time, data_tte, data_y, data_tte_new, data_y_new, test_size=0.2, random_state=seed+out_itr\n",
    "    ) \n",
    "\n",
    "    (tr_data_s,va_data_s, tr_data_t,va_data_t, tr_time,va_time, tr_tte,va_tte, tr_label,va_label, tr_tte_new,va_tte_new, tr_label_new,va_label_new) = train_test_split(\n",
    "        tr_data_s, tr_data_t, tr_time, tr_tte, tr_label, tr_tte_new, tr_label_new, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.45)\n",
    "    config      = tf.ConfigProto(gpu_options=gpu_options)\n",
    "#     config.gpu_options.allow_growth = True\n",
    "    sess        = tf.Session(config=config)\n",
    "    model       = Model_DeepHit_Weibull(sess, \"Version1\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    CHECK_STEP = 100\n",
    "\n",
    "    min_loss = 1e8\n",
    "    max_flag = 50\n",
    "\n",
    "    avg_loss_tte   = 0   \n",
    "    avg_loss_mle   = 0\n",
    "    stopflag = 0\n",
    "\n",
    "    print( \"MAIN TRAINING ...\")\n",
    "    for itr in range(iteration):\n",
    "        xs_mb, xt_mb, t_mb, tte_mb, m_mb = f_get_minibatch(\n",
    "            mb_size, tr_data_s, tr_data_t, tr_time, tr_tte_new, tr_label_new\n",
    "        )\n",
    "\n",
    "        DATA       = (xs_mb, xt_mb, t_mb, tte_mb, m_mb)\n",
    "\n",
    "        _, loss_tte, loss_mle = model.train(DATA, alpha, keep_prob, lr_train)\n",
    "        \n",
    "        avg_loss_tte   += loss_tte/CHECK_STEP\n",
    "        avg_loss_mle   += loss_mle/CHECK_STEP\n",
    "\n",
    "        if (itr+1)%CHECK_STEP == 0:\n",
    "            stopflag += 1\n",
    "\n",
    "            DATA    = (va_data_s, va_data_t, va_time, va_tte_new, va_label_new)\n",
    "\n",
    "            va_loss_tte, va_loss_mle = model.get_cost(DATA, alpha)\n",
    "\n",
    "            print('|| Epoch:{0:05d} | TR_Loss_tte:{1:0.4f} |  TR_Loss_mle:{2:0.4f} || VA_Loss_tte:{3:0.4f} || VA_Loss_mle:{4:0.4f} ||'.format(\n",
    "                itr + 1, avg_loss_tte, avg_loss_mle, va_loss_tte, va_loss_mle))\n",
    "\n",
    "            avg_loss_tte  = 0   \n",
    "            avg_loss_mle  = 0\n",
    "            \n",
    "            if min_loss > va_loss_tte:\n",
    "                stopflag = 0\n",
    "                min_loss = va_loss_tte\n",
    "                \n",
    "#             if min_loss > va_loss_mle:\n",
    "#                 stopflag = 0\n",
    "#                 min_loss = va_loss_mle\n",
    "\n",
    "                saver.save(sess, save_path + '/models/model_tte')\n",
    "                print('saved...')\n",
    "\n",
    "#                 tmp_pred, _ = f_get_prediction(model, va_data_s, va_data_t, va_time, EVAL_TIMES)\n",
    "#                 print('saved... | max prediction: {}'.format(np.max(np.max(tmp_pred, axis=2), axis=0)) )     \n",
    "    \n",
    "\n",
    "\n",
    "                RESULT1 = np.zeros([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "                RESULT2 = np.zeros([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "\n",
    "                for p_idx, pred_time in enumerate(PRED_TIMES):\n",
    "                    p_idx_te  = te_tte >= pred_time\n",
    "                    p_idx_tr  = tr_tte >= pred_time\n",
    "\n",
    "                    tmp_pred, tmp_idx = f_get_prediction(model, te_data_s, te_data_t, te_time, EVAL_TIMES, pred_time_= pred_time)\n",
    "\n",
    "\n",
    "                    tmp_y = tr_label[p_idx_tr]\n",
    "                    tmp_t = tr_tte[p_idx_tr] - pred_time\n",
    "\n",
    "                    tr_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "                    tr_y_structured = np.array(tr_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "                    tmp_y = te_label[p_idx_te]\n",
    "                    tmp_t = te_tte[p_idx_te] - pred_time\n",
    "\n",
    "                    te_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "                    te_y_structured = np.array(te_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "\n",
    "                    for e_idx, eval_time in enumerate(EVAL_TIMES):\n",
    "                        if np.sum((tmp_t<=eval_time) & (tmp_y==1)) > 0:\n",
    "                            RESULT1[p_idx, e_idx] = concordance_index_ipcw(tr_y_structured, te_y_structured, tmp_pred[p_idx_te][:, e_idx], tau=eval_time)[0]\n",
    "                            RESULT2[p_idx, e_idx] = brier_score(tr_y_structured, te_y_structured, 1.- tmp_pred[p_idx_te][:, e_idx], times=eval_time)[1][0]\n",
    "                        \n",
    "                print(RESULT1)\n",
    "                \n",
    "        if stopflag >= max_flag:\n",
    "            print('model trained...')\n",
    "            break\n",
    "            \n",
    "    saver.restore(sess, save_path + '/models/model_tte')\n",
    "\n",
    "    PRED_TIMES = [0, 365, 365*2, 365*3]\n",
    "    EVAL_TIMES = [365, 365*2, 365*3, 365*4, 365*5, 365*6, 365*7, 365*8, 365*9, 365*10]\n",
    "\n",
    "    RESULT1 = -1. * np.ones([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "    RESULT2 = -1. * np.ones([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "\n",
    "    for p_idx, pred_time in enumerate(PRED_TIMES):\n",
    "        p_idx_te  = te_tte >= pred_time\n",
    "        p_idx_tr  = tr_tte >= pred_time\n",
    "\n",
    "        tmp_pred, tmp_idx = f_get_prediction(model, te_data_s, te_data_t, te_time, EVAL_TIMES, pred_time_= pred_time)\n",
    "\n",
    "\n",
    "        tmp_y = tr_label[p_idx_tr]\n",
    "        tmp_t = tr_tte[p_idx_tr] - pred_time\n",
    "\n",
    "        tr_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "        tr_y_structured = np.array(tr_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "        tmp_y = te_label[p_idx_te]\n",
    "        tmp_t = te_tte[p_idx_te] - pred_time\n",
    "\n",
    "        te_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "        te_y_structured = np.array(te_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "\n",
    "        for e_idx, eval_time in enumerate(EVAL_TIMES):\n",
    "            if np.sum((tmp_t<=eval_time) & (tmp_y==1)) > 0:\n",
    "                RESULT1[p_idx, e_idx] = concordance_index_ipcw(tr_y_structured, te_y_structured, tmp_pred[p_idx_te][:, e_idx], tau=eval_time)[0]\n",
    "                RESULT2[p_idx, e_idx] = brier_score(tr_y_structured, te_y_structured, 1.- tmp_pred[p_idx_te][:, e_idx], times=eval_time)[1][0]\n",
    "\n",
    "\n",
    "    pd.DataFrame(RESULT1, index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path+'/results/c_index.csv')\n",
    "    pd.DataFrame(RESULT2, index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path+'/results/brier_score.csv')\n",
    "    \n",
    "    FINAL_RESULT1[out_itr, :, :] = RESULT1\n",
    "    FINAL_RESULT2[out_itr, :, :] = RESULT2\n",
    "    \n",
    "    \n",
    "    RESULT1 = -1. * np.ones([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "    RESULT2 = -1. * np.ones([len(PRED_TIMES), len(EVAL_TIMES)])\n",
    "\n",
    "    for p_idx, pred_time in enumerate(PRED_TIMES):\n",
    "        p_idx_te  = te_tte >= 0\n",
    "        p_idx_tr  = tr_tte >= 0\n",
    "\n",
    "        tmp_pred, tmp_idx = f_get_prediction(model, te_data_s, te_data_t, te_time, EVAL_TIMES, pred_time_= pred_time)\n",
    "\n",
    "\n",
    "        tmp_y = tr_label[p_idx_tr]\n",
    "        tmp_t = tr_tte[p_idx_tr] - 0\n",
    "\n",
    "        tr_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "        tr_y_structured = np.array(tr_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "        tmp_y = te_label[p_idx_te]\n",
    "        tmp_t = te_tte[p_idx_te] - 0\n",
    "\n",
    "        te_y_structured =  [(tmp_y[i], tmp_t[i]) for i in range(len(tmp_y))]\n",
    "        te_y_structured = np.array(te_y_structured, dtype=[('status', 'bool'),('time','<f8')])\n",
    "\n",
    "\n",
    "        for e_idx, eval_time in enumerate(EVAL_TIMES):\n",
    "            if np.sum((tmp_t<=eval_time) & (tmp_y==1)) > 0:\n",
    "                RESULT1[p_idx, e_idx] = concordance_index_ipcw(tr_y_structured, te_y_structured, tmp_pred[p_idx_te][:, e_idx], tau=eval_time)[0]\n",
    "                RESULT2[p_idx, e_idx] = brier_score(tr_y_structured, te_y_structured, 1.-tmp_pred[p_idx_te][:, e_idx], times=eval_time)[1][0]\n",
    "\n",
    "    pd.DataFrame(RESULT1, index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path+'/results/c_index_pred.csv')\n",
    "    pd.DataFrame(RESULT2, index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path+'/results/brier_score_pred.csv')\n",
    "    \n",
    "    FINAL_RESULT1_pred[out_itr, :, :] = RESULT1\n",
    "    FINAL_RESULT2_pred[out_itr, :, :] = RESULT2\n",
    "    \n",
    "    save_path2 = './{}/p{}/'.format(data_mode, p_weibull)\n",
    "\n",
    "FINAL_RESULT1[FINAL_RESULT1 == -1] = np.nan\n",
    "FINAL_RESULT2[FINAL_RESULT2 == -1] = np.nan\n",
    "\n",
    "pd.DataFrame(np.nanmean(FINAL_RESULT1, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_c_index_mean.csv')\n",
    "pd.DataFrame(np.nanmean(FINAL_RESULT2, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_brier_score_mean.csv')\n",
    "\n",
    "pd.DataFrame(np.nanstd(FINAL_RESULT1, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_c_index_std.csv')\n",
    "pd.DataFrame(np.nanstd(FINAL_RESULT2, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_brier_score_std.csv')\n",
    "\n",
    "\n",
    "FINAL_RESULT1_pred[FINAL_RESULT1_pred == -1] = np.nan\n",
    "FINAL_RESULT2_pred[FINAL_RESULT2_pred == -1] = np.nan\n",
    "\n",
    "pd.DataFrame(np.nanmean(FINAL_RESULT1_pred, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_c_index_pred_mean.csv')\n",
    "pd.DataFrame(np.nanmean(FINAL_RESULT2_pred, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_brier_score_pred_mean.csv')\n",
    "\n",
    "pd.DataFrame(np.nanstd(FINAL_RESULT1_pred, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_c_index_pred_std.csv')\n",
    "pd.DataFrame(np.nanstd(FINAL_RESULT2_pred, axis=0), index=PRED_TIMES, columns=EVAL_TIMES).to_csv(save_path2 + 'final_brier_score_pred_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
